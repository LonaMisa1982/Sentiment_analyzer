{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc73fcbb-42f6-42c5-b058-ff85da33e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ddfe7a3-5724-48f0-8492-893360570bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352dae7b-33d6-4844-a273-7e334f2b6082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1413b07c-70ad-42cc-9467-310c6dddcbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax= df['Score'].value_counts().sort_index().plot(\n",
    "    kind='bar', \n",
    "    figsize=(10,5)\n",
    ")\n",
    "ax.set_xlabel('Reviews')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b25c83b-1a98-4e4b-b101-801a02afc1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dbf6ca6-27ef-473f-969c-d1783c740c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afcea036-accb-43c1-9783-c2619438346d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of          Id   ProductId          UserId                      ProfileName  \\\n",
       "0         1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1         2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2         3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3         4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4         5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "...     ...         ...             ...                              ...   \n",
       "9995   9996  B000P41A28  A3A63RACXR1XIL            A. Boodhoo \"deaddodo\"   \n",
       "9996   9997  B000P41A28    A5VVRGL8JA7R                             Adam   \n",
       "9997   9998  B000P41A28  A2TGDTJ8YCU6PD                          geena77   \n",
       "9998   9999  B000P41A28   AUV4GIZZE693O              Susan Coe \"sueysis\"   \n",
       "9999  10000  B000P41A28   A82WIMR4RSVLI                       Emrose mom   \n",
       "\n",
       "      HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                        1                       1      5  1303862400   \n",
       "1                        0                       0      1  1346976000   \n",
       "2                        1                       1      4  1219017600   \n",
       "3                        3                       3      2  1307923200   \n",
       "4                        0                       0      5  1350777600   \n",
       "...                    ...                     ...    ...         ...   \n",
       "9995                    10                      15      1  1204502400   \n",
       "9996                     2                       3      5  1306368000   \n",
       "9997                     0                       0      5  1347494400   \n",
       "9998                     1                       2      5  1203638400   \n",
       "9999                     0                       1      4  1337472000   \n",
       "\n",
       "                               Summary  \\\n",
       "0                Good Quality Dog Food   \n",
       "1                    Not as Advertised   \n",
       "2                \"Delight\" says it all   \n",
       "3                       Cough Medicine   \n",
       "4                          Great taffy   \n",
       "...                                ...   \n",
       "9995                      constipation   \n",
       "9996  Constipation Not A Problem if...   \n",
       "9997                Love this formula!   \n",
       "9998                   very convenient   \n",
       "9999        The best weve tried so far   \n",
       "\n",
       "                                                   Text  \n",
       "0     I have bought several of the Vitality canned d...  \n",
       "1     Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2     This is a confection that has been around a fe...  \n",
       "3     If you are looking for the secret ingredient i...  \n",
       "4     Great taffy at a great price.  There was a wid...  \n",
       "...                                                 ...  \n",
       "9995  we switched from the advance similac to the or...  \n",
       "9996  Like the bad reviews say, the organic formula ...  \n",
       "9997  I wanted to solely breastfeed but was unable t...  \n",
       "9998  i love the fact that i can get this delieved t...  \n",
       "9999  We have a 7 week old... He had gas and constip...  \n",
       "\n",
       "[10000 rows x 10 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f65d7c2b-39d1-49be-940e-b5f0c510148b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.8.1 in c:\\users\\archi\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\archi\\anaconda3\\lib\\site-packages (from nltk==3.8.1) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\archi\\anaconda3\\lib\\site-packages (from nltk==3.8.1) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from nltk==3.8.1) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\archi\\anaconda3\\lib\\site-packages (from nltk==3.8.1) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\archi\\anaconda3\\lib\\site-packages (from click->nltk==3.8.1) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk==3.8.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "971f5992-1c5b-4ae0-b9c4-8ccc8373cdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I am having a cup right now.  I went all over town looking for this coffee after been invited to a non profit organization to a banquet and besides the delicious food they top it off with this amazing delicious coffee.  I paid and extra 20 bucks to get it right way and it is worth the money. Highly recommend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example= df['Text'].iloc[333]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59d9c8f5-a85c-492b-b6c9-7a01a699243c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yes',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'having',\n",
       " 'a',\n",
       " 'cup',\n",
       " 'right',\n",
       " 'now',\n",
       " '.',\n",
       " 'I',\n",
       " 'went',\n",
       " 'all',\n",
       " 'over',\n",
       " 'town',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'this',\n",
       " 'coffee',\n",
       " 'after',\n",
       " 'been',\n",
       " 'invited',\n",
       " 'to',\n",
       " 'a',\n",
       " 'non',\n",
       " 'profit',\n",
       " 'organization',\n",
       " 'to',\n",
       " 'a',\n",
       " 'banquet',\n",
       " 'and',\n",
       " 'besides',\n",
       " 'the',\n",
       " 'delicious',\n",
       " 'food',\n",
       " 'they',\n",
       " 'top',\n",
       " 'it',\n",
       " 'off',\n",
       " 'with',\n",
       " 'this',\n",
       " 'amazing',\n",
       " 'delicious',\n",
       " 'coffee',\n",
       " '.',\n",
       " 'I',\n",
       " 'paid',\n",
       " 'and',\n",
       " 'extra',\n",
       " '20',\n",
       " 'bucks',\n",
       " 'to',\n",
       " 'get',\n",
       " 'it',\n",
       " 'right',\n",
       " 'way',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'worth',\n",
       " 'the',\n",
       " 'money',\n",
       " '.',\n",
       " 'Highly',\n",
       " 'recommend',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = word_tokenize(example)\n",
    "word[:66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ad788d-de08-4cdd-aa0b-039fa7314448",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\archi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fc56122-2aff-4360-93fd-1a40b38f3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "tagged= pos_tag(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10e10166-e0f2-4195-b85c-ea5115f5c7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\archi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\archi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d491ff7-bdcd-4073-9763-bcef088ec561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking the words into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f0eaa03-d524-4628-a6bb-2305609ecf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks= nltk.chunk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d80db8c6-5078-4690-98d7-952f86e876d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Yes/UH\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  having/VBG\n",
      "  a/DT\n",
      "  cup/NN\n",
      "  right/RB\n",
      "  now/RB\n",
      "  ./.\n",
      "  I/PRP\n",
      "  went/VBD\n",
      "  all/DT\n",
      "  over/IN\n",
      "  town/NN\n",
      "  looking/VBG\n",
      "  for/IN\n",
      "  this/DT\n",
      "  coffee/NN\n",
      "  after/IN\n",
      "  been/VBN\n",
      "  invited/VBN\n",
      "  to/TO\n",
      "  a/DT\n",
      "  non/JJ\n",
      "  profit/NN\n",
      "  organization/NN\n",
      "  to/TO\n",
      "  a/DT\n",
      "  banquet/NN\n",
      "  and/CC\n",
      "  besides/IN\n",
      "  the/DT\n",
      "  delicious/JJ\n",
      "  food/NN\n",
      "  they/PRP\n",
      "  top/VBD\n",
      "  it/PRP\n",
      "  off/RP\n",
      "  with/IN\n",
      "  this/DT\n",
      "  amazing/VBG\n",
      "  delicious/JJ\n",
      "  coffee/NN\n",
      "  ./.\n",
      "  I/PRP\n",
      "  paid/VBD\n",
      "  and/CC\n",
      "  extra/JJ\n",
      "  20/CD\n",
      "  bucks/NNS\n",
      "  to/TO\n",
      "  get/VB\n",
      "  it/PRP\n",
      "  right/JJ\n",
      "  way/NN\n",
      "  and/CC\n",
      "  it/PRP\n",
      "  is/VBZ\n",
      "  worth/IN\n",
      "  the/DT\n",
      "  money/NN\n",
      "  ./.\n",
      "  (PERSON Highly/NNP)\n",
      "  recommend/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "242f2a27-9bb8-46eb-b370-d03e93960e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VADER SENTIMENT ANALYSIS\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4faa93df-e31a-4cf0-8978-aff7dcfa865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\archi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36093f77-417b-43a6-b282-700e6e3b8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia= SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "782623f7-6f49-4cdb-ba26-2d0f9bdde56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I am having a cup right now.  I went all over town looking for this coffee after been invited to a non profit organization to a banquet and besides the delicious food they top it off with this amazing delicious coffee.  I paid and extra 20 bucks to get it right way and it is worth the money. Highly recommend.\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "837e4b7a-585a-40ed-b0f0-70511412c5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.662, 'pos': 0.338, 'compound': 0.9719}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dc04a06-c4f0-48fe-b3c3-66634c0f1509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the polarity score for the entire dataset of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "121c9702-3eb1-4928-bef7-b2239c0a9885",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_scores=[]\n",
    "for text in df['Text']:\n",
    "    compound_scores.append(sia.polarity_scores(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f0021f1-65da-4bd8-b67c-91074f425af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'neg': 0.0, 'neu': 0.695, 'pos': 0.305, 'compound': 0.9441},\n",
       " {'neg': 0.138, 'neu': 0.862, 'pos': 0.0, 'compound': -0.5664},\n",
       " {'neg': 0.091, 'neu': 0.754, 'pos': 0.155, 'compound': 0.8265},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.0, 'neu': 0.552, 'pos': 0.448, 'compound': 0.9468},\n",
       " {'neg': 0.029, 'neu': 0.809, 'pos': 0.163, 'compound': 0.883},\n",
       " {'neg': 0.034, 'neu': 0.693, 'pos': 0.273, 'compound': 0.9346},\n",
       " {'neg': 0.0, 'neu': 0.52, 'pos': 0.48, 'compound': 0.9487},\n",
       " {'neg': 0.0, 'neu': 0.851, 'pos': 0.149, 'compound': 0.6369},\n",
       " {'neg': 0.0, 'neu': 0.705, 'pos': 0.295, 'compound': 0.8313},\n",
       " {'neg': 0.017, 'neu': 0.846, 'pos': 0.137, 'compound': 0.9746},\n",
       " {'neg': 0.113, 'neu': 0.887, 'pos': 0.0, 'compound': -0.7579},\n",
       " {'neg': 0.031, 'neu': 0.923, 'pos': 0.046, 'compound': 0.296},\n",
       " {'neg': 0.0, 'neu': 0.355, 'pos': 0.645, 'compound': 0.9466},\n",
       " {'neg': 0.104, 'neu': 0.632, 'pos': 0.264, 'compound': 0.6486},\n",
       " {'neg': 0.0, 'neu': 0.861, 'pos': 0.139, 'compound': 0.5719},\n",
       " {'neg': 0.097, 'neu': 0.694, 'pos': 0.209, 'compound': 0.7481},\n",
       " {'neg': 0.0, 'neu': 0.61, 'pos': 0.39, 'compound': 0.8883},\n",
       " {'neg': 0.012, 'neu': 0.885, 'pos': 0.103, 'compound': 0.8957},\n",
       " {'neg': 0.0, 'neu': 0.863, 'pos': 0.137, 'compound': 0.6077},\n",
       " {'neg': 0.0, 'neu': 0.865, 'pos': 0.135, 'compound': 0.6249},\n",
       " {'neg': 0.0, 'neu': 0.739, 'pos': 0.261, 'compound': 0.9153},\n",
       " {'neg': 0.0, 'neu': 0.768, 'pos': 0.232, 'compound': 0.7687},\n",
       " {'neg': 0.085, 'neu': 0.771, 'pos': 0.143, 'compound': 0.2617},\n",
       " {'neg': 0.038, 'neu': 0.895, 'pos': 0.068, 'compound': 0.3939},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.128, 'neu': 0.872, 'pos': 0.0, 'compound': -0.296},\n",
       " {'neg': 0.04, 'neu': 0.808, 'pos': 0.152, 'compound': 0.5956},\n",
       " {'neg': 0.022, 'neu': 0.669, 'pos': 0.309, 'compound': 0.9913},\n",
       " {'neg': 0.017, 'neu': 0.846, 'pos': 0.137, 'compound': 0.9746},\n",
       " {'neg': 0.041, 'neu': 0.692, 'pos': 0.267, 'compound': 0.9713},\n",
       " {'neg': 0.0, 'neu': 0.484, 'pos': 0.516, 'compound': 0.9153},\n",
       " {'neg': 0.069, 'neu': 0.839, 'pos': 0.092, 'compound': 0.7103},\n",
       " {'neg': 0.024, 'neu': 0.72, 'pos': 0.256, 'compound': 0.9779},\n",
       " {'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'compound': 0.9091},\n",
       " {'neg': 0.024, 'neu': 0.821, 'pos': 0.155, 'compound': 0.7622},\n",
       " {'neg': 0.0, 'neu': 0.754, 'pos': 0.246, 'compound': 0.9196},\n",
       " {'neg': 0.0, 'neu': 0.938, 'pos': 0.062, 'compound': 0.4457},\n",
       " {'neg': 0.05, 'neu': 0.846, 'pos': 0.104, 'compound': 0.7638},\n",
       " {'neg': 0.0, 'neu': 0.856, 'pos': 0.144, 'compound': 0.8114},\n",
       " {'neg': 0.033, 'neu': 0.82, 'pos': 0.147, 'compound': 0.9301},\n",
       " {'neg': 0.03, 'neu': 0.848, 'pos': 0.122, 'compound': 0.9435},\n",
       " {'neg': 0.0, 'neu': 0.588, 'pos': 0.412, 'compound': 0.9441},\n",
       " {'neg': 0.0, 'neu': 0.685, 'pos': 0.315, 'compound': 0.9161},\n",
       " {'neg': 0.031, 'neu': 0.778, 'pos': 0.191, 'compound': 0.8421},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.0, 'neu': 0.737, 'pos': 0.263, 'compound': 0.9169},\n",
       " {'neg': 0.0, 'neu': 0.868, 'pos': 0.132, 'compound': 0.4404},\n",
       " {'neg': 0.0, 'neu': 0.821, 'pos': 0.179, 'compound': 0.747},\n",
       " {'neg': 0.056, 'neu': 0.865, 'pos': 0.079, 'compound': 0.2363},\n",
       " {'neg': 0.22, 'neu': 0.78, 'pos': 0.0, 'compound': -0.5448},\n",
       " {'neg': 0.047, 'neu': 0.735, 'pos': 0.218, 'compound': 0.9194},\n",
       " {'neg': 0.09, 'neu': 0.858, 'pos': 0.052, 'compound': -0.8259},\n",
       " {'neg': 0.075, 'neu': 0.925, 'pos': 0.0, 'compound': -0.3612},\n",
       " {'neg': 0.0, 'neu': 0.857, 'pos': 0.143, 'compound': 0.8761},\n",
       " {'neg': 0.071, 'neu': 0.708, 'pos': 0.221, 'compound': 0.8908},\n",
       " {'neg': 0.029, 'neu': 0.694, 'pos': 0.277, 'compound': 0.908},\n",
       " {'neg': 0.0, 'neu': 0.701, 'pos': 0.299, 'compound': 0.91},\n",
       " {'neg': 0.0, 'neu': 0.611, 'pos': 0.389, 'compound': 0.9323},\n",
       " {'neg': 0.0, 'neu': 0.638, 'pos': 0.362, 'compound': 0.8807},\n",
       " {'neg': 0.0, 'neu': 0.9, 'pos': 0.1, 'compound': 0.4404},\n",
       " {'neg': 0.0, 'neu': 0.741, 'pos': 0.259, 'compound': 0.8442},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.055, 'neu': 0.765, 'pos': 0.179, 'compound': 0.9817},\n",
       " {'neg': 0.046, 'neu': 0.75, 'pos': 0.205, 'compound': 0.8674},\n",
       " {'neg': 0.04, 'neu': 0.822, 'pos': 0.138, 'compound': 0.5165},\n",
       " {'neg': 0.057, 'neu': 0.869, 'pos': 0.073, 'compound': 0.492},\n",
       " {'neg': 0.183, 'neu': 0.776, 'pos': 0.041, 'compound': -0.9116},\n",
       " {'neg': 0.135, 'neu': 0.71, 'pos': 0.155, 'compound': -0.0096},\n",
       " {'neg': 0.344, 'neu': 0.52, 'pos': 0.136, 'compound': -0.7345},\n",
       " {'neg': 0.036, 'neu': 0.916, 'pos': 0.048, 'compound': 0.2228},\n",
       " {'neg': 0.078, 'neu': 0.701, 'pos': 0.222, 'compound': 0.9733},\n",
       " {'neg': 0.025, 'neu': 0.653, 'pos': 0.323, 'compound': 0.9787},\n",
       " {'neg': 0.093, 'neu': 0.762, 'pos': 0.144, 'compound': 0.9665},\n",
       " {'neg': 0.0, 'neu': 0.872, 'pos': 0.128, 'compound': 0.2263},\n",
       " {'neg': 0.106, 'neu': 0.768, 'pos': 0.126, 'compound': 0.1098},\n",
       " {'neg': 0.019, 'neu': 0.898, 'pos': 0.083, 'compound': 0.5647},\n",
       " {'neg': 0.034, 'neu': 0.798, 'pos': 0.168, 'compound': 0.8303},\n",
       " {'neg': 0.0, 'neu': 0.763, 'pos': 0.237, 'compound': 0.7814},\n",
       " {'neg': 0.087, 'neu': 0.589, 'pos': 0.324, 'compound': 0.8636},\n",
       " {'neg': 0.0, 'neu': 0.723, 'pos': 0.277, 'compound': 0.9098},\n",
       " {'neg': 0.0, 'neu': 0.663, 'pos': 0.337, 'compound': 0.9041},\n",
       " {'neg': 0.04, 'neu': 0.794, 'pos': 0.165, 'compound': 0.9957},\n",
       " {'neg': 0.055, 'neu': 0.767, 'pos': 0.178, 'compound': 0.8642},\n",
       " {'neg': 0.109, 'neu': 0.676, 'pos': 0.214, 'compound': 0.8431},\n",
       " {'neg': 0.035, 'neu': 0.698, 'pos': 0.267, 'compound': 0.9487},\n",
       " {'neg': 0.019, 'neu': 0.855, 'pos': 0.126, 'compound': 0.8797},\n",
       " {'neg': 0.05, 'neu': 0.735, 'pos': 0.215, 'compound': 0.7424},\n",
       " {'neg': 0.048, 'neu': 0.762, 'pos': 0.19, 'compound': 0.9716},\n",
       " {'neg': 0.029, 'neu': 0.645, 'pos': 0.326, 'compound': 0.9554},\n",
       " {'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'compound': 0.7351},\n",
       " {'neg': 0.0, 'neu': 0.837, 'pos': 0.163, 'compound': 0.6249},\n",
       " {'neg': 0.069, 'neu': 0.663, 'pos': 0.268, 'compound': 0.8255},\n",
       " {'neg': 0.01, 'neu': 0.781, 'pos': 0.208, 'compound': 0.9882},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.031, 'neu': 0.732, 'pos': 0.237, 'compound': 0.9273},\n",
       " {'neg': 0.0, 'neu': 0.818, 'pos': 0.182, 'compound': 0.982},\n",
       " {'neg': 0.053, 'neu': 0.793, 'pos': 0.154, 'compound': 0.7729},\n",
       " {'neg': 0.024, 'neu': 0.91, 'pos': 0.066, 'compound': 0.5106},\n",
       " {'neg': 0.173, 'neu': 0.735, 'pos': 0.092, 'compound': -0.5267},\n",
       " {'neg': 0.0, 'neu': 0.807, 'pos': 0.193, 'compound': 0.7717},\n",
       " {'neg': 0.103, 'neu': 0.752, 'pos': 0.145, 'compound': 0.2285},\n",
       " {'neg': 0.0, 'neu': 0.75, 'pos': 0.25, 'compound': 0.9287},\n",
       " {'neg': 0.0, 'neu': 0.859, 'pos': 0.141, 'compound': 0.7249},\n",
       " {'neg': 0.051, 'neu': 0.577, 'pos': 0.372, 'compound': 0.9313},\n",
       " {'neg': 0.0, 'neu': 0.696, 'pos': 0.304, 'compound': 0.9603},\n",
       " {'neg': 0.0, 'neu': 0.791, 'pos': 0.209, 'compound': 0.5719},\n",
       " {'neg': 0.0, 'neu': 0.804, 'pos': 0.196, 'compound': 0.9503},\n",
       " {'neg': 0.059, 'neu': 0.676, 'pos': 0.265, 'compound': 0.9116},\n",
       " {'neg': 0.014, 'neu': 0.764, 'pos': 0.222, 'compound': 0.9841},\n",
       " {'neg': 0.059, 'neu': 0.879, 'pos': 0.062, 'compound': 0.0176},\n",
       " {'neg': 0.0, 'neu': 0.81, 'pos': 0.19, 'compound': 0.8769},\n",
       " {'neg': 0.037, 'neu': 0.786, 'pos': 0.177, 'compound': 0.9946},\n",
       " {'neg': 0.0, 'neu': 0.631, 'pos': 0.369, 'compound': 0.8779},\n",
       " {'neg': 0.027, 'neu': 0.727, 'pos': 0.245, 'compound': 0.9379},\n",
       " {'neg': 0.0, 'neu': 0.645, 'pos': 0.355, 'compound': 0.872},\n",
       " {'neg': 0.0, 'neu': 0.892, 'pos': 0.108, 'compound': 0.6573},\n",
       " {'neg': 0.0, 'neu': 0.781, 'pos': 0.219, 'compound': 0.9751},\n",
       " {'neg': 0.05, 'neu': 0.872, 'pos': 0.079, 'compound': 0.8972},\n",
       " {'neg': 0.013, 'neu': 0.785, 'pos': 0.203, 'compound': 0.9828},\n",
       " {'neg': 0.026, 'neu': 0.759, 'pos': 0.215, 'compound': 0.9509},\n",
       " {'neg': 0.102, 'neu': 0.822, 'pos': 0.076, 'compound': -0.3626},\n",
       " {'neg': 0.025, 'neu': 0.803, 'pos': 0.172, 'compound': 0.9022},\n",
       " {'neg': 0.017, 'neu': 0.795, 'pos': 0.188, 'compound': 0.9769},\n",
       " {'neg': 0.079, 'neu': 0.67, 'pos': 0.252, 'compound': 0.9678},\n",
       " {'neg': 0.035, 'neu': 0.87, 'pos': 0.095, 'compound': 0.5709},\n",
       " {'neg': 0.0, 'neu': 0.721, 'pos': 0.279, 'compound': 0.9258},\n",
       " {'neg': 0.067, 'neu': 0.633, 'pos': 0.299, 'compound': 0.9022},\n",
       " {'neg': 0.043, 'neu': 0.728, 'pos': 0.229, 'compound': 0.8142},\n",
       " {'neg': 0.114, 'neu': 0.676, 'pos': 0.21, 'compound': 0.6721},\n",
       " {'neg': 0.0, 'neu': 0.755, 'pos': 0.245, 'compound': 0.8658},\n",
       " {'neg': 0.135, 'neu': 0.76, 'pos': 0.105, 'compound': -0.3612},\n",
       " {'neg': 0.046, 'neu': 0.772, 'pos': 0.181, 'compound': 0.7902},\n",
       " {'neg': 0.02, 'neu': 0.878, 'pos': 0.103, 'compound': 0.8082},\n",
       " {'neg': 0.0, 'neu': 0.877, 'pos': 0.123, 'compound': 0.4215},\n",
       " {'neg': 0.0, 'neu': 0.9, 'pos': 0.1, 'compound': 0.6503},\n",
       " {'neg': 0.0, 'neu': 0.695, 'pos': 0.305, 'compound': 0.9661},\n",
       " {'neg': 0.0, 'neu': 0.689, 'pos': 0.311, 'compound': 0.8591},\n",
       " {'neg': 0.15, 'neu': 0.773, 'pos': 0.077, 'compound': -0.4199},\n",
       " {'neg': 0.043, 'neu': 0.833, 'pos': 0.125, 'compound': 0.835},\n",
       " {'neg': 0.098, 'neu': 0.787, 'pos': 0.114, 'compound': 0.2023},\n",
       " {'neg': 0.0, 'neu': 0.782, 'pos': 0.218, 'compound': 0.7814},\n",
       " {'neg': 0.0, 'neu': 0.763, 'pos': 0.237, 'compound': 0.9296},\n",
       " {'neg': 0.059, 'neu': 0.667, 'pos': 0.274, 'compound': 0.9653},\n",
       " {'neg': 0.058, 'neu': 0.841, 'pos': 0.102, 'compound': 0.6124},\n",
       " {'neg': 0.144, 'neu': 0.677, 'pos': 0.178, 'compound': 0.6341},\n",
       " {'neg': 0.087, 'neu': 0.783, 'pos': 0.13, 'compound': 0.7567},\n",
       " {'neg': 0.058, 'neu': 0.867, 'pos': 0.075, 'compound': 0.1533},\n",
       " {'neg': 0.04, 'neu': 0.833, 'pos': 0.127, 'compound': 0.6956},\n",
       " {'neg': 0.0, 'neu': 0.709, 'pos': 0.291, 'compound': 0.9231},\n",
       " {'neg': 0.0, 'neu': 0.564, 'pos': 0.436, 'compound': 0.9858},\n",
       " {'neg': 0.0, 'neu': 0.784, 'pos': 0.216, 'compound': 0.765},\n",
       " {'neg': 0.0, 'neu': 0.775, 'pos': 0.225, 'compound': 0.7269},\n",
       " {'neg': 0.12, 'neu': 0.76, 'pos': 0.12, 'compound': 0.2502},\n",
       " {'neg': 0.0, 'neu': 0.647, 'pos': 0.353, 'compound': 0.9803},\n",
       " {'neg': 0.0, 'neu': 0.768, 'pos': 0.232, 'compound': 0.9681},\n",
       " {'neg': 0.191, 'neu': 0.809, 'pos': 0.0, 'compound': -0.7269},\n",
       " {'neg': 0.071, 'neu': 0.514, 'pos': 0.415, 'compound': 0.8934},\n",
       " {'neg': 0.065, 'neu': 0.893, 'pos': 0.042, 'compound': -0.4721},\n",
       " {'neg': 0.081, 'neu': 0.779, 'pos': 0.14, 'compound': 0.4194},\n",
       " {'neg': 0.0, 'neu': 0.644, 'pos': 0.356, 'compound': 0.9117},\n",
       " {'neg': 0.106, 'neu': 0.894, 'pos': 0.0, 'compound': -0.5504},\n",
       " {'neg': 0.072, 'neu': 0.652, 'pos': 0.276, 'compound': 0.9517},\n",
       " {'neg': 0.047, 'neu': 0.869, 'pos': 0.085, 'compound': 0.4199},\n",
       " {'neg': 0.025, 'neu': 0.752, 'pos': 0.223, 'compound': 0.8957},\n",
       " {'neg': 0.032, 'neu': 0.717, 'pos': 0.251, 'compound': 0.9597},\n",
       " {'neg': 0.0, 'neu': 0.657, 'pos': 0.343, 'compound': 0.9098},\n",
       " {'neg': 0.05, 'neu': 0.905, 'pos': 0.045, 'compound': -0.1154},\n",
       " {'neg': 0.186, 'neu': 0.74, 'pos': 0.074, 'compound': -0.5283},\n",
       " {'neg': 0.141, 'neu': 0.832, 'pos': 0.028, 'compound': -0.7721},\n",
       " {'neg': 0.0, 'neu': 0.854, 'pos': 0.146, 'compound': 0.6476},\n",
       " {'neg': 0.04, 'neu': 0.844, 'pos': 0.116, 'compound': 0.6808},\n",
       " {'neg': 0.0, 'neu': 0.763, 'pos': 0.237, 'compound': 0.8906},\n",
       " {'neg': 0.022, 'neu': 0.788, 'pos': 0.189, 'compound': 0.9901},\n",
       " {'neg': 0.04, 'neu': 0.722, 'pos': 0.237, 'compound': 0.9782},\n",
       " {'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'compound': 0.7579},\n",
       " {'neg': 0.0, 'neu': 0.938, 'pos': 0.062, 'compound': 0.4215},\n",
       " {'neg': 0.058, 'neu': 0.794, 'pos': 0.148, 'compound': 0.6249},\n",
       " {'neg': 0.2, 'neu': 0.63, 'pos': 0.171, 'compound': 0.1203},\n",
       " {'neg': 0.048, 'neu': 0.829, 'pos': 0.122, 'compound': 0.7458},\n",
       " {'neg': 0.076, 'neu': 0.767, 'pos': 0.156, 'compound': 0.6085},\n",
       " {'neg': 0.0, 'neu': 0.433, 'pos': 0.567, 'compound': 0.9667},\n",
       " {'neg': 0.088, 'neu': 0.743, 'pos': 0.169, 'compound': 0.943},\n",
       " {'neg': 0.0, 'neu': 0.857, 'pos': 0.143, 'compound': 0.9577},\n",
       " {'neg': 0.11, 'neu': 0.593, 'pos': 0.297, 'compound': 0.6597},\n",
       " {'neg': 0.189, 'neu': 0.811, 'pos': 0.0, 'compound': -0.5994},\n",
       " {'neg': 0.016, 'neu': 0.842, 'pos': 0.142, 'compound': 0.9944},\n",
       " {'neg': 0.0, 'neu': 0.824, 'pos': 0.176, 'compound': 0.6983},\n",
       " {'neg': 0.0, 'neu': 0.843, 'pos': 0.157, 'compound': 0.8868},\n",
       " {'neg': 0.0, 'neu': 0.934, 'pos': 0.066, 'compound': 0.3506},\n",
       " {'neg': 0.148, 'neu': 0.64, 'pos': 0.212, 'compound': 0.4926},\n",
       " {'neg': 0.0, 'neu': 0.75, 'pos': 0.25, 'compound': 0.9062},\n",
       " {'neg': 0.055, 'neu': 0.728, 'pos': 0.217, 'compound': 0.8756},\n",
       " {'neg': 0.031, 'neu': 0.735, 'pos': 0.234, 'compound': 0.9595},\n",
       " {'neg': 0.082, 'neu': 0.483, 'pos': 0.435, 'compound': 0.8299},\n",
       " {'neg': 0.0, 'neu': 0.761, 'pos': 0.239, 'compound': 0.9538},\n",
       " {'neg': 0.0, 'neu': 0.917, 'pos': 0.083, 'compound': 0.4738},\n",
       " {'neg': 0.0, 'neu': 0.904, 'pos': 0.096, 'compound': 0.4153},\n",
       " {'neg': 0.0, 'neu': 0.701, 'pos': 0.299, 'compound': 0.8268},\n",
       " {'neg': 0.0, 'neu': 0.811, 'pos': 0.189, 'compound': 0.7178},\n",
       " {'neg': 0.039, 'neu': 0.888, 'pos': 0.072, 'compound': 0.6381},\n",
       " {'neg': 0.064, 'neu': 0.597, 'pos': 0.339, 'compound': 0.9531},\n",
       " {'neg': 0.0, 'neu': 0.688, 'pos': 0.312, 'compound': 0.8225},\n",
       " {'neg': 0.061, 'neu': 0.814, 'pos': 0.125, 'compound': 0.8728},\n",
       " {'neg': 0.0, 'neu': 0.882, 'pos': 0.118, 'compound': 0.6249},\n",
       " {'neg': 0.0, 'neu': 0.754, 'pos': 0.246, 'compound': 0.9368},\n",
       " {'neg': 0.0, 'neu': 0.59, 'pos': 0.41, 'compound': 0.8779},\n",
       " {'neg': 0.051, 'neu': 0.8, 'pos': 0.15, 'compound': 0.8436},\n",
       " {'neg': 0.05, 'neu': 0.82, 'pos': 0.13, 'compound': 0.8913},\n",
       " {'neg': 0.045, 'neu': 0.761, 'pos': 0.194, 'compound': 0.9893},\n",
       " {'neg': 0.075, 'neu': 0.755, 'pos': 0.171, 'compound': 0.9218},\n",
       " {'neg': 0.051, 'neu': 0.821, 'pos': 0.129, 'compound': 0.9529},\n",
       " {'neg': 0.051, 'neu': 0.838, 'pos': 0.11, 'compound': 0.4404},\n",
       " {'neg': 0.095, 'neu': 0.883, 'pos': 0.022, 'compound': -0.9726},\n",
       " {'neg': 0.0, 'neu': 0.891, 'pos': 0.109, 'compound': 0.6476},\n",
       " {'neg': 0.0, 'neu': 0.798, 'pos': 0.202, 'compound': 0.7964},\n",
       " {'neg': 0.078, 'neu': 0.922, 'pos': 0.0, 'compound': -0.296},\n",
       " {'neg': 0.015, 'neu': 0.884, 'pos': 0.101, 'compound': 0.9736},\n",
       " {'neg': 0.059, 'neu': 0.774, 'pos': 0.167, 'compound': 0.9424},\n",
       " {'neg': 0.031, 'neu': 0.702, 'pos': 0.267, 'compound': 0.9812},\n",
       " {'neg': 0.027, 'neu': 0.909, 'pos': 0.064, 'compound': 0.25},\n",
       " {'neg': 0.068, 'neu': 0.666, 'pos': 0.266, 'compound': 0.9883},\n",
       " {'neg': 0.0, 'neu': 0.779, 'pos': 0.221, 'compound': 0.9623},\n",
       " {'neg': 0.0, 'neu': 0.607, 'pos': 0.393, 'compound': 0.923},\n",
       " {'neg': 0.152, 'neu': 0.739, 'pos': 0.109, 'compound': -0.25},\n",
       " {'neg': 0.064, 'neu': 0.794, 'pos': 0.141, 'compound': 0.7951},\n",
       " {'neg': 0.139, 'neu': 0.754, 'pos': 0.108, 'compound': -0.3774},\n",
       " {'neg': 0.106, 'neu': 0.718, 'pos': 0.176, 'compound': 0.5475},\n",
       " {'neg': 0.0, 'neu': 0.837, 'pos': 0.163, 'compound': 0.6486},\n",
       " {'neg': 0.025, 'neu': 0.854, 'pos': 0.121, 'compound': 0.6478},\n",
       " {'neg': 0.03, 'neu': 0.726, 'pos': 0.244, 'compound': 0.9281},\n",
       " {'neg': 0.0, 'neu': 0.904, 'pos': 0.096, 'compound': 0.8144},\n",
       " {'neg': 0.0, 'neu': 0.807, 'pos': 0.193, 'compound': 0.8126},\n",
       " {'neg': 0.103, 'neu': 0.729, 'pos': 0.169, 'compound': 0.2481},\n",
       " {'neg': 0.0, 'neu': 0.805, 'pos': 0.195, 'compound': 0.8655},\n",
       " {'neg': 0.11, 'neu': 0.792, 'pos': 0.098, 'compound': -0.4786},\n",
       " {'neg': 0.041, 'neu': 0.793, 'pos': 0.166, 'compound': 0.9387},\n",
       " {'neg': 0.029, 'neu': 0.798, 'pos': 0.174, 'compound': 0.9936},\n",
       " {'neg': 0.064, 'neu': 0.7, 'pos': 0.236, 'compound': 0.9677},\n",
       " {'neg': 0.0, 'neu': 0.72, 'pos': 0.28, 'compound': 0.765},\n",
       " {'neg': 0.066, 'neu': 0.71, 'pos': 0.223, 'compound': 0.9553},\n",
       " {'neg': 0.0, 'neu': 0.765, 'pos': 0.235, 'compound': 0.807},\n",
       " {'neg': 0.0, 'neu': 0.76, 'pos': 0.24, 'compound': 0.9344},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.081, 'neu': 0.63, 'pos': 0.289, 'compound': 0.765},\n",
       " {'neg': 0.072, 'neu': 0.825, 'pos': 0.103, 'compound': 0.682},\n",
       " {'neg': 0.075, 'neu': 0.633, 'pos': 0.292, 'compound': 0.9757},\n",
       " {'neg': 0.0, 'neu': 0.869, 'pos': 0.131, 'compound': 0.7717},\n",
       " {'neg': 0.0, 'neu': 0.602, 'pos': 0.398, 'compound': 0.9351},\n",
       " {'neg': 0.0, 'neu': 0.75, 'pos': 0.25, 'compound': 0.7184},\n",
       " {'neg': 0.047, 'neu': 0.781, 'pos': 0.172, 'compound': 0.9476},\n",
       " {'neg': 0.076, 'neu': 0.924, 'pos': 0.0, 'compound': -0.4823},\n",
       " {'neg': 0.107, 'neu': 0.893, 'pos': 0.0, 'compound': -0.4767},\n",
       " {'neg': 0.0, 'neu': 0.801, 'pos': 0.199, 'compound': 0.9698},\n",
       " {'neg': 0.091, 'neu': 0.736, 'pos': 0.172, 'compound': 0.4118},\n",
       " {'neg': 0.103, 'neu': 0.699, 'pos': 0.198, 'compound': 0.9805},\n",
       " {'neg': 0.034, 'neu': 0.664, 'pos': 0.302, 'compound': 0.9463},\n",
       " {'neg': 0.105, 'neu': 0.816, 'pos': 0.079, 'compound': -0.3489},\n",
       " {'neg': 0.04, 'neu': 0.841, 'pos': 0.119, 'compound': 0.8883},\n",
       " {'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'compound': 0.8824},\n",
       " {'neg': 0.0, 'neu': 0.613, 'pos': 0.387, 'compound': 0.9493},\n",
       " {'neg': 0.0, 'neu': 0.54, 'pos': 0.46, 'compound': 0.9153},\n",
       " {'neg': 0.106, 'neu': 0.706, 'pos': 0.188, 'compound': 0.5849},\n",
       " {'neg': 0.098, 'neu': 0.875, 'pos': 0.026, 'compound': -0.9218},\n",
       " {'neg': 0.051, 'neu': 0.802, 'pos': 0.147, 'compound': 0.872},\n",
       " {'neg': 0.0, 'neu': 0.619, 'pos': 0.381, 'compound': 0.902},\n",
       " {'neg': 0.0, 'neu': 0.862, 'pos': 0.138, 'compound': 0.4926},\n",
       " {'neg': 0.062, 'neu': 0.911, 'pos': 0.028, 'compound': -0.7067},\n",
       " {'neg': 0.0, 'neu': 0.767, 'pos': 0.233, 'compound': 0.8176},\n",
       " {'neg': 0.032, 'neu': 0.794, 'pos': 0.174, 'compound': 0.9354},\n",
       " {'neg': 0.0, 'neu': 0.839, 'pos': 0.161, 'compound': 0.5927},\n",
       " {'neg': 0.062, 'neu': 0.863, 'pos': 0.074, 'compound': 0.2609},\n",
       " {'neg': 0.052, 'neu': 0.817, 'pos': 0.132, 'compound': 0.7003},\n",
       " {'neg': 0.0, 'neu': 0.733, 'pos': 0.267, 'compound': 0.7346},\n",
       " {'neg': 0.037, 'neu': 0.693, 'pos': 0.271, 'compound': 0.9421},\n",
       " {'neg': 0.132, 'neu': 0.711, 'pos': 0.157, 'compound': 0.3303},\n",
       " {'neg': 0.0, 'neu': 0.523, 'pos': 0.477, 'compound': 0.9542},\n",
       " {'neg': 0.025, 'neu': 0.809, 'pos': 0.167, 'compound': 0.937},\n",
       " {'neg': 0.072, 'neu': 0.641, 'pos': 0.288, 'compound': 0.8565},\n",
       " {'neg': 0.066, 'neu': 0.859, 'pos': 0.075, 'compound': 0.1666},\n",
       " {'neg': 0.049, 'neu': 0.823, 'pos': 0.127, 'compound': 0.6438},\n",
       " {'neg': 0.0, 'neu': 0.754, 'pos': 0.246, 'compound': 0.8016},\n",
       " {'neg': 0.028, 'neu': 0.934, 'pos': 0.038, 'compound': 0.1779},\n",
       " {'neg': 0.032, 'neu': 0.792, 'pos': 0.176, 'compound': 0.9852},\n",
       " {'neg': 0.0, 'neu': 0.864, 'pos': 0.136, 'compound': 0.5255},\n",
       " {'neg': 0.0, 'neu': 0.898, 'pos': 0.102, 'compound': 0.7917},\n",
       " {'neg': 0.0, 'neu': 0.857, 'pos': 0.143, 'compound': 0.919},\n",
       " {'neg': 0.035, 'neu': 0.801, 'pos': 0.163, 'compound': 0.9676},\n",
       " {'neg': 0.054, 'neu': 0.745, 'pos': 0.2, 'compound': 0.9557},\n",
       " {'neg': 0.039, 'neu': 0.697, 'pos': 0.264, 'compound': 0.8439},\n",
       " {'neg': 0.104, 'neu': 0.705, 'pos': 0.191, 'compound': 0.6257},\n",
       " {'neg': 0.052, 'neu': 0.745, 'pos': 0.203, 'compound': 0.9434},\n",
       " {'neg': 0.09, 'neu': 0.705, 'pos': 0.205, 'compound': 0.8636},\n",
       " {'neg': 0.034, 'neu': 0.757, 'pos': 0.209, 'compound': 0.9823},\n",
       " {'neg': 0.0, 'neu': 0.887, 'pos': 0.113, 'compound': 0.4939},\n",
       " {'neg': 0.12, 'neu': 0.781, 'pos': 0.099, 'compound': -0.7095},\n",
       " {'neg': 0.025, 'neu': 0.737, 'pos': 0.239, 'compound': 0.9566},\n",
       " {'neg': 0.0, 'neu': 0.811, 'pos': 0.189, 'compound': 0.8781},\n",
       " {'neg': 0.0, 'neu': 0.681, 'pos': 0.319, 'compound': 0.8934},\n",
       " {'neg': 0.078, 'neu': 0.735, 'pos': 0.187, 'compound': 0.9637},\n",
       " {'neg': 0.0, 'neu': 0.632, 'pos': 0.368, 'compound': 0.9661},\n",
       " {'neg': 0.148, 'neu': 0.625, 'pos': 0.227, 'compound': 0.5849},\n",
       " {'neg': 0.014, 'neu': 0.705, 'pos': 0.281, 'compound': 0.9763},\n",
       " {'neg': 0.076, 'neu': 0.791, 'pos': 0.133, 'compound': 0.25},\n",
       " {'neg': 0.058, 'neu': 0.778, 'pos': 0.165, 'compound': 0.5734},\n",
       " {'neg': 0.15, 'neu': 0.773, 'pos': 0.077, 'compound': -0.9037},\n",
       " {'neg': 0.097, 'neu': 0.781, 'pos': 0.122, 'compound': 0.4733},\n",
       " {'neg': 0.0, 'neu': 0.649, 'pos': 0.351, 'compound': 0.894},\n",
       " {'neg': 0.0, 'neu': 0.796, 'pos': 0.204, 'compound': 0.9695},\n",
       " {'neg': 0.0, 'neu': 0.774, 'pos': 0.226, 'compound': 0.9287},\n",
       " {'neg': 0.031, 'neu': 0.657, 'pos': 0.312, 'compound': 0.9644},\n",
       " {'neg': 0.087, 'neu': 0.913, 'pos': 0.0, 'compound': -0.4939},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.018, 'neu': 0.914, 'pos': 0.069, 'compound': 0.4971},\n",
       " {'neg': 0.024, 'neu': 0.828, 'pos': 0.148, 'compound': 0.6897},\n",
       " {'neg': 0.06, 'neu': 0.772, 'pos': 0.168, 'compound': 0.9109},\n",
       " {'neg': 0.0, 'neu': 0.823, 'pos': 0.177, 'compound': 0.5783},\n",
       " {'neg': 0.07, 'neu': 0.839, 'pos': 0.091, 'compound': 0.6785},\n",
       " {'neg': 0.0, 'neu': 0.904, 'pos': 0.096, 'compound': 0.3716},\n",
       " {'neg': 0.0, 'neu': 0.758, 'pos': 0.242, 'compound': 0.7717},\n",
       " {'neg': 0.065, 'neu': 0.562, 'pos': 0.373, 'compound': 0.886},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.05, 'neu': 0.69, 'pos': 0.26, 'compound': 0.7712},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.213, 'neu': 0.514, 'pos': 0.274, 'compound': 0.3185},\n",
       " {'neg': 0.0, 'neu': 0.688, 'pos': 0.312, 'compound': 0.8979},\n",
       " {'neg': 0.075, 'neu': 0.726, 'pos': 0.199, 'compound': 0.9373},\n",
       " {'neg': 0.064, 'neu': 0.594, 'pos': 0.342, 'compound': 0.9581},\n",
       " {'neg': 0.163, 'neu': 0.708, 'pos': 0.129, 'compound': -0.8462},\n",
       " {'neg': 0.029, 'neu': 0.856, 'pos': 0.115, 'compound': 0.5709},\n",
       " {'neg': 0.0, 'neu': 0.837, 'pos': 0.163, 'compound': 0.6249},\n",
       " {'neg': 0.115, 'neu': 0.885, 'pos': 0.0, 'compound': -0.4588},\n",
       " {'neg': 0.0, 'neu': 0.689, 'pos': 0.311, 'compound': 0.9732},\n",
       " {'neg': 0.0, 'neu': 0.662, 'pos': 0.338, 'compound': 0.9719},\n",
       " {'neg': 0.0, 'neu': 0.886, 'pos': 0.114, 'compound': 0.6124},\n",
       " {'neg': 0.046, 'neu': 0.8, 'pos': 0.154, 'compound': 0.6796},\n",
       " {'neg': 0.078, 'neu': 0.651, 'pos': 0.271, 'compound': 0.8506},\n",
       " {'neg': 0.0, 'neu': 0.765, 'pos': 0.235, 'compound': 0.9008},\n",
       " {'neg': 0.0, 'neu': 0.734, 'pos': 0.266, 'compound': 0.784},\n",
       " {'neg': 0.078, 'neu': 0.823, 'pos': 0.098, 'compound': 0.4416},\n",
       " {'neg': 0.069, 'neu': 0.782, 'pos': 0.149, 'compound': 0.8499},\n",
       " {'neg': 0.041, 'neu': 0.657, 'pos': 0.302, 'compound': 0.8731},\n",
       " {'neg': 0.0, 'neu': 0.912, 'pos': 0.088, 'compound': 0.4939},\n",
       " {'neg': 0.11, 'neu': 0.678, 'pos': 0.211, 'compound': 0.8053},\n",
       " {'neg': 0.101, 'neu': 0.627, 'pos': 0.273, 'compound': 0.9758},\n",
       " {'neg': 0.044, 'neu': 0.725, 'pos': 0.231, 'compound': 0.8319},\n",
       " {'neg': 0.0, 'neu': 0.608, 'pos': 0.392, 'compound': 0.9694},\n",
       " {'neg': 0.093, 'neu': 0.752, 'pos': 0.155, 'compound': 0.7667},\n",
       " {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.908},\n",
       " {'neg': 0.071, 'neu': 0.861, 'pos': 0.068, 'compound': -0.0258},\n",
       " {'neg': 0.0, 'neu': 0.715, 'pos': 0.285, 'compound': 0.9177},\n",
       " {'neg': 0.064, 'neu': 0.727, 'pos': 0.209, 'compound': 0.7337},\n",
       " {'neg': 0.0, 'neu': 0.893, 'pos': 0.107, 'compound': 0.802},\n",
       " {'neg': 0.0, 'neu': 0.888, 'pos': 0.112, 'compound': 0.6604},\n",
       " {'neg': 0.0, 'neu': 0.802, 'pos': 0.198, 'compound': 0.6892},\n",
       " {'neg': 0.05, 'neu': 0.734, 'pos': 0.215, 'compound': 0.8008},\n",
       " {'neg': 0.027, 'neu': 0.835, 'pos': 0.138, 'compound': 0.8805},\n",
       " {'neg': 0.0, 'neu': 0.895, 'pos': 0.105, 'compound': 0.631},\n",
       " {'neg': 0.164, 'neu': 0.694, 'pos': 0.142, 'compound': 0.283},\n",
       " {'neg': 0.0, 'neu': 0.705, 'pos': 0.295, 'compound': 0.954},\n",
       " {'neg': 0.033, 'neu': 0.785, 'pos': 0.182, 'compound': 0.9441},\n",
       " {'neg': 0.228, 'neu': 0.772, 'pos': 0.0, 'compound': -0.734},\n",
       " {'neg': 0.0, 'neu': 0.891, 'pos': 0.109, 'compound': 0.8802},\n",
       " {'neg': 0.0, 'neu': 0.742, 'pos': 0.258, 'compound': 0.8088},\n",
       " {'neg': 0.033, 'neu': 0.621, 'pos': 0.346, 'compound': 0.9334},\n",
       " {'neg': 0.076, 'neu': 0.768, 'pos': 0.156, 'compound': 0.4434},\n",
       " {'neg': 0.0, 'neu': 0.685, 'pos': 0.315, 'compound': 0.9366},\n",
       " {'neg': 0.038, 'neu': 0.84, 'pos': 0.122, 'compound': 0.8016},\n",
       " {'neg': 0.064, 'neu': 0.871, 'pos': 0.066, 'compound': 0.0258},\n",
       " {'neg': 0.0, 'neu': 0.913, 'pos': 0.087, 'compound': 0.7703},\n",
       " {'neg': 0.012, 'neu': 0.86, 'pos': 0.128, 'compound': 0.9923},\n",
       " {'neg': 0.087, 'neu': 0.643, 'pos': 0.27, 'compound': 0.6912},\n",
       " {'neg': 0.11, 'neu': 0.748, 'pos': 0.142, 'compound': 0.1264},\n",
       " {'neg': 0.0, 'neu': 0.588, 'pos': 0.412, 'compound': 0.9168},\n",
       " {'neg': 0.0, 'neu': 0.728, 'pos': 0.272, 'compound': 0.9472},\n",
       " {'neg': 0.054, 'neu': 0.69, 'pos': 0.256, 'compound': 0.8962},\n",
       " {'neg': 0.0, 'neu': 0.796, 'pos': 0.204, 'compound': 0.874},\n",
       " {'neg': 0.046, 'neu': 0.793, 'pos': 0.161, 'compound': 0.9341},\n",
       " {'neg': 0.063, 'neu': 0.524, 'pos': 0.413, 'compound': 0.9709},\n",
       " {'neg': 0.036, 'neu': 0.695, 'pos': 0.269, 'compound': 0.9468},\n",
       " {'neg': 0.074, 'neu': 0.715, 'pos': 0.212, 'compound': 0.8349},\n",
       " {'neg': 0.318, 'neu': 0.515, 'pos': 0.167, 'compound': -0.7184},\n",
       " {'neg': 0.0, 'neu': 0.905, 'pos': 0.095, 'compound': 0.6369},\n",
       " {'neg': 0.027, 'neu': 0.78, 'pos': 0.193, 'compound': 0.9913},\n",
       " {'neg': 0.0, 'neu': 0.767, 'pos': 0.233, 'compound': 0.8065},\n",
       " {'neg': 0.0, 'neu': 0.774, 'pos': 0.226, 'compound': 0.9796},\n",
       " {'neg': 0.0, 'neu': 0.839, 'pos': 0.161, 'compound': 0.8625},\n",
       " {'neg': 0.089, 'neu': 0.75, 'pos': 0.161, 'compound': 0.8201},\n",
       " {'neg': 0.088, 'neu': 0.537, 'pos': 0.375, 'compound': 0.755},\n",
       " {'neg': 0.031, 'neu': 0.764, 'pos': 0.205, 'compound': 0.9183},\n",
       " {'neg': 0.248, 'neu': 0.636, 'pos': 0.116, 'compound': -0.8174},\n",
       " {'neg': 0.0, 'neu': 0.642, 'pos': 0.358, 'compound': 0.8591},\n",
       " {'neg': 0.0, 'neu': 0.661, 'pos': 0.339, 'compound': 0.8481},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.0, 'neu': 0.83, 'pos': 0.17, 'compound': 0.8016},\n",
       " {'neg': 0.0, 'neu': 0.502, 'pos': 0.498, 'compound': 0.9677},\n",
       " {'neg': 0.0, 'neu': 0.638, 'pos': 0.362, 'compound': 0.9682},\n",
       " {'neg': 0.046, 'neu': 0.703, 'pos': 0.251, 'compound': 0.867},\n",
       " {'neg': 0.0, 'neu': 0.8, 'pos': 0.2, 'compound': 0.9885},\n",
       " {'neg': 0.0, 'neu': 0.787, 'pos': 0.213, 'compound': 0.7644},\n",
       " {'neg': 0.234, 'neu': 0.556, 'pos': 0.211, 'compound': 0.0},\n",
       " {'neg': 0.093, 'neu': 0.813, 'pos': 0.095, 'compound': 0.0258},\n",
       " {'neg': 0.215, 'neu': 0.697, 'pos': 0.088, 'compound': -0.6351},\n",
       " {'neg': 0.194, 'neu': 0.771, 'pos': 0.035, 'compound': -0.9058},\n",
       " {'neg': 0.0, 'neu': 0.691, 'pos': 0.309, 'compound': 0.8172},\n",
       " {'neg': 0.019, 'neu': 0.702, 'pos': 0.279, 'compound': 0.9622},\n",
       " {'neg': 0.0, 'neu': 0.954, 'pos': 0.046, 'compound': 0.6249},\n",
       " {'neg': 0.036, 'neu': 0.772, 'pos': 0.192, 'compound': 0.9477},\n",
       " {'neg': 0.0, 'neu': 0.713, 'pos': 0.287, 'compound': 0.9257},\n",
       " {'neg': 0.05, 'neu': 0.758, 'pos': 0.192, 'compound': 0.8316},\n",
       " {'neg': 0.016, 'neu': 0.879, 'pos': 0.105, 'compound': 0.8681},\n",
       " {'neg': 0.0, 'neu': 0.802, 'pos': 0.198, 'compound': 0.8555},\n",
       " {'neg': 0.0, 'neu': 0.815, 'pos': 0.185, 'compound': 0.7777},\n",
       " {'neg': 0.0, 'neu': 0.914, 'pos': 0.086, 'compound': 0.4118},\n",
       " {'neg': 0.0, 'neu': 0.722, 'pos': 0.278, 'compound': 0.8902},\n",
       " {'neg': 0.0, 'neu': 0.594, 'pos': 0.406, 'compound': 0.9612},\n",
       " {'neg': 0.07, 'neu': 0.799, 'pos': 0.131, 'compound': 0.9222},\n",
       " {'neg': 0.166, 'neu': 0.809, 'pos': 0.025, 'compound': -0.8957},\n",
       " {'neg': 0.0, 'neu': 0.784, 'pos': 0.216, 'compound': 0.8876},\n",
       " {'neg': 0.148, 'neu': 0.815, 'pos': 0.037, 'compound': -0.5983},\n",
       " {'neg': 0.035, 'neu': 0.754, 'pos': 0.211, 'compound': 0.9561},\n",
       " {'neg': 0.0, 'neu': 0.861, 'pos': 0.139, 'compound': 0.4404},\n",
       " {'neg': 0.223, 'neu': 0.68, 'pos': 0.096, 'compound': -0.3314},\n",
       " {'neg': 0.055, 'neu': 0.687, 'pos': 0.258, 'compound': 0.9106},\n",
       " {'neg': 0.017, 'neu': 0.821, 'pos': 0.161, 'compound': 0.9576},\n",
       " {'neg': 0.0, 'neu': 0.806, 'pos': 0.194, 'compound': 0.7717},\n",
       " {'neg': 0.029, 'neu': 0.817, 'pos': 0.154, 'compound': 0.7845},\n",
       " {'neg': 0.0, 'neu': 0.761, 'pos': 0.239, 'compound': 0.9337},\n",
       " {'neg': 0.0, 'neu': 0.739, 'pos': 0.261, 'compound': 0.9741},\n",
       " {'neg': 0.0, 'neu': 0.617, 'pos': 0.383, 'compound': 0.9876},\n",
       " {'neg': 0.04, 'neu': 0.786, 'pos': 0.174, 'compound': 0.9847},\n",
       " {'neg': 0.0, 'neu': 0.73, 'pos': 0.27, 'compound': 0.9516},\n",
       " {'neg': 0.083, 'neu': 0.751, 'pos': 0.166, 'compound': 0.8044},\n",
       " {'neg': 0.108, 'neu': 0.593, 'pos': 0.299, 'compound': 0.8655},\n",
       " {'neg': 0.0, 'neu': 0.771, 'pos': 0.229, 'compound': 0.9179},\n",
       " {'neg': 0.0, 'neu': 0.829, 'pos': 0.171, 'compound': 0.8519},\n",
       " {'neg': 0.0, 'neu': 0.926, 'pos': 0.074, 'compound': 0.7383},\n",
       " {'neg': 0.0, 'neu': 0.887, 'pos': 0.113, 'compound': 0.6369},\n",
       " {'neg': 0.0, 'neu': 0.728, 'pos': 0.272, 'compound': 0.87},\n",
       " {'neg': 0.072, 'neu': 0.781, 'pos': 0.147, 'compound': 0.9307},\n",
       " {'neg': 0.078, 'neu': 0.793, 'pos': 0.129, 'compound': 0.5176},\n",
       " {'neg': 0.054, 'neu': 0.69, 'pos': 0.257, 'compound': 0.9683},\n",
       " {'neg': 0.0, 'neu': 0.616, 'pos': 0.384, 'compound': 0.9603},\n",
       " {'neg': 0.044, 'neu': 0.898, 'pos': 0.058, 'compound': 0.1882},\n",
       " {'neg': 0.055, 'neu': 0.873, 'pos': 0.072, 'compound': 0.0935},\n",
       " {'neg': 0.077, 'neu': 0.78, 'pos': 0.143, 'compound': 0.3699},\n",
       " {'neg': 0.042, 'neu': 0.763, 'pos': 0.195, 'compound': 0.9883},\n",
       " {'neg': 0.0, 'neu': 0.713, 'pos': 0.287, 'compound': 0.967},\n",
       " {'neg': 0.0, 'neu': 0.737, 'pos': 0.263, 'compound': 0.8531},\n",
       " {'neg': 0.0, 'neu': 0.845, 'pos': 0.155, 'compound': 0.6908},\n",
       " {'neg': 0.034, 'neu': 0.743, 'pos': 0.223, 'compound': 0.9873},\n",
       " {'neg': 0.054, 'neu': 0.782, 'pos': 0.164, 'compound': 0.9337},\n",
       " {'neg': 0.0, 'neu': 0.5, 'pos': 0.5, 'compound': 0.943},\n",
       " {'neg': 0.0, 'neu': 0.603, 'pos': 0.397, 'compound': 0.8811},\n",
       " {'neg': 0.0, 'neu': 0.699, 'pos': 0.301, 'compound': 0.9619},\n",
       " {'neg': 0.082, 'neu': 0.854, 'pos': 0.064, 'compound': -0.4854},\n",
       " {'neg': 0.0, 'neu': 0.684, 'pos': 0.316, 'compound': 0.926},\n",
       " {'neg': 0.0, 'neu': 0.564, 'pos': 0.436, 'compound': 0.9642},\n",
       " {'neg': 0.045, 'neu': 0.717, 'pos': 0.239, 'compound': 0.8455},\n",
       " {'neg': 0.066, 'neu': 0.743, 'pos': 0.19, 'compound': 0.9481},\n",
       " {'neg': 0.08, 'neu': 0.821, 'pos': 0.099, 'compound': 0.4883},\n",
       " {'neg': 0.037, 'neu': 0.87, 'pos': 0.093, 'compound': 0.34},\n",
       " {'neg': 0.099, 'neu': 0.794, 'pos': 0.108, 'compound': 0.5983},\n",
       " {'neg': 0.019, 'neu': 0.868, 'pos': 0.113, 'compound': 0.8443},\n",
       " {'neg': 0.0, 'neu': 0.838, 'pos': 0.162, 'compound': 0.7823},\n",
       " {'neg': 0.0, 'neu': 0.772, 'pos': 0.228, 'compound': 0.9606},\n",
       " {'neg': 0.009, 'neu': 0.845, 'pos': 0.147, 'compound': 0.9874},\n",
       " {'neg': 0.008, 'neu': 0.818, 'pos': 0.174, 'compound': 0.9926},\n",
       " {'neg': 0.049, 'neu': 0.951, 'pos': 0.0, 'compound': -0.3595},\n",
       " {'neg': 0.0, 'neu': 0.957, 'pos': 0.043, 'compound': 0.25},\n",
       " {'neg': 0.051, 'neu': 0.676, 'pos': 0.273, 'compound': 0.9749},\n",
       " {'neg': 0.0, 'neu': 0.565, 'pos': 0.435, 'compound': 0.9649},\n",
       " {'neg': 0.0, 'neu': 0.686, 'pos': 0.314, 'compound': 0.7506},\n",
       " {'neg': 0.013, 'neu': 0.75, 'pos': 0.237, 'compound': 0.9828},\n",
       " {'neg': 0.0, 'neu': 0.585, 'pos': 0.415, 'compound': 0.9095},\n",
       " {'neg': 0.066, 'neu': 0.614, 'pos': 0.32, 'compound': 0.9684},\n",
       " {'neg': 0.034, 'neu': 0.728, 'pos': 0.238, 'compound': 0.8555},\n",
       " {'neg': 0.0, 'neu': 0.823, 'pos': 0.177, 'compound': 0.6239},\n",
       " {'neg': 0.245, 'neu': 0.652, 'pos': 0.103, 'compound': -0.3855},\n",
       " {'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'compound': 0.9935},\n",
       " {'neg': 0.022, 'neu': 0.728, 'pos': 0.249, 'compound': 0.9451},\n",
       " {'neg': 0.0, 'neu': 0.605, 'pos': 0.395, 'compound': 0.9079},\n",
       " {'neg': 0.0, 'neu': 0.862, 'pos': 0.138, 'compound': 0.3384},\n",
       " {'neg': 0.088, 'neu': 0.767, 'pos': 0.145, 'compound': 0.4516},\n",
       " {'neg': 0.0, 'neu': 0.761, 'pos': 0.239, 'compound': 0.8547},\n",
       " {'neg': 0.0, 'neu': 0.818, 'pos': 0.182, 'compound': 0.9224},\n",
       " {'neg': 0.0, 'neu': 0.909, 'pos': 0.091, 'compound': 0.296},\n",
       " {'neg': 0.179, 'neu': 0.707, 'pos': 0.114, 'compound': -0.3723},\n",
       " {'neg': 0.0, 'neu': 0.861, 'pos': 0.139, 'compound': 0.9598},\n",
       " {'neg': 0.0, 'neu': 0.763, 'pos': 0.237, 'compound': 0.9788},\n",
       " {'neg': 0.055, 'neu': 0.704, 'pos': 0.241, 'compound': 0.9287},\n",
       " {'neg': 0.0, 'neu': 0.717, 'pos': 0.283, 'compound': 0.9367},\n",
       " {'neg': 0.056, 'neu': 0.855, 'pos': 0.089, 'compound': 0.5976},\n",
       " {'neg': 0.1, 'neu': 0.645, 'pos': 0.254, 'compound': 0.6486},\n",
       " {'neg': 0.0, 'neu': 0.788, 'pos': 0.212, 'compound': 0.9743},\n",
       " {'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'compound': 0.9725},\n",
       " {'neg': 0.059, 'neu': 0.799, 'pos': 0.142, 'compound': 0.7833},\n",
       " {'neg': 0.025, 'neu': 0.762, 'pos': 0.212, 'compound': 0.9848},\n",
       " {'neg': 0.041, 'neu': 0.904, 'pos': 0.055, 'compound': 0.128},\n",
       " {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.9811},\n",
       " {'neg': 0.067, 'neu': 0.887, 'pos': 0.046, 'compound': -0.2003},\n",
       " {'neg': 0.0, 'neu': 0.62, 'pos': 0.38, 'compound': 0.8922},\n",
       " {'neg': 0.0, 'neu': 0.715, 'pos': 0.285, 'compound': 0.8213},\n",
       " {'neg': 0.082, 'neu': 0.753, 'pos': 0.165, 'compound': 0.7866},\n",
       " {'neg': 0.016, 'neu': 0.816, 'pos': 0.168, 'compound': 0.9062},\n",
       " {'neg': 0.0, 'neu': 0.732, 'pos': 0.268, 'compound': 0.8555},\n",
       " {'neg': 0.017, 'neu': 0.745, 'pos': 0.238, 'compound': 0.9887},\n",
       " {'neg': 0.147, 'neu': 0.641, 'pos': 0.212, 'compound': 0.7285},\n",
       " {'neg': 0.138, 'neu': 0.763, 'pos': 0.099, 'compound': -0.8157},\n",
       " {'neg': 0.12, 'neu': 0.642, 'pos': 0.237, 'compound': 0.6369},\n",
       " {'neg': 0.052, 'neu': 0.643, 'pos': 0.304, 'compound': 0.9312},\n",
       " {'neg': 0.079, 'neu': 0.779, 'pos': 0.142, 'compound': 0.7184},\n",
       " {'neg': 0.0, 'neu': 0.895, 'pos': 0.105, 'compound': 0.5859},\n",
       " {'neg': 0.029, 'neu': 0.81, 'pos': 0.16, 'compound': 0.935},\n",
       " {'neg': 0.023, 'neu': 0.852, 'pos': 0.125, 'compound': 0.9321},\n",
       " {'neg': 0.0, 'neu': 0.681, 'pos': 0.319, 'compound': 0.8934},\n",
       " {'neg': 0.092, 'neu': 0.736, 'pos': 0.172, 'compound': 0.8074},\n",
       " {'neg': 0.0, 'neu': 0.689, 'pos': 0.311, 'compound': 0.8221},\n",
       " {'neg': 0.0, 'neu': 0.646, 'pos': 0.354, 'compound': 0.899},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.011, 'neu': 0.736, 'pos': 0.253, 'compound': 0.9839},\n",
       " {'neg': 0.0, 'neu': 0.684, 'pos': 0.316, 'compound': 0.8658},\n",
       " {'neg': 0.032, 'neu': 0.751, 'pos': 0.217, 'compound': 0.9954},\n",
       " {'neg': 0.086, 'neu': 0.818, 'pos': 0.096, 'compound': 0.4246},\n",
       " {'neg': 0.045, 'neu': 0.737, 'pos': 0.218, 'compound': 0.8583},\n",
       " {'neg': 0.038, 'neu': 0.73, 'pos': 0.232, 'compound': 0.9844},\n",
       " {'neg': 0.0, 'neu': 0.782, 'pos': 0.218, 'compound': 0.8539},\n",
       " {'neg': 0.051, 'neu': 0.79, 'pos': 0.16, 'compound': 0.9538},\n",
       " {'neg': 0.047, 'neu': 0.803, 'pos': 0.15, 'compound': 0.9966},\n",
       " {'neg': 0.03, 'neu': 0.71, 'pos': 0.26, 'compound': 0.9757},\n",
       " {'neg': 0.0, 'neu': 0.756, 'pos': 0.244, 'compound': 0.9523},\n",
       " {'neg': 0.069, 'neu': 0.766, 'pos': 0.164, 'compound': 0.91},\n",
       " {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'compound': 0.6705},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.045, 'neu': 0.824, 'pos': 0.13, 'compound': 0.6114},\n",
       " {'neg': 0.065, 'neu': 0.918, 'pos': 0.018, 'compound': -0.7641},\n",
       " {'neg': 0.069, 'neu': 0.757, 'pos': 0.174, 'compound': 0.7028},\n",
       " {'neg': 0.236, 'neu': 0.72, 'pos': 0.045, 'compound': -0.9032},\n",
       " {'neg': 0.128, 'neu': 0.872, 'pos': 0.0, 'compound': -0.5209},\n",
       " {'neg': 0.045, 'neu': 0.788, 'pos': 0.168, 'compound': 0.9989},\n",
       " {'neg': 0.087, 'neu': 0.844, 'pos': 0.069, 'compound': -0.2092},\n",
       " {'neg': 0.013, 'neu': 0.935, 'pos': 0.052, 'compound': 0.4926},\n",
       " {'neg': 0.044, 'neu': 0.752, 'pos': 0.203, 'compound': 0.9183},\n",
       " {'neg': 0.14, 'neu': 0.742, 'pos': 0.117, 'compound': -0.1779},\n",
       " {'neg': 0.193, 'neu': 0.658, 'pos': 0.149, 'compound': -0.417},\n",
       " {'neg': 0.357, 'neu': 0.643, 'pos': 0.0, 'compound': -0.8745},\n",
       " {'neg': 0.054, 'neu': 0.813, 'pos': 0.133, 'compound': 0.8161},\n",
       " {'neg': 0.142, 'neu': 0.746, 'pos': 0.111, 'compound': -0.5089},\n",
       " {'neg': 0.0, 'neu': 0.659, 'pos': 0.341, 'compound': 0.8855},\n",
       " {'neg': 0.0, 'neu': 0.828, 'pos': 0.172, 'compound': 0.4404},\n",
       " {'neg': 0.0, 'neu': 0.867, 'pos': 0.133, 'compound': 0.7555},\n",
       " {'neg': 0.144, 'neu': 0.856, 'pos': 0.0, 'compound': -0.8144},\n",
       " {'neg': 0.0, 'neu': 0.801, 'pos': 0.199, 'compound': 0.921},\n",
       " {'neg': 0.0, 'neu': 0.916, 'pos': 0.084, 'compound': 0.9136},\n",
       " {'neg': 0.0, 'neu': 0.927, 'pos': 0.073, 'compound': 0.5106},\n",
       " {'neg': 0.0, 'neu': 0.93, 'pos': 0.07, 'compound': 0.3252},\n",
       " {'neg': 0.096, 'neu': 0.745, 'pos': 0.158, 'compound': 0.25},\n",
       " {'neg': 0.269, 'neu': 0.66, 'pos': 0.071, 'compound': -0.8004},\n",
       " {'neg': 0.115, 'neu': 0.755, 'pos': 0.13, 'compound': 0.1104},\n",
       " {'neg': 0.0, 'neu': 0.718, 'pos': 0.282, 'compound': 0.9612},\n",
       " {'neg': 0.09, 'neu': 0.847, 'pos': 0.062, 'compound': -0.7754},\n",
       " {'neg': 0.093, 'neu': 0.876, 'pos': 0.031, 'compound': -0.664},\n",
       " {'neg': 0.12, 'neu': 0.795, 'pos': 0.084, 'compound': -0.8814},\n",
       " {'neg': 0.026, 'neu': 0.797, 'pos': 0.177, 'compound': 0.9217},\n",
       " {'neg': 0.0, 'neu': 0.832, 'pos': 0.168, 'compound': 0.8172},\n",
       " {'neg': 0.0, 'neu': 0.608, 'pos': 0.392, 'compound': 0.9859},\n",
       " {'neg': 0.177, 'neu': 0.823, 'pos': 0.0, 'compound': -0.9679},\n",
       " {'neg': 0.114, 'neu': 0.735, 'pos': 0.151, 'compound': 0.6749},\n",
       " {'neg': 0.026, 'neu': 0.897, 'pos': 0.077, 'compound': 0.3291},\n",
       " {'neg': 0.0, 'neu': 0.803, 'pos': 0.197, 'compound': 0.8422},\n",
       " {'neg': 0.0, 'neu': 0.751, 'pos': 0.249, 'compound': 0.9118},\n",
       " {'neg': 0.01, 'neu': 0.817, 'pos': 0.172, 'compound': 0.9945},\n",
       " {'neg': 0.018, 'neu': 0.818, 'pos': 0.164, 'compound': 0.9915},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.009, 'neu': 0.845, 'pos': 0.147, 'compound': 0.9874},\n",
       " {'neg': 0.0, 'neu': 0.601, 'pos': 0.399, 'compound': 0.8834},\n",
       " {'neg': 0.019, 'neu': 0.835, 'pos': 0.146, 'compound': 0.8291},\n",
       " {'neg': 0.0, 'neu': 0.781, 'pos': 0.219, 'compound': 0.9749},\n",
       " {'neg': 0.0, 'neu': 0.536, 'pos': 0.464, 'compound': 0.9307},\n",
       " {'neg': 0.0, 'neu': 0.802, 'pos': 0.198, 'compound': 0.6369},\n",
       " {'neg': 0.0, 'neu': 0.857, 'pos': 0.143, 'compound': 0.6124},\n",
       " {'neg': 0.043, 'neu': 0.649, 'pos': 0.308, 'compound': 0.9254},\n",
       " {'neg': 0.0, 'neu': 0.907, 'pos': 0.093, 'compound': 0.4019},\n",
       " {'neg': 0.091, 'neu': 0.909, 'pos': 0.0, 'compound': -0.2732},\n",
       " {'neg': 0.081, 'neu': 0.785, 'pos': 0.134, 'compound': 0.25},\n",
       " {'neg': 0.083, 'neu': 0.721, 'pos': 0.196, 'compound': 0.8132},\n",
       " {'neg': 0.0, 'neu': 0.779, 'pos': 0.221, 'compound': 0.9323},\n",
       " {'neg': 0.116, 'neu': 0.75, 'pos': 0.135, 'compound': 0.2169},\n",
       " {'neg': 0.0, 'neu': 0.81, 'pos': 0.19, 'compound': 0.9042},\n",
       " {'neg': 0.016, 'neu': 0.861, 'pos': 0.123, 'compound': 0.846},\n",
       " {'neg': 0.0, 'neu': 0.826, 'pos': 0.174, 'compound': 0.6369},\n",
       " {'neg': 0.0, 'neu': 0.889, 'pos': 0.111, 'compound': 0.9446},\n",
       " {'neg': 0.056, 'neu': 0.726, 'pos': 0.217, 'compound': 0.9739},\n",
       " {'neg': 0.042, 'neu': 0.857, 'pos': 0.101, 'compound': 0.8112},\n",
       " {'neg': 0.031, 'neu': 0.831, 'pos': 0.138, 'compound': 0.8932},\n",
       " {'neg': 0.022, 'neu': 0.845, 'pos': 0.133, 'compound': 0.9206},\n",
       " {'neg': 0.0, 'neu': 0.797, 'pos': 0.203, 'compound': 0.8567},\n",
       " {'neg': 0.063, 'neu': 0.859, 'pos': 0.078, 'compound': 0.4101},\n",
       " {'neg': 0.146, 'neu': 0.571, 'pos': 0.283, 'compound': 0.8802},\n",
       " {'neg': 0.051, 'neu': 0.693, 'pos': 0.257, 'compound': 0.9842},\n",
       " {'neg': 0.186, 'neu': 0.665, 'pos': 0.149, 'compound': -0.4503},\n",
       " {'neg': 0.05, 'neu': 0.639, 'pos': 0.311, 'compound': 0.9834},\n",
       " {'neg': 0.007, 'neu': 0.827, 'pos': 0.166, 'compound': 0.9938},\n",
       " {'neg': 0.009, 'neu': 0.845, 'pos': 0.147, 'compound': 0.9874},\n",
       " {'neg': 0.0, 'neu': 0.83, 'pos': 0.17, 'compound': 0.8481},\n",
       " {'neg': 0.0, 'neu': 0.837, 'pos': 0.163, 'compound': 0.8715},\n",
       " {'neg': 0.085, 'neu': 0.682, 'pos': 0.233, 'compound': 0.5705},\n",
       " {'neg': 0.014, 'neu': 0.733, 'pos': 0.253, 'compound': 0.9779},\n",
       " {'neg': 0.061, 'neu': 0.875, 'pos': 0.063, 'compound': 0.0384},\n",
       " {'neg': 0.02, 'neu': 0.834, 'pos': 0.146, 'compound': 0.9668},\n",
       " {'neg': 0.088, 'neu': 0.713, 'pos': 0.2, 'compound': 0.6222},\n",
       " {'neg': 0.052, 'neu': 0.758, 'pos': 0.191, 'compound': 0.8139},\n",
       " {'neg': 0.147, 'neu': 0.786, 'pos': 0.067, 'compound': -0.6341},\n",
       " {'neg': 0.0, 'neu': 0.746, 'pos': 0.254, 'compound': 0.9585},\n",
       " {'neg': 0.039, 'neu': 0.801, 'pos': 0.159, 'compound': 0.9169},\n",
       " {'neg': 0.024, 'neu': 0.706, 'pos': 0.27, 'compound': 0.982},\n",
       " {'neg': 0.0, 'neu': 0.7, 'pos': 0.3, 'compound': 0.8901},\n",
       " {'neg': 0.0, 'neu': 0.694, 'pos': 0.306, 'compound': 0.8979},\n",
       " {'neg': 0.0, 'neu': 0.875, 'pos': 0.125, 'compound': 0.5067},\n",
       " {'neg': 0.0, 'neu': 0.599, 'pos': 0.401, 'compound': 0.9586},\n",
       " {'neg': 0.075, 'neu': 0.817, 'pos': 0.108, 'compound': 0.8004},\n",
       " {'neg': 0.157, 'neu': 0.788, 'pos': 0.055, 'compound': -0.8088},\n",
       " {'neg': 0.039, 'neu': 0.627, 'pos': 0.334, 'compound': 0.969},\n",
       " {'neg': 0.077, 'neu': 0.707, 'pos': 0.217, 'compound': 0.983},\n",
       " {'neg': 0.0, 'neu': 0.867, 'pos': 0.133, 'compound': 0.9697},\n",
       " {'neg': 0.028, 'neu': 0.697, 'pos': 0.275, 'compound': 0.9745},\n",
       " {'neg': 0.078, 'neu': 0.792, 'pos': 0.13, 'compound': 0.5574},\n",
       " {'neg': 0.061, 'neu': 0.674, 'pos': 0.265, 'compound': 0.8678},\n",
       " {'neg': 0.053, 'neu': 0.947, 'pos': 0.0, 'compound': -0.3919},\n",
       " {'neg': 0.13, 'neu': 0.587, 'pos': 0.283, 'compound': 0.9169},\n",
       " {'neg': 0.029, 'neu': 0.918, 'pos': 0.053, 'compound': 0.4166},\n",
       " {'neg': 0.0, 'neu': 0.69, 'pos': 0.31, 'compound': 0.933},\n",
       " {'neg': 0.01, 'neu': 0.866, 'pos': 0.124, 'compound': 0.9167},\n",
       " {'neg': 0.042, 'neu': 0.775, 'pos': 0.183, 'compound': 0.9739},\n",
       " {'neg': 0.213, 'neu': 0.608, 'pos': 0.179, 'compound': -0.0997},\n",
       " {'neg': 0.187, 'neu': 0.728, 'pos': 0.086, 'compound': -0.9194},\n",
       " {'neg': 0.05, 'neu': 0.856, 'pos': 0.093, 'compound': 0.5597},\n",
       " {'neg': 0.095, 'neu': 0.589, 'pos': 0.316, 'compound': 0.7074},\n",
       " {'neg': 0.034, 'neu': 0.722, 'pos': 0.244, 'compound': 0.9375},\n",
       " {'neg': 0.03, 'neu': 0.734, 'pos': 0.236, 'compound': 0.9554},\n",
       " {'neg': 0.0, 'neu': 0.936, 'pos': 0.064, 'compound': 0.0772},\n",
       " {'neg': 0.0, 'neu': 0.799, 'pos': 0.201, 'compound': 0.9545},\n",
       " {'neg': 0.031, 'neu': 0.793, 'pos': 0.176, 'compound': 0.8316},\n",
       " {'neg': 0.02, 'neu': 0.685, 'pos': 0.295, 'compound': 0.9814},\n",
       " {'neg': 0.076, 'neu': 0.585, 'pos': 0.339, 'compound': 0.8595},\n",
       " {'neg': 0.0, 'neu': 0.694, 'pos': 0.306, 'compound': 0.9151},\n",
       " {'neg': 0.021, 'neu': 0.83, 'pos': 0.149, 'compound': 0.9249},\n",
       " {'neg': 0.096, 'neu': 0.7, 'pos': 0.204, 'compound': 0.5719},\n",
       " {'neg': 0.0, 'neu': 0.89, 'pos': 0.11, 'compound': 0.3818},\n",
       " {'neg': 0.093, 'neu': 0.77, 'pos': 0.137, 'compound': 0.9356},\n",
       " {'neg': 0.0, 'neu': 0.814, 'pos': 0.186, 'compound': 0.8395},\n",
       " {'neg': 0.043, 'neu': 0.867, 'pos': 0.09, 'compound': 0.8845},\n",
       " {'neg': 0.029, 'neu': 0.9, 'pos': 0.071, 'compound': 0.9377},\n",
       " {'neg': 0.038, 'neu': 0.89, 'pos': 0.072, 'compound': 0.7588},\n",
       " {'neg': 0.0, 'neu': 0.909, 'pos': 0.091, 'compound': 0.4404},\n",
       " {'neg': 0.0, 'neu': 0.614, 'pos': 0.386, 'compound': 0.8932},\n",
       " {'neg': 0.0, 'neu': 0.654, 'pos': 0.346, 'compound': 0.9022},\n",
       " {'neg': 0.0, 'neu': 0.811, 'pos': 0.189, 'compound': 0.6808},\n",
       " {'neg': 0.046, 'neu': 0.637, 'pos': 0.316, 'compound': 0.906},\n",
       " {'neg': 0.02, 'neu': 0.777, 'pos': 0.203, 'compound': 0.9601},\n",
       " {'neg': 0.228, 'neu': 0.545, 'pos': 0.228, 'compound': 0.25},\n",
       " {'neg': 0.0, 'neu': 0.788, 'pos': 0.212, 'compound': 0.8439},\n",
       " {'neg': 0.106, 'neu': 0.7, 'pos': 0.193, 'compound': 0.624},\n",
       " {'neg': 0.018, 'neu': 0.882, 'pos': 0.099, 'compound': 0.9621},\n",
       " {'neg': 0.0, 'neu': 0.774, 'pos': 0.226, 'compound': 0.9757},\n",
       " {'neg': 0.0, 'neu': 0.697, 'pos': 0.303, 'compound': 0.9476},\n",
       " {'neg': 0.0, 'neu': 0.816, 'pos': 0.184, 'compound': 0.7263},\n",
       " {'neg': 0.05, 'neu': 0.69, 'pos': 0.26, 'compound': 0.8689},\n",
       " {'neg': 0.0, 'neu': 0.744, 'pos': 0.256, 'compound': 0.8225},\n",
       " {'neg': 0.077, 'neu': 0.634, 'pos': 0.289, 'compound': 0.7906},\n",
       " {'neg': 0.0, 'neu': 0.604, 'pos': 0.396, 'compound': 0.8957},\n",
       " {'neg': 0.046, 'neu': 0.854, 'pos': 0.1, 'compound': 0.8074},\n",
       " {'neg': 0.065, 'neu': 0.88, 'pos': 0.055, 'compound': -0.1263},\n",
       " {'neg': 0.062, 'neu': 0.708, 'pos': 0.231, 'compound': 0.9313},\n",
       " {'neg': 0.0, 'neu': 0.797, 'pos': 0.203, 'compound': 0.9098},\n",
       " {'neg': 0.095, 'neu': 0.773, 'pos': 0.132, 'compound': 0.6588},\n",
       " {'neg': 0.044, 'neu': 0.72, 'pos': 0.237, 'compound': 0.9566},\n",
       " {'neg': 0.049, 'neu': 0.685, 'pos': 0.266, 'compound': 0.749},\n",
       " {'neg': 0.0, 'neu': 0.746, 'pos': 0.254, 'compound': 0.8896},\n",
       " {'neg': 0.126, 'neu': 0.686, 'pos': 0.188, 'compound': 0.879},\n",
       " {'neg': 0.052, 'neu': 0.829, 'pos': 0.119, 'compound': 0.491},\n",
       " {'neg': 0.121, 'neu': 0.879, 'pos': 0.0, 'compound': -0.509},\n",
       " {'neg': 0.06, 'neu': 0.753, 'pos': 0.187, 'compound': 0.6369},\n",
       " {'neg': 0.184, 'neu': 0.816, 'pos': 0.0, 'compound': -0.5423},\n",
       " {'neg': 0.141, 'neu': 0.411, 'pos': 0.449, 'compound': 0.9354},\n",
       " {'neg': 0.0, 'neu': 0.56, 'pos': 0.44, 'compound': 0.9318},\n",
       " {'neg': 0.015, 'neu': 0.844, 'pos': 0.141, 'compound': 0.9605},\n",
       " {'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'compound': 0.7183},\n",
       " {'neg': 0.051, 'neu': 0.743, 'pos': 0.206, 'compound': 0.9694},\n",
       " {'neg': 0.041, 'neu': 0.716, 'pos': 0.243, 'compound': 0.8878},\n",
       " {'neg': 0.045, 'neu': 0.817, 'pos': 0.138, 'compound': 0.8189},\n",
       " {'neg': 0.035, 'neu': 0.826, 'pos': 0.139, 'compound': 0.7845},\n",
       " {'neg': 0.0, 'neu': 0.916, 'pos': 0.084, 'compound': 0.8356},\n",
       " {'neg': 0.0, 'neu': 0.86, 'pos': 0.14, 'compound': 0.7345},\n",
       " {'neg': 0.0, 'neu': 0.705, 'pos': 0.295, 'compound': 0.8718},\n",
       " {'neg': 0.0, 'neu': 0.734, 'pos': 0.266, 'compound': 0.9118},\n",
       " {'neg': 0.037, 'neu': 0.89, 'pos': 0.073, 'compound': 0.3585},\n",
       " {'neg': 0.052, 'neu': 0.759, 'pos': 0.189, 'compound': 0.9103},\n",
       " {'neg': 0.076, 'neu': 0.698, 'pos': 0.227, 'compound': 0.911},\n",
       " {'neg': 0.052, 'neu': 0.687, 'pos': 0.261, 'compound': 0.905},\n",
       " {'neg': 0.055, 'neu': 0.731, 'pos': 0.214, 'compound': 0.7414},\n",
       " {'neg': 0.067, 'neu': 0.672, 'pos': 0.261, 'compound': 0.7467},\n",
       " {'neg': 0.0, 'neu': 0.623, 'pos': 0.377, 'compound': 0.9603},\n",
       " {'neg': 0.0, 'neu': 0.552, 'pos': 0.448, 'compound': 0.9393},\n",
       " {'neg': 0.059, 'neu': 0.536, 'pos': 0.405, 'compound': 0.9312},\n",
       " {'neg': 0.091, 'neu': 0.563, 'pos': 0.345, 'compound': 0.7824},\n",
       " {'neg': 0.07, 'neu': 0.648, 'pos': 0.282, 'compound': 0.8176},\n",
       " {'neg': 0.0, 'neu': 0.635, 'pos': 0.365, 'compound': 0.7845},\n",
       " {'neg': 0.0, 'neu': 0.717, 'pos': 0.283, 'compound': 0.8172},\n",
       " {'neg': 0.028, 'neu': 0.865, 'pos': 0.107, 'compound': 0.9035},\n",
       " {'neg': 0.0, 'neu': 0.88, 'pos': 0.12, 'compound': 0.9131},\n",
       " {'neg': 0.029, 'neu': 0.911, 'pos': 0.06, 'compound': 0.3116},\n",
       " {'neg': 0.048, 'neu': 0.773, 'pos': 0.179, 'compound': 0.9605},\n",
       " {'neg': 0.0, 'neu': 0.85, 'pos': 0.15, 'compound': 0.8725},\n",
       " {'neg': 0.0, 'neu': 0.76, 'pos': 0.24, 'compound': 0.9001},\n",
       " {'neg': 0.088, 'neu': 0.669, 'pos': 0.243, 'compound': 0.9403},\n",
       " {'neg': 0.09, 'neu': 0.887, 'pos': 0.023, 'compound': -0.6607},\n",
       " {'neg': 0.075, 'neu': 0.858, 'pos': 0.066, 'compound': -0.1045},\n",
       " {'neg': 0.087, 'neu': 0.774, 'pos': 0.139, 'compound': 0.6675},\n",
       " {'neg': 0.102, 'neu': 0.824, 'pos': 0.073, 'compound': -0.2617},\n",
       " {'neg': 0.024, 'neu': 0.75, 'pos': 0.226, 'compound': 0.9318},\n",
       " {'neg': 0.029, 'neu': 0.714, 'pos': 0.256, 'compound': 0.9449},\n",
       " {'neg': 0.073, 'neu': 0.757, 'pos': 0.17, 'compound': 0.8395},\n",
       " {'neg': 0.091, 'neu': 0.779, 'pos': 0.13, 'compound': 0.224},\n",
       " {'neg': 0.0, 'neu': 0.738, 'pos': 0.262, 'compound': 0.6901},\n",
       " {'neg': 0.028, 'neu': 0.891, 'pos': 0.081, 'compound': 0.7227},\n",
       " {'neg': 0.04, 'neu': 0.577, 'pos': 0.383, 'compound': 0.9168},\n",
       " {'neg': 0.0, 'neu': 0.864, 'pos': 0.136, 'compound': 0.3987},\n",
       " {'neg': 0.0, 'neu': 0.617, 'pos': 0.383, 'compound': 0.9825},\n",
       " {'neg': 0.0, 'neu': 0.915, 'pos': 0.085, 'compound': 0.6697},\n",
       " {'neg': 0.1, 'neu': 0.709, 'pos': 0.191, 'compound': 0.6886},\n",
       " {'neg': 0.0, 'neu': 0.805, 'pos': 0.195, 'compound': 0.9022},\n",
       " {'neg': 0.109, 'neu': 0.785, 'pos': 0.106, 'compound': 0.2249},\n",
       " {'neg': 0.0, 'neu': 0.442, 'pos': 0.558, 'compound': 0.9764},\n",
       " {'neg': 0.054, 'neu': 0.828, 'pos': 0.118, 'compound': 0.5632},\n",
       " {'neg': 0.022, 'neu': 0.88, 'pos': 0.098, 'compound': 0.9257},\n",
       " {'neg': 0.176, 'neu': 0.685, 'pos': 0.139, 'compound': -0.4376},\n",
       " {'neg': 0.083, 'neu': 0.822, 'pos': 0.095, 'compound': -0.409},\n",
       " {'neg': 0.135, 'neu': 0.758, 'pos': 0.108, 'compound': -0.3625},\n",
       " {'neg': 0.247, 'neu': 0.562, 'pos': 0.191, 'compound': -0.7067},\n",
       " {'neg': 0.154, 'neu': 0.754, 'pos': 0.091, 'compound': -0.9047},\n",
       " {'neg': 0.126, 'neu': 0.821, 'pos': 0.053, 'compound': -0.9321},\n",
       " {'neg': 0.0, 'neu': 0.834, 'pos': 0.166, 'compound': 0.8745},\n",
       " {'neg': 0.039, 'neu': 0.722, 'pos': 0.239, 'compound': 0.9113},\n",
       " {'neg': 0.049, 'neu': 0.86, 'pos': 0.091, 'compound': 0.8617},\n",
       " {'neg': 0.016, 'neu': 0.858, 'pos': 0.126, 'compound': 0.9909},\n",
       " {'neg': 0.092, 'neu': 0.838, 'pos': 0.07, 'compound': -0.2047},\n",
       " {'neg': 0.012, 'neu': 0.875, 'pos': 0.113, 'compound': 0.9834},\n",
       " {'neg': 0.22, 'neu': 0.78, 'pos': 0.0, 'compound': -0.7351},\n",
       " {'neg': 0.152, 'neu': 0.664, 'pos': 0.185, 'compound': 0.1779},\n",
       " {'neg': 0.354, 'neu': 0.566, 'pos': 0.079, 'compound': -0.8067},\n",
       " {'neg': 0.0, 'neu': 0.823, 'pos': 0.177, 'compound': 0.9916},\n",
       " {'neg': 0.0, 'neu': 0.786, 'pos': 0.214, 'compound': 0.9807},\n",
       " {'neg': 0.023, 'neu': 0.796, 'pos': 0.18, 'compound': 0.9527},\n",
       " {'neg': 0.0, 'neu': 0.598, 'pos': 0.402, 'compound': 0.902},\n",
       " {'neg': 0.0, 'neu': 0.794, 'pos': 0.206, 'compound': 0.9097},\n",
       " {'neg': 0.0, 'neu': 0.69, 'pos': 0.31, 'compound': 0.8934},\n",
       " {'neg': 0.0, 'neu': 0.699, 'pos': 0.301, 'compound': 0.7841},\n",
       " {'neg': 0.0, 'neu': 0.842, 'pos': 0.158, 'compound': 0.7768},\n",
       " {'neg': 0.216, 'neu': 0.727, 'pos': 0.056, 'compound': -0.8049},\n",
       " {'neg': 0.038, 'neu': 0.789, 'pos': 0.173, 'compound': 0.9582},\n",
       " {'neg': 0.14, 'neu': 0.663, 'pos': 0.197, 'compound': 0.617},\n",
       " {'neg': 0.0, 'neu': 0.789, 'pos': 0.211, 'compound': 0.9001},\n",
       " {'neg': 0.023, 'neu': 0.807, 'pos': 0.169, 'compound': 0.9011},\n",
       " {'neg': 0.0, 'neu': 0.749, 'pos': 0.251, 'compound': 0.9688},\n",
       " {'neg': 0.0, 'neu': 0.815, 'pos': 0.185, 'compound': 0.9601},\n",
       " {'neg': 0.033, 'neu': 0.851, 'pos': 0.116, 'compound': 0.966},\n",
       " {'neg': 0.0, 'neu': 0.63, 'pos': 0.37, 'compound': 0.875},\n",
       " {'neg': 0.166, 'neu': 0.75, 'pos': 0.083, 'compound': -0.4574},\n",
       " {'neg': 0.278, 'neu': 0.722, 'pos': 0.0, 'compound': -0.7112},\n",
       " {'neg': 0.035, 'neu': 0.797, 'pos': 0.168, 'compound': 0.9861},\n",
       " {'neg': 0.0, 'neu': 0.777, 'pos': 0.223, 'compound': 0.7755},\n",
       " {'neg': 0.0, 'neu': 0.717, 'pos': 0.283, 'compound': 0.9595},\n",
       " {'neg': 0.0, 'neu': 0.677, 'pos': 0.323, 'compound': 0.9531},\n",
       " {'neg': 0.365, 'neu': 0.635, 'pos': 0.0, 'compound': -0.8481},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.075, 'neu': 0.76, 'pos': 0.166, 'compound': 0.785},\n",
       " {'neg': 0.0, 'neu': 0.782, 'pos': 0.218, 'compound': 0.8481},\n",
       " {'neg': 0.047, 'neu': 0.815, 'pos': 0.139, 'compound': 0.9021},\n",
       " {'neg': 0.0, 'neu': 0.786, 'pos': 0.214, 'compound': 0.7783},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.17, 'neu': 0.779, 'pos': 0.051, 'compound': -0.7785},\n",
       " {'neg': 0.041, 'neu': 0.867, 'pos': 0.092, 'compound': 0.9161},\n",
       " {'neg': 0.029, 'neu': 0.744, 'pos': 0.227, 'compound': 0.962},\n",
       " {'neg': 0.139, 'neu': 0.783, 'pos': 0.078, 'compound': -0.2351},\n",
       " {'neg': 0.107, 'neu': 0.837, 'pos': 0.056, 'compound': -0.9315},\n",
       " {'neg': 0.0, 'neu': 0.735, 'pos': 0.265, 'compound': 0.9598},\n",
       " {'neg': 0.075, 'neu': 0.694, 'pos': 0.231, 'compound': 0.8345},\n",
       " {'neg': 0.119, 'neu': 0.881, 'pos': 0.0, 'compound': -0.4767},\n",
       " {'neg': 0.078, 'neu': 0.77, 'pos': 0.151, 'compound': 0.769},\n",
       " {'neg': 0.037, 'neu': 0.782, 'pos': 0.181, 'compound': 0.8807},\n",
       " {'neg': 0.027, 'neu': 0.851, 'pos': 0.122, 'compound': 0.8841},\n",
       " {'neg': 0.0, 'neu': 0.856, 'pos': 0.144, 'compound': 0.6369},\n",
       " {'neg': 0.022, 'neu': 0.747, 'pos': 0.23, 'compound': 0.8879},\n",
       " {'neg': 0.0, 'neu': 0.882, 'pos': 0.118, 'compound': 0.9091},\n",
       " {'neg': 0.0, 'neu': 0.879, 'pos': 0.121, 'compound': 0.4404},\n",
       " {'neg': 0.0, 'neu': 0.849, 'pos': 0.151, 'compound': 0.5848},\n",
       " {'neg': 0.042, 'neu': 0.799, 'pos': 0.159, 'compound': 0.9371},\n",
       " {'neg': 0.064, 'neu': 0.716, 'pos': 0.22, 'compound': 0.785},\n",
       " {'neg': 0.0, 'neu': 0.526, 'pos': 0.474, 'compound': 0.9577},\n",
       " {'neg': 0.0, 'neu': 0.711, 'pos': 0.289, 'compound': 0.899},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.143, 'neu': 0.717, 'pos': 0.14, 'compound': 0.2278},\n",
       " {'neg': 0.034, 'neu': 0.833, 'pos': 0.134, 'compound': 0.8289},\n",
       " {'neg': 0.0, 'neu': 0.79, 'pos': 0.21, 'compound': 0.8422},\n",
       " {'neg': 0.0, 'neu': 0.703, 'pos': 0.297, 'compound': 0.9366},\n",
       " {'neg': 0.0, 'neu': 0.697, 'pos': 0.303, 'compound': 0.9298},\n",
       " {'neg': 0.0, 'neu': 0.875, 'pos': 0.125, 'compound': 0.6124},\n",
       " {'neg': 0.0, 'neu': 0.731, 'pos': 0.269, 'compound': 0.7088},\n",
       " {'neg': 0.0, 'neu': 0.79, 'pos': 0.21, 'compound': 0.8271},\n",
       " {'neg': 0.025, 'neu': 0.735, 'pos': 0.24, 'compound': 0.9505},\n",
       " {'neg': 0.0, 'neu': 0.798, 'pos': 0.202, 'compound': 0.5859},\n",
       " {'neg': 0.0, 'neu': 0.832, 'pos': 0.168, 'compound': 0.7537},\n",
       " {'neg': 0.043, 'neu': 0.925, 'pos': 0.033, 'compound': -0.1502},\n",
       " {'neg': 0.018, 'neu': 0.917, 'pos': 0.065, 'compound': 0.5859},\n",
       " {'neg': 0.0, 'neu': 0.721, 'pos': 0.279, 'compound': 0.8655},\n",
       " {'neg': 0.124, 'neu': 0.705, 'pos': 0.171, 'compound': 0.8427},\n",
       " {'neg': 0.092, 'neu': 0.717, 'pos': 0.191, 'compound': 0.4404},\n",
       " {'neg': 0.071, 'neu': 0.802, 'pos': 0.126, 'compound': 0.7569},\n",
       " {'neg': 0.0, 'neu': 0.932, 'pos': 0.068, 'compound': 0.7712},\n",
       " {'neg': 0.045, 'neu': 0.782, 'pos': 0.173, 'compound': 0.8791},\n",
       " {'neg': 0.0, 'neu': 0.782, 'pos': 0.218, 'compound': 0.9198},\n",
       " {'neg': 0.064, 'neu': 0.842, 'pos': 0.094, 'compound': 0.4005},\n",
       " {'neg': 0.05, 'neu': 0.86, 'pos': 0.09, 'compound': 0.8774},\n",
       " {'neg': 0.06, 'neu': 0.378, 'pos': 0.562, 'compound': 0.9634},\n",
       " {'neg': 0.0, 'neu': 0.645, 'pos': 0.355, 'compound': 0.968},\n",
       " {'neg': 0.119, 'neu': 0.628, 'pos': 0.253, 'compound': 0.7906},\n",
       " {'neg': 0.0, 'neu': 0.895, 'pos': 0.105, 'compound': 0.7088},\n",
       " {'neg': 0.0, 'neu': 0.947, 'pos': 0.053, 'compound': 0.2382},\n",
       " {'neg': 0.0, 'neu': 0.779, 'pos': 0.221, 'compound': 0.945},\n",
       " {'neg': 0.0, 'neu': 0.684, 'pos': 0.316, 'compound': 0.8633},\n",
       " {'neg': 0.085, 'neu': 0.864, 'pos': 0.051, 'compound': -0.2755},\n",
       " {'neg': 0.025, 'neu': 0.881, 'pos': 0.094, 'compound': 0.7184},\n",
       " {'neg': 0.072, 'neu': 0.804, 'pos': 0.124, 'compound': 0.8118},\n",
       " {'neg': 0.0, 'neu': 0.805, 'pos': 0.195, 'compound': 0.9074},\n",
       " {'neg': 0.04, 'neu': 0.765, 'pos': 0.195, 'compound': 0.836},\n",
       " {'neg': 0.197, 'neu': 0.769, 'pos': 0.033, 'compound': -0.8353},\n",
       " {'neg': 0.0, 'neu': 0.811, 'pos': 0.189, 'compound': 0.7845},\n",
       " {'neg': 0.0, 'neu': 0.791, 'pos': 0.209, 'compound': 0.9622},\n",
       " {'neg': 0.079, 'neu': 0.684, 'pos': 0.237, 'compound': 0.9398},\n",
       " {'neg': 0.038, 'neu': 0.722, 'pos': 0.24, 'compound': 0.9648},\n",
       " {'neg': 0.0, 'neu': 0.736, 'pos': 0.264, 'compound': 0.7845},\n",
       " {'neg': 0.0, 'neu': 0.777, 'pos': 0.223, 'compound': 0.6494},\n",
       " {'neg': 0.023, 'neu': 0.904, 'pos': 0.073, 'compound': 0.6242},\n",
       " {'neg': 0.0, 'neu': 0.785, 'pos': 0.215, 'compound': 0.9419},\n",
       " {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.8999},\n",
       " {'neg': 0.053, 'neu': 0.791, 'pos': 0.156, 'compound': 0.6486},\n",
       " {'neg': 0.0, 'neu': 0.879, 'pos': 0.121, 'compound': 0.4404},\n",
       " {'neg': 0.0, 'neu': 0.844, 'pos': 0.156, 'compound': 0.8588},\n",
       " {'neg': 0.0, 'neu': 0.734, 'pos': 0.266, 'compound': 0.8264},\n",
       " {'neg': 0.049, 'neu': 0.734, 'pos': 0.218, 'compound': 0.9834},\n",
       " {'neg': 0.028, 'neu': 0.852, 'pos': 0.12, 'compound': 0.9403},\n",
       " {'neg': 0.0, 'neu': 0.846, 'pos': 0.154, 'compound': 0.8442},\n",
       " {'neg': 0.0, 'neu': 0.613, 'pos': 0.387, 'compound': 0.8922},\n",
       " {'neg': 0.0, 'neu': 0.764, 'pos': 0.236, 'compound': 0.8591},\n",
       " {'neg': 0.0, 'neu': 0.78, 'pos': 0.22, 'compound': 0.8412},\n",
       " {'neg': 0.051, 'neu': 0.818, 'pos': 0.132, 'compound': 0.7738},\n",
       " {'neg': 0.0, 'neu': 0.75, 'pos': 0.25, 'compound': 0.8748},\n",
       " {'neg': 0.0, 'neu': 0.603, 'pos': 0.397, 'compound': 0.9413},\n",
       " {'neg': 0.011, 'neu': 0.801, 'pos': 0.187, 'compound': 0.9964},\n",
       " {'neg': 0.033, 'neu': 0.839, 'pos': 0.128, 'compound': 0.6486},\n",
       " {'neg': 0.035, 'neu': 0.551, 'pos': 0.415, 'compound': 0.978},\n",
       " {'neg': 0.08, 'neu': 0.812, 'pos': 0.108, 'compound': 0.8955},\n",
       " {'neg': 0.0, 'neu': 0.543, 'pos': 0.457, 'compound': 0.9118},\n",
       " {'neg': 0.0, 'neu': 0.86, 'pos': 0.14, 'compound': 0.4754},\n",
       " {'neg': 0.042, 'neu': 0.797, 'pos': 0.161, 'compound': 0.9943},\n",
       " {'neg': 0.071, 'neu': 0.673, 'pos': 0.256, 'compound': 0.9795},\n",
       " {'neg': 0.078, 'neu': 0.851, 'pos': 0.071, 'compound': -0.101},\n",
       " {'neg': 0.09, 'neu': 0.586, 'pos': 0.324, 'compound': 0.8315},\n",
       " {'neg': 0.0, 'neu': 0.854, 'pos': 0.146, 'compound': 0.7174},\n",
       " {'neg': 0.128, 'neu': 0.717, 'pos': 0.155, 'compound': 0.1759},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.072, 'neu': 0.882, 'pos': 0.046, 'compound': -0.7391},\n",
       " {'neg': 0.0, 'neu': 0.697, 'pos': 0.303, 'compound': 0.9794},\n",
       " {'neg': 0.155, 'neu': 0.658, 'pos': 0.187, 'compound': 0.296},\n",
       " {'neg': 0.056, 'neu': 0.809, 'pos': 0.135, 'compound': 0.9227},\n",
       " {'neg': 0.0, 'neu': 0.672, 'pos': 0.328, 'compound': 0.8516},\n",
       " {'neg': 0.152, 'neu': 0.637, 'pos': 0.212, 'compound': 0.832},\n",
       " {'neg': 0.023, 'neu': 0.831, 'pos': 0.145, 'compound': 0.9339},\n",
       " {'neg': 0.073, 'neu': 0.868, 'pos': 0.059, 'compound': 0.0867},\n",
       " {'neg': 0.012, 'neu': 0.866, 'pos': 0.122, 'compound': 0.9383},\n",
       " {'neg': 0.043, 'neu': 0.824, 'pos': 0.133, 'compound': 0.9547},\n",
       " {'neg': 0.07, 'neu': 0.568, 'pos': 0.362, 'compound': 0.8955},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.0, 'neu': 0.644, 'pos': 0.356, 'compound': 0.8915},\n",
       " {'neg': 0.02, 'neu': 0.686, 'pos': 0.294, 'compound': 0.969},\n",
       " {'neg': 0.029, 'neu': 0.785, 'pos': 0.186, 'compound': 0.9429},\n",
       " {'neg': 0.0, 'neu': 0.689, 'pos': 0.311, 'compound': 0.9695},\n",
       " {'neg': 0.0, 'neu': 0.815, 'pos': 0.185, 'compound': 0.7717},\n",
       " {'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'compound': 0.802},\n",
       " {'neg': 0.0, 'neu': 0.747, 'pos': 0.253, 'compound': 0.6982},\n",
       " {'neg': 0.129, 'neu': 0.773, 'pos': 0.097, 'compound': -0.7771},\n",
       " {'neg': 0.0, 'neu': 0.883, 'pos': 0.117, 'compound': 0.8349},\n",
       " {'neg': 0.0, 'neu': 0.786, 'pos': 0.214, 'compound': 0.9098},\n",
       " {'neg': 0.0, 'neu': 0.777, 'pos': 0.223, 'compound': 0.9498},\n",
       " {'neg': 0.009, 'neu': 0.775, 'pos': 0.215, 'compound': 0.9956},\n",
       " {'neg': 0.11, 'neu': 0.712, 'pos': 0.178, 'compound': 0.6361},\n",
       " {'neg': 0.0, 'neu': 0.563, 'pos': 0.437, 'compound': 0.9747},\n",
       " {'neg': 0.089, 'neu': 0.895, 'pos': 0.017, 'compound': -0.7795},\n",
       " {'neg': 0.013, 'neu': 0.783, 'pos': 0.205, 'compound': 0.9963},\n",
       " {'neg': 0.012, 'neu': 0.741, 'pos': 0.247, 'compound': 0.9946},\n",
       " {'neg': 0.0, 'neu': 0.836, 'pos': 0.164, 'compound': 0.9738},\n",
       " {'neg': 0.045, 'neu': 0.918, 'pos': 0.037, 'compound': -0.126},\n",
       " {'neg': 0.0, 'neu': 0.78, 'pos': 0.22, 'compound': 0.9891},\n",
       " {'neg': 0.0, 'neu': 0.86, 'pos': 0.14, 'compound': 0.7066},\n",
       " {'neg': 0.0, 'neu': 0.725, 'pos': 0.275, 'compound': 0.802},\n",
       " {'neg': 0.085, 'neu': 0.714, 'pos': 0.201, 'compound': 0.913},\n",
       " {'neg': 0.018, 'neu': 0.805, 'pos': 0.177, 'compound': 0.9832},\n",
       " {'neg': 0.0, 'neu': 0.601, 'pos': 0.399, 'compound': 0.9856},\n",
       " {'neg': 0.162, 'neu': 0.646, 'pos': 0.192, 'compound': 0.5719},\n",
       " {'neg': 0.145, 'neu': 0.772, 'pos': 0.083, 'compound': -0.4588},\n",
       " {'neg': 0.031, 'neu': 0.808, 'pos': 0.161, 'compound': 0.9702},\n",
       " {'neg': 0.062, 'neu': 0.705, 'pos': 0.233, 'compound': 0.886},\n",
       " {'neg': 0.0, 'neu': 0.8, 'pos': 0.2, 'compound': 0.6696},\n",
       " {'neg': 0.0, 'neu': 0.925, 'pos': 0.075, 'compound': 0.2732},\n",
       " {'neg': 0.11, 'neu': 0.638, 'pos': 0.252, 'compound': 0.8374},\n",
       " {'neg': 0.094, 'neu': 0.655, 'pos': 0.252, 'compound': 0.989},\n",
       " {'neg': 0.0, 'neu': 0.682, 'pos': 0.318, 'compound': 0.882},\n",
       " {'neg': 0.0, 'neu': 0.742, 'pos': 0.258, 'compound': 0.8885},\n",
       " {'neg': 0.0, 'neu': 0.607, 'pos': 0.393, 'compound': 0.9001},\n",
       " {'neg': 0.091, 'neu': 0.769, 'pos': 0.139, 'compound': 0.7094},\n",
       " {'neg': 0.0, 'neu': 0.716, 'pos': 0.284, 'compound': 0.8537},\n",
       " {'neg': 0.088, 'neu': 0.67, 'pos': 0.242, 'compound': 0.9054},\n",
       " {'neg': 0.06, 'neu': 0.814, 'pos': 0.126, 'compound': 0.5899},\n",
       " {'neg': 0.066, 'neu': 0.631, 'pos': 0.303, 'compound': 0.836},\n",
       " {'neg': 0.103, 'neu': 0.805, 'pos': 0.092, 'compound': -0.1531},\n",
       " {'neg': 0.065, 'neu': 0.852, 'pos': 0.083, 'compound': -0.031},\n",
       " {'neg': 0.036, 'neu': 0.794, 'pos': 0.171, 'compound': 0.766},\n",
       " {'neg': 0.0, 'neu': 0.796, 'pos': 0.204, 'compound': 0.9633},\n",
       " {'neg': 0.037, 'neu': 0.797, 'pos': 0.167, 'compound': 0.9609},\n",
       " {'neg': 0.0, 'neu': 0.668, 'pos': 0.332, 'compound': 0.9175},\n",
       " {'neg': 0.023, 'neu': 0.698, 'pos': 0.279, 'compound': 0.9945},\n",
       " {'neg': 0.0, 'neu': 0.858, 'pos': 0.142, 'compound': 0.7537},\n",
       " {'neg': 0.041, 'neu': 0.786, 'pos': 0.172, 'compound': 0.9235},\n",
       " {'neg': 0.034, 'neu': 0.534, 'pos': 0.432, 'compound': 0.9666},\n",
       " {'neg': 0.047, 'neu': 0.833, 'pos': 0.121, 'compound': 0.8633},\n",
       " {'neg': 0.0, 'neu': 0.777, 'pos': 0.223, 'compound': 0.8225},\n",
       " {'neg': 0.0, 'neu': 0.642, 'pos': 0.358, 'compound': 0.9055},\n",
       " {'neg': 0.056, 'neu': 0.776, 'pos': 0.168, 'compound': 0.7449},\n",
       " {'neg': 0.0, 'neu': 0.648, 'pos': 0.352, 'compound': 0.9732},\n",
       " {'neg': 0.344, 'neu': 0.62, 'pos': 0.037, 'compound': -0.9419},\n",
       " {'neg': 0.124, 'neu': 0.698, 'pos': 0.177, 'compound': 0.8038},\n",
       " {'neg': 0.0, 'neu': 0.625, 'pos': 0.375, 'compound': 0.9781},\n",
       " {'neg': 0.018, 'neu': 0.922, 'pos': 0.06, 'compound': 0.7089},\n",
       " {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.8591},\n",
       " {'neg': 0.0, 'neu': 0.759, 'pos': 0.241, 'compound': 0.872},\n",
       " {'neg': 0.092, 'neu': 0.774, 'pos': 0.135, 'compound': 0.685},\n",
       " {'neg': 0.0, 'neu': 0.814, 'pos': 0.186, 'compound': 0.9195},\n",
       " {'neg': 0.062, 'neu': 0.397, 'pos': 0.541, 'compound': 0.9509},\n",
       " {'neg': 0.158, 'neu': 0.658, 'pos': 0.184, 'compound': 0.1012},\n",
       " {'neg': 0.0, 'neu': 0.602, 'pos': 0.398, 'compound': 0.9109},\n",
       " {'neg': 0.0, 'neu': 0.647, 'pos': 0.353, 'compound': 0.9808},\n",
       " {'neg': 0.0, 'neu': 0.642, 'pos': 0.358, 'compound': 0.9668},\n",
       " {'neg': 0.0, 'neu': 0.807, 'pos': 0.193, 'compound': 0.9416},\n",
       " {'neg': 0.023, 'neu': 0.829, 'pos': 0.148, 'compound': 0.8623},\n",
       " {'neg': 0.0, 'neu': 0.85, 'pos': 0.15, 'compound': 0.5719},\n",
       " {'neg': 0.041, 'neu': 0.663, 'pos': 0.296, 'compound': 0.9103},\n",
       " {'neg': 0.103, 'neu': 0.758, 'pos': 0.139, 'compound': 0.4274},\n",
       " {'neg': 0.113, 'neu': 0.887, 'pos': 0.0, 'compound': -0.6249},\n",
       " {'neg': 0.066, 'neu': 0.913, 'pos': 0.02, 'compound': -0.7129},\n",
       " {'neg': 0.0, 'neu': 0.72, 'pos': 0.28, 'compound': 0.7906},\n",
       " {'neg': 0.058, 'neu': 0.942, 'pos': 0.0, 'compound': -0.3875},\n",
       " {'neg': 0.153, 'neu': 0.677, 'pos': 0.169, 'compound': 0.2263},\n",
       " {'neg': 0.022, 'neu': 0.928, 'pos': 0.05, 'compound': 0.5267},\n",
       " {'neg': 0.039, 'neu': 0.784, 'pos': 0.177, 'compound': 0.8705},\n",
       " {'neg': 0.0, 'neu': 0.853, 'pos': 0.147, 'compound': 0.8625},\n",
       " {'neg': 0.0, 'neu': 0.932, 'pos': 0.068, 'compound': 0.6463},\n",
       " {'neg': 0.162, 'neu': 0.796, 'pos': 0.042, 'compound': -0.943},\n",
       " {'neg': 0.0, 'neu': 0.736, 'pos': 0.264, 'compound': 0.8011},\n",
       " {'neg': 0.0, 'neu': 0.859, 'pos': 0.141, 'compound': 0.9651},\n",
       " {'neg': 0.04, 'neu': 0.766, 'pos': 0.194, 'compound': 0.7529},\n",
       " {'neg': 0.0, 'neu': 0.666, 'pos': 0.334, 'compound': 0.9003},\n",
       " {'neg': 0.122, 'neu': 0.649, 'pos': 0.229, 'compound': 0.8013},\n",
       " {'neg': 0.026, 'neu': 0.806, 'pos': 0.168, 'compound': 0.9875},\n",
       " {'neg': 0.0, 'neu': 0.741, 'pos': 0.259, 'compound': 0.9829},\n",
       " {'neg': 0.0, 'neu': 0.846, 'pos': 0.154, 'compound': 0.6369},\n",
       " {'neg': 0.056, 'neu': 0.876, 'pos': 0.069, 'compound': -0.0676},\n",
       " {'neg': 0.036, 'neu': 0.836, 'pos': 0.129, 'compound': 0.9554},\n",
       " {'neg': 0.089, 'neu': 0.791, 'pos': 0.12, 'compound': 0.5264},\n",
       " {'neg': 0.0, 'neu': 0.661, 'pos': 0.339, 'compound': 0.9233},\n",
       " {'neg': 0.0, 'neu': 0.484, 'pos': 0.516, 'compound': 0.9595},\n",
       " {'neg': 0.0, 'neu': 0.767, 'pos': 0.233, 'compound': 0.9552},\n",
       " {'neg': 0.0, 'neu': 0.883, 'pos': 0.117, 'compound': 0.6249},\n",
       " {'neg': 0.0, 'neu': 0.781, 'pos': 0.219, 'compound': 0.9663},\n",
       " {'neg': 0.0, 'neu': 0.846, 'pos': 0.154, 'compound': 0.7181},\n",
       " {'neg': 0.022, 'neu': 0.805, 'pos': 0.172, 'compound': 0.9401},\n",
       " {'neg': 0.059, 'neu': 0.778, 'pos': 0.163, 'compound': 0.5984},\n",
       " {'neg': 0.0, 'neu': 0.805, 'pos': 0.195, 'compound': 0.902},\n",
       " {'neg': 0.0, 'neu': 0.785, 'pos': 0.215, 'compound': 0.9586},\n",
       " {'neg': 0.0, 'neu': 0.674, 'pos': 0.326, 'compound': 0.9631},\n",
       " {'neg': 0.032, 'neu': 0.667, 'pos': 0.301, 'compound': 0.973},\n",
       " {'neg': 0.0, 'neu': 0.633, 'pos': 0.367, 'compound': 0.9749},\n",
       " {'neg': 0.0, 'neu': 0.662, 'pos': 0.338, 'compound': 0.9636},\n",
       " {'neg': 0.0, 'neu': 0.886, 'pos': 0.114, 'compound': 0.8858},\n",
       " {'neg': 0.0, 'neu': 0.828, 'pos': 0.172, 'compound': 0.7552},\n",
       " {'neg': 0.026, 'neu': 0.721, 'pos': 0.253, 'compound': 0.9788},\n",
       " {'neg': 0.0, 'neu': 0.786, 'pos': 0.214, 'compound': 0.9309},\n",
       " {'neg': 0.0, 'neu': 0.673, 'pos': 0.327, 'compound': 0.9634},\n",
       " {'neg': 0.063, 'neu': 0.874, 'pos': 0.062, 'compound': -0.0129},\n",
       " {'neg': 0.027, 'neu': 0.939, 'pos': 0.034, 'compound': -0.1027},\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compound_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f83a8a86-697a-409f-bc92-f03398f465fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df= pd.DataFrame(compound_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd41c67b-8e6d-48c1-907e-ab9eccba0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the our df and the current df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "024102b8-afde-47b1-980f-cbbaaf39a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df= new_df.reset_index().rename(columns={'index': 'Id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3b79020-ac8b-40f8-88fe-9f8eb8c21456",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.Id= new_df.Id+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7dcecf2-4a7a-424d-ad5f-952f59d42498",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df= new_df.merge(df, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61314907-d316-4239-abd6-f8586e894f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.9441</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5664</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.8265</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.9468</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.5267</td>\n",
       "      <td>B000P41A28</td>\n",
       "      <td>A3A63RACXR1XIL</td>\n",
       "      <td>A. Boodhoo \"deaddodo\"</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1204502400</td>\n",
       "      <td>constipation</td>\n",
       "      <td>we switched from the advance similac to the or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>B000P41A28</td>\n",
       "      <td>A5VVRGL8JA7R</td>\n",
       "      <td>Adam</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1306368000</td>\n",
       "      <td>Constipation Not A Problem if...</td>\n",
       "      <td>Like the bad reviews say, the organic formula ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.9305</td>\n",
       "      <td>B000P41A28</td>\n",
       "      <td>A2TGDTJ8YCU6PD</td>\n",
       "      <td>geena77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1347494400</td>\n",
       "      <td>Love this formula!</td>\n",
       "      <td>I wanted to solely breastfeed but was unable t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.2809</td>\n",
       "      <td>B000P41A28</td>\n",
       "      <td>AUV4GIZZE693O</td>\n",
       "      <td>Susan Coe \"sueysis\"</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1203638400</td>\n",
       "      <td>very convenient</td>\n",
       "      <td>i love the fact that i can get this delieved t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>B000P41A28</td>\n",
       "      <td>A82WIMR4RSVLI</td>\n",
       "      <td>Emrose mom</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1337472000</td>\n",
       "      <td>The best weve tried so far</td>\n",
       "      <td>We have a 7 week old... He had gas and constip...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id    neg    neu    pos  compound   ProductId          UserId  \\\n",
       "0         1  0.000  0.695  0.305    0.9441  B001E4KFG0  A3SGXH7AUHU8GW   \n",
       "1         2  0.138  0.862  0.000   -0.5664  B00813GRG4  A1D87F6ZCVE5NK   \n",
       "2         3  0.091  0.754  0.155    0.8265  B000LQOCH0   ABXLMWJIXXAIN   \n",
       "3         4  0.000  1.000  0.000    0.0000  B000UA0QIQ  A395BORC6FGVXV   \n",
       "4         5  0.000  0.552  0.448    0.9468  B006K2ZZ7K  A1UQRSCLF8GW1T   \n",
       "...     ...    ...    ...    ...       ...         ...             ...   \n",
       "9995   9996  0.089  0.852  0.059   -0.5267  B000P41A28  A3A63RACXR1XIL   \n",
       "9996   9997  0.091  0.747  0.162    0.6808  B000P41A28    A5VVRGL8JA7R   \n",
       "9997   9998  0.063  0.811  0.126    0.9305  B000P41A28  A2TGDTJ8YCU6PD   \n",
       "9998   9999  0.149  0.697  0.154    0.2809  B000P41A28   AUV4GIZZE693O   \n",
       "9999  10000  0.026  0.811  0.164    0.9850  B000P41A28   A82WIMR4RSVLI   \n",
       "\n",
       "                          ProfileName  HelpfulnessNumerator  \\\n",
       "0                          delmartian                     1   \n",
       "1                              dll pa                     0   \n",
       "2     Natalia Corres \"Natalia Corres\"                     1   \n",
       "3                                Karl                     3   \n",
       "4       Michael D. Bigham \"M. Wassir\"                     0   \n",
       "...                               ...                   ...   \n",
       "9995            A. Boodhoo \"deaddodo\"                    10   \n",
       "9996                             Adam                     2   \n",
       "9997                          geena77                     0   \n",
       "9998              Susan Coe \"sueysis\"                     1   \n",
       "9999                       Emrose mom                     0   \n",
       "\n",
       "      HelpfulnessDenominator  Score        Time  \\\n",
       "0                          1      5  1303862400   \n",
       "1                          0      1  1346976000   \n",
       "2                          1      4  1219017600   \n",
       "3                          3      2  1307923200   \n",
       "4                          0      5  1350777600   \n",
       "...                      ...    ...         ...   \n",
       "9995                      15      1  1204502400   \n",
       "9996                       3      5  1306368000   \n",
       "9997                       0      5  1347494400   \n",
       "9998                       2      5  1203638400   \n",
       "9999                       1      4  1337472000   \n",
       "\n",
       "                               Summary  \\\n",
       "0                Good Quality Dog Food   \n",
       "1                    Not as Advertised   \n",
       "2                \"Delight\" says it all   \n",
       "3                       Cough Medicine   \n",
       "4                          Great taffy   \n",
       "...                                ...   \n",
       "9995                      constipation   \n",
       "9996  Constipation Not A Problem if...   \n",
       "9997                Love this formula!   \n",
       "9998                   very convenient   \n",
       "9999        The best weve tried so far   \n",
       "\n",
       "                                                   Text  \n",
       "0     I have bought several of the Vitality canned d...  \n",
       "1     Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2     This is a confection that has been around a fe...  \n",
       "3     If you are looking for the secret ingredient i...  \n",
       "4     Great taffy at a great price.  There was a wid...  \n",
       "...                                                 ...  \n",
       "9995  we switched from the advance similac to the or...  \n",
       "9996  Like the bad reviews say, the organic formula ...  \n",
       "9997  I wanted to solely breastfeed but was unable t...  \n",
       "9998  i love the fact that i can get this delieved t...  \n",
       "9999  We have a 7 week old... He had gas and constip...  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b8d1eca-a75c-4c13-a3c9-0c5679e503db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'AMAZON reviews')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax= sns.barplot(new_df, x='Score', y='compound')\n",
    "ax.set_title('AMAZON reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "070a33c2-efcf-4d75-b2a4-c9b032408dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABNoAAAHyCAYAAAA9TKO2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABbrklEQVR4nO3de1hVZf7//9cWZG9HRxoPARYimilEmm06gJE1Kg42xszoR9LCDlAxmIqMzUToqEzFZKbYJKilMX6mksrKpiilg6eBbEScmsmOatuPbSLomyhNILh+f/hz526DB1iwQZ6P61rX1br3vdZ630vrjhf32stiGIYhAAAAAAAAAC3SxdsFAAAAAAAAAOcCgjYAAAAAAADABARtAAAAAAAAgAkI2gAAAAAAAAATELQBAAAAAAAAJiBoAwAAAAAAAExA0AYAAAAAAACYgKANAAAAAAAAMAFBGwAAAAAAAGACgjYAAHBO2rFjh37961+rf//+slqtCggIUFRUlH73u995pZ79+/frhhtuUK9evWSxWJSWluaVOs4VCxYskMVicWurq6tTSkqKgoKC5OPjo8suu8w7xQEAgE7L19sFAAAAmO21117TjTfeqOuuu06LFi1SUFCQnE6ndu7cqXXr1unRRx9t85pmz56tHTt2aM2aNQoMDFRQUFCb13Cuy8vL08qVK/WXv/xFdrtdPXr08HZJAACgk7EYhmF4uwgAAAAzjRo1SgcPHtRHH30kX1/33yseO3ZMXbq0/aL+wYMHa/DgwSosLDTlfA0NDaqvr5fVajXlfK3pu+++009+8hNTz7lgwQItXLhQJ/+v7J133qmnn35a3333nanXAgAAOFM8OgoAAM45VVVV6tOnj0fIJqnRkK2goEBRUVHq3r27evTooXHjxqmsrMyj386dO3XjjTeqV69estlsGjFihJ577rlT1rJ582ZZLBZ99tlnev3112WxWGSxWLR//35JksPh0C233KLzzz9fVqtVYWFhevTRR3Xs2DHXOfbv3y+LxaJFixbpgQceUGhoqKxWq955551Grzl69GgNHTpUP/59qmEYuuiii3TDDTe42r755hulpqbqggsukJ+fnwYOHKjMzEzV1tZ6XD8/P9/jWhaLRQsWLHDtn3ikc9euXZo0aZJ+9rOfadCgQZKkvXv36qabblK/fv1cj/OOHj1au3fvdjvnmf55/LiOJ598Uv/9739d97ixegEAAFoTQRsAADjnREVFaceOHZo5c6Z27Niho0ePNtn3oYce0pQpUxQeHq7nnntO//u//6vDhw8rJiZGH374oavfO++8o5EjR+rbb7/VihUrtGHDBl122WVKSEg4ZaBz+eWXq6SkRIGBgRo5cqRKSkpUUlKioKAgff3114qOjtamTZv0pz/9Sa+88orGjBmjOXPm6J577vE412OPPaa3335bixcv1uuvv66hQ4c2es1Zs2bp448/1ltvveXW/vrrr+vzzz/X9OnTJUnff/+9rr/+eq1du1bp6el67bXXdMstt2jRokX6zW9+c6pbfFq/+c1vdNFFF+n555/XihUrJEnjx49XaWmpFi1apKKiIuXl5WnEiBH69ttvXced6Z/Hj5WUlGj8+PHq1q2b6x6fHCgCAAC0CQMAAOAcU1lZaVxzzTWGJEOS0bVrVyM6OtrIzs42Dh8+7OrncDgMX19fY8aMGW7HHz582AgMDDQmT57sahs6dKgxYsQI4+jRo259f/nLXxpBQUFGQ0PDKWsKCQkxbrjhBre2++67z5Bk7Nixw639t7/9rWGxWIyPP/7YMAzD2LdvnyHJGDRokFFXV3fa8Tc0NBgDBw404uPj3drj4uKMQYMGGceOHTMMwzBWrFhhSDKee+45t34PP/ywIcnYtGmT2/Wfeuopj2tJMubPn+/anz9/viHJ+OMf/+jWr7Ky0pBk5OTkNFn32fx5nLjOyW699Vaje/fuTZ4fAACgtbGiDQAAnHN69+6tbdu26Z///Kf+/Oc/Kz4+Xp988okyMjJ06aWXqrKyUpK0ceNG1dfXa9q0aaqvr3dtNptNo0aN0ubNmyVJn332mT766CPdfPPNkuTWd/z48XI6nfr444/Pus63335b4eHhuvLKK93ab7vtNhmGobffftut/cYbb1TXrl1Pe94uXbronnvu0auvviqHwyFJ+vzzz/XGG28oNTXV9bbOt99+W927d9ekSZM8ri/JY0Xc2Zg4caLbfq9evTRo0CA98sgjWrJkicrKytwej5XO/M8DAACgvSJoAwAA56zIyEj94Q9/0PPPP68vv/xSs2fP1v79+7Vo0SJJ0ldffSVJuuKKK9S1a1e3raCgwBXIneg3Z84cj36pqamS5Op7Nqqqqhp9+2i/fv1cn5/sbN5Uescdd6hbt26uxzaXL1+ubt266Y477nC7fmBgoCt4O+H888+Xr6+vx/XPxo9rtVgseuuttzRu3DgtWrRIl19+ufr27auZM2fq8OHDks78zwMAAKC98vyGYAAAgHNQ165dNX/+fC1dulT//ve/JUl9+vSRJL3wwgsKCQlp8tgT/TIyMpr87rIhQ4acdU29e/eW0+n0aP/yyy/drnvCjwOxU/H399ett96qJ598UnPmzNFTTz2lqVOn6rzzznO7/o4dO2QYhtu5KyoqVF9f77q+zWaTJLcXJEieQeDpag0JCdHq1aslSZ988omee+45LViwQHV1dVqxYsUZ/3kAAAC0VwRtAADgnON0Ohtd/bVnzx5JP6wYGzdunHx9ffX55597POp4siFDhmjw4MH617/+pYceesi0OkePHq3s7Gzt2rVLl19+uat97dq1slgsuv7661t0/pkzZyo3N1eTJk3St99+6/GChdGjR+u5557Tyy+/rF//+tdu1z/xuSQFBATIZrPp/fffdzt+w4YNza7t4osv1ty5c7V+/Xrt2rVL0pn/eQAAALRXBG0AAOCcM27cOF144YWaMGGChg4dqmPHjmn37t169NFH1aNHD82aNUuSNGDAAGVlZSkzM1N79+7VL37xC/3sZz/TV199pffee0/du3fXwoULJUkrV65UXFycxo0bp9tuu00XXHCBvvnmG+3Zs0e7du3S888/f9Z1zp49W2vXrtUNN9ygrKwshYSE6LXXXlNubq5++9vf6uKLL27Rfbj44ov1i1/8Qq+//rquueYaDR8+3O3zadOmafny5br11lu1f/9+XXrppdq+fbseeughjR8/XmPGjJF0fHXaLbfcojVr1mjQoEEaPny43nvvPT3zzDNnXMv777+ve+65R//zP/+jwYMHy8/PT2+//bbef/993XfffZLO7s8DAACgPSJoAwAA55y5c+dqw4YNWrp0qZxOp2praxUUFKQxY8YoIyNDYWFhrr4ZGRkKDw/XsmXL9Oyzz6q2tlaBgYG64oorlJKS4up3/fXX67333tODDz6otLQ0/b//9//Uu3dvhYeHa/Lkyc2qs2/fviouLlZGRoYyMjJUXV2tgQMHatGiRUpPT2/xfZCkhIQEvf766x6r2aTjj4S+8847yszM1COPPKKvv/5aF1xwgebMmaP58+e79X300UclSYsWLdKRI0f085//XK+++qoGDBhwRnUEBgZq0KBBys3N1YEDB2SxWDRw4EA9+uijmjFjhqvfmf55AAAAtEcWwzAMbxcBAACA1jFx4kS9++672r9//xm9sRQAAADNx4o2AACAc0xtba127dql9957Ty+99JKWLFlCyAYAANAGWNEGAABwjtm/f79CQ0PVs2dPTZ06VY8//rh8fHy8XRYAAMA5j6ANAAAAAAAAMEEXbxcAAAAAAAAAnAsI2gAAAAAAAAATELQBAAAAAAAAJiBoAwAAAAAAAExA0AYAAAAAAACYgKANAAAAAAAAMAFBGwAAAAAAAGACgjYAAAAAAADABARtAAAAAAAAgAkI2gAAAAAAAAATELQBAAAAAAAAJiBoAwAAAAAAAExA0AYAAAAAAACYgKANAAAAAAAAMAFBGwAAAAAAAGACgjYAAAAAAADABARtAAAAAAAAgAkI2gAAAAAAAAATELQBAAAAAAAAJiBoAwAAAAAAAExA0AYAAAAAAACYgKANAAAAAAAAMAFBGwAAAAAAAGACgjYAAAAAAADABARtAAAAAAAAgAkI2gAAAAAAAAATELQBAAAAAAAAJiBoAwAAAAAAAExA0AYAAAAAAACYgKANAAAAAAAAMAFBGwAAAAAAAGACgjYAAAAAAADABARtAAAAAAAAgAl8vV1Ae3Ts2DF9+eWX+ulPfyqLxeLtcgCgwzMMQ4cPH1a/fv3UpQu/45GYawDATMwznphnAMBcZzrXELQ14ssvv1RwcLC3ywCAc86BAwd04YUXeruMdoG5BgDMxzzzA+YZAGgdp5trCNoa8dOf/lTS8ZvXs2dPL1cDAB1fdXW1goODXf99BXMNAJiJecYT8wwAmOtM5xqCtkacWFrds2dPJiUAMBGPrvyAuQYAzMc88wPmGQBoHaeba/gCAwAAAAAAAMAEBG0AAAAAAACACQjaAAAAAAAAABMQtAEAAAAAAAAmIGgDAAAAAAAATEDQBgAAAAAAAJiAoA0AAAAAAAAwAUEbAAAAAAAAYAKCNgAAAAAAAMAEBG0AAAAAAACACQjaAAAAAAAAABMQtAEAAAAAAAAmIGgDAAAAAAAATODr7QIAAK3DMAzV1NS49rt37y6LxeLFigB0NPx3BEBL8N8QAJ0RQRsAnKNqamoUHx/v2t+wYYN69OjhxYrQ3vADkDvuhyf+OwKgJfhvCIDOiKANAIBOih+A3HE/AAAA0FJ8RxsAAAAAAABgAoI2AAAk5ebmKjQ0VDabTXa7Xdu2bTtl/6efflrDhw/XT37yEwUFBen2229XVVVVG1ULAAAAoD3i0VEAQKdXUFCgtLQ05ebmauTIkVq5cqXi4uL04Ycfqn///h79t2/frmnTpmnp0qWaMGGCDh48qJSUFCUnJ+ull15qcT32e9e2+BxnwlJfJ/+T9q+bt06Gr1+rXrP0kWmtev7Ogr8jAAAA7RNBGwB4QVv8kMwPyGduyZIlSkpKUnJysiQpJydHGzduVF5enrKzsz36v/vuuxowYIBmzpwpSQoNDdXdd9+tRYsWtWndAAAAANoXgjYAQKdWV1en0tJS3XfffW7tsbGxKi4ubvSY6OhoZWZmqrCwUHFxcaqoqNALL7ygG264ocnr1NbWqra21rVfXV1tzgA6CcJpAAAAdAR8RxsAoFOrrKxUQ0ODAgIC3NoDAgJUXl7e6DHR0dF6+umnlZCQID8/PwUGBuq8887TX/7ylyavk52dLX9/f9cWHBxs6jgAAAAAeB9BGwAAkiwWi9u+YRgebSd8+OGHmjlzpv74xz+qtLRUb7zxhvbt26eUlJQmz5+RkaFDhw65tgMHDphaP9AaDJ+uOjRsimszfLp6uyQAAIB2jaANANCp9enTRz4+Ph6r1yoqKjxWuZ2QnZ2tkSNH6t5779WwYcM0btw45ebmas2aNXI6nY0eY7Va1bNnT7fN2whRcFoWiwxfP9emJsJnoLM7mzdXO51OTZ06VUOGDFGXLl2UlpbWaL/169crPDxcVqtV4eHhprxsBwDQ+gjaAACdmp+fn+x2u4qKitzai4qKFB0d3egx3333nbp0cZ9CfXx8JB1fCddhEKIAQIudeHN1ZmamysrKFBMTo7i4ODkcjkb719bWqm/fvsrMzNTw4cMb7VNSUqKEhAQlJibqX//6lxITEzV58mTt2LGjNYcCADABL0MAgHPUidVKJ++jcenp6UpMTFRkZKSioqK0atUqORwO16OgGRkZOnjwoNauPf6F/BMmTNCdd96pvLw8jRs3Tk6nU2lpabryyivVr18/bw4FLcC/MwCa42zfXD1gwAAtW7ZMkrRmzZpGz5mTk6OxY8cqIyND0vF5aMuWLcrJydGzzz7bSiMBAJiBoA0AzlX//2olnF5CQoKqqqqUlZUlp9OpiIgIFRYWKiQkRNLxx3xOXplw22236fDhw3r88cf1u9/9Tuedd55+/vOf6+GHH/bWEGAG/p0BcJaa8+bqM1FSUqLZs2e7tY0bN045OTlNHsPbrQGgfSBoAwBAUmpqqlJTUxv9LD8/36NtxowZmjFjRitXBQBoz5rz5uozUV5eftbnzM7O1sKFC5t9TQCAOfiONgAAAABogbN5c3VrnZO3WwNA+8CKNgAAAOAMGIahmpoa13737t1bHKagY2vOm6vPRGBg4Fmf02q1ymq1NvuaAABzsKINAAAAOAM1NTWKj493bSeHbuicmvPm6jMRFRXlcc5Nmza16JwAgLbBijYAAAAAaKazfXO1JO3evVuSdOTIEX399dfavXu3/Pz8FB4eLkmaNWuWrr32Wj388MOKj4/Xhg0b9Oabb2r79u1tPj4AwNkhaAMAAACAZjrbN1dL0ogRI1z/XFpaqmeeeUYhISHav3+/JCk6Olrr1q3T3LlzNW/ePA0aNEgFBQW66qqrTKvbfu/a03dqIUt9nfxP2r9u3rpWf7tz6SPTWvX8AHA6BG0AAADo8AgN4E1n++ZqwzBOe85JkyZp0qRJLS0NANDG+I42AAAAAAAAwAQEbQAAAAAAAIAJCNoAAAAAAAAAExC0AQAAAAAAACbgZQgAAADAGTB8uurQsClu+wAAACcjaAMAAADOhMXS6m8ZBQAAHRuPjgIAAAAAAAAmIGgDAAAAAAAATMCjowAAAADOmmEYqqmpce13795dFovFixUBAOB9BG0AAAAAzlpNTY3i4+Nd+xs2bFCPHj28WBEAAN7Ho6MAAAAAAACACVjRBgAAAJxj7PeubfVrWOrr5H/S/nXz1rXJW1lLH5nW6tcAAKC5WNEGAAAAAAAAmICgDQAAAAAAADABj44CAAAAOGuGT1cdGjbFbR8AgM6OoA0AAADA2bNY2uQ72dBxEcYC6IwI2gAAAAAA5iOMBdAJ8R1tAAAAAAAAgAm8HrTl5uYqNDRUNptNdrtd27Zta7Lviy++qLFjx6pv377q2bOnoqKitHHjRo9+69evV3h4uKxWq8LDw/XSSy+15hAAAAAAAAAA7wZtBQUFSktLU2ZmpsrKyhQTE6O4uDg5HI5G+2/dulVjx45VYWGhSktLdf3112vChAkqKytz9SkpKVFCQoISExP1r3/9S4mJiZo8ebJ27NjRVsMCAAAAAABAJ+TVoG3JkiVKSkpScnKywsLClJOTo+DgYOXl5TXaPycnR7///e91xRVXaPDgwXrooYc0ePBg/f3vf3frM3bsWGVkZGjo0KHKyMjQ6NGjlZOT00ajAgAAAAAAQGfktaCtrq5OpaWlio2NdWuPjY1VcXHxGZ3j2LFjOnz4sHr16uVqKykp8TjnuHHjTnnO2tpaVVdXu20AAAAAAADA2fBa0FZZWamGhgYFBAS4tQcEBKi8vPyMzvHoo4+qpqZGkydPdrWVl5ef9Tmzs7Pl7+/v2oKDg89iJAAAAAAAAEA7eBmCxWJx2zcMw6OtMc8++6wWLFiggoICnX/++S06Z0ZGhg4dOuTaDhw4cBYjAAAAAAAAACRfb124T58+8vHx8VhpVlFR4bEi7ccKCgqUlJSk559/XmPGjHH7LDAw8KzPabVaZbVaz3IEAAAAAAAAwA+8tqLNz89PdrtdRUVFbu1FRUWKjo5u8rhnn31Wt912m5555hndcMMNHp9HRUV5nHPTpk2nPCcAAAAAAADQUl5b0SZJ6enpSkxMVGRkpKKiorRq1So5HA6lpKRIOv5I58GDB7V27VpJx0O2adOmadmyZbr66qtdK9e6desmf39/SdKsWbN07bXX6uGHH1Z8fLw2bNigN998U9u3b/fOIAEAAAAAANApePU72hISEpSTk6OsrCxddtll2rp1qwoLCxUSEiJJcjqdcjgcrv4rV65UfX29pk+frqCgINc2a9YsV5/o6GitW7dOTz31lIYNG6b8/HwVFBToqquuavPxAQAAAAAAoPPw+ssQUlNTtX//ftXW1qq0tFTXXnut67P8/Hxt3rzZtb9582YZhuGx5efnu51z0qRJ+uijj1RXV6c9e/boN7/5TRuNBgDQUeXm5io0NFQ2m012u13btm1rsu9tt90mi8XisV1yySVtWDEAAACA9sbrQRsAmMUwDB05csS1GYbh7ZLQQRQUFCgtLU2ZmZkqKytTTEyM4uLi3FZVn2zZsmVyOp2u7cCBA+rVq5f+53/+p40rBwAAANCeELQBOGfU1NQoPj7etdXU1Hi7JHQQS5YsUVJSkpKTkxUWFqacnBwFBwcrLy+v0f7+/v4KDAx0bTt37tT/+3//T7fffnsbVw4AAACgPSFoAwB0anV1dSotLVVsbKxbe2xsrIqLi8/oHKtXr9aYMWNc3zHamNraWlVXV7ttAAAAAM4tBG0AgE6tsrJSDQ0NCggIcGsPCAhwvd36VJxOp15//XUlJyefsl92drb8/f1dW3BwcIvqBgAAAND+ELQBACDJYrG47RuG4dHWmPz8fJ133nn61a9+dcp+GRkZOnTokGs7cOBAS8oFAAAA0A75ersAAAC8qU+fPvLx8fFYvVZRUeGxyu3HDMPQmjVrlJiYKD8/v1P2tVqtslqtLa4XAAAAQPvFijYAQKfm5+cnu92uoqIit/aioiJFR0ef8tgtW7bos88+U1JSUmuWCAAAAKCDYEUbAKDTS09PV2JioiIjIxUVFaVVq1bJ4XAoJSVF0vHHPg8ePKi1a9e6Hbd69WpdddVVioiI8EbZAAAAANoZgjYAQKeXkJCgqqoqZWVlyel0KiIiQoWFha63iDqdTjkcDrdjDh06pPXr12vZsmXeKBkAAABAO0TQBgCApNTUVKWmpjb6WX5+vkebv7+/vvvuu1auCgAAAEBHQtAGoNXZ7117+k4msNTXyf+k/evmrZPhe+ovqG+p0kemter5AQAAAAAdBy9DAAAAAAAAAExA0AYAAAAAAACYgKANAAAAAAAAMAFBGwAAAAAAAGACgjYAAAAAAADABARtAAAAAAAAgAkI2gAAAAAAAAATELQBAAAAAAAAJvD1dgEAYBbDp6sODZvitg8AAAAAQFshaANw7rBYZPj6ebsKAAAAAEAnxaOjAAAAAAAAgAkI2gAAAAAAAAATELQBAAAAAAAAJiBoAwAAAAAAAExA0AYAAAAAAACYgKANAAAAAAAAMAFBGwAAAAAAAGACgjYAAAAAAADABARtAAAAAAAAgAkI2gAAAAAAAAATELQBAAAAQAvk5uYqNDRUNptNdrtd27ZtO2X/LVu2yG63y2azaeDAgVqxYoVHn5ycHA0ZMkTdunVTcHCwZs+ere+//761hgAAMAlBGwAAAAA0U0FBgdLS0pSZmamysjLFxMQoLi5ODoej0f779u3T+PHjFRMTo7KyMt1///2aOXOm1q9f7+rz9NNP67777tP8+fO1Z88erV69WgUFBcrIyGirYaEVGIahI0eOuDbDMLxdEoBW4OvtAgAAAACgo1qyZImSkpKUnJws6fhKtI0bNyovL0/Z2dke/VesWKH+/fsrJydHkhQWFqadO3dq8eLFmjhxoiSppKREI0eO1NSpUyVJAwYM0JQpU/Tee++1zaDQKmpqahQfH+/a37Bhg3r06OHFigC0Bla0AQAAAEAz1NXVqbS0VLGxsW7tsbGxKi4ubvSYkpISj/7jxo3Tzp07dfToUUnSNddco9LSUlewtnfvXhUWFuqGG25ospba2lpVV1e7bQCAtseKNgAAAABohsrKSjU0NCggIMCtPSAgQOXl5Y0eU15e3mj/+vp6VVZWKigoSDfddJO+/vprXXPNNTIMQ/X19frtb3+r++67r8lasrOztXDhwpYPCgDQIqxoAwAAAIAWsFgsbvuGYXi0na7/ye2bN2/Wgw8+qNzcXO3atUsvvviiXn31Vf3pT39q8pwZGRk6dOiQaztw4EBzhwMAaAFWtAEAAABAM/Tp00c+Pj4eq9cqKio8Vq2dEBgY2Gh/X19f9e7dW5I0b948JSYmur737dJLL1VNTY3uuusuZWZmqksXz/USVqtVVqvVjGEBAFqAFW0AAAAA0Ax+fn6y2+0qKipyay8qKlJ0dHSjx0RFRXn037RpkyIjI9W1a1dJ0nfffecRpvn4+MgwDN5UCQDtHEEbAAAAADRTenq6nnzySa1Zs0Z79uzR7Nmz5XA4lJKSIun4I53Tpk1z9U9JSdEXX3yh9PR07dmzR2vWrNHq1as1Z84cV58JEyYoLy9P69at0759+1RUVKR58+bpxhtvlI+PT5uPEQBw5nh0FAAAAACaKSEhQVVVVcrKypLT6VRERIQKCwsVEhIiSXI6nXI4HK7+oaGhKiws1OzZs7V8+XL169dPjz32mCZOnOjqM3fuXFksFs2dO1cHDx5U3759NWHCBD344INtPj4AwNkhaAMAAACAFkhNTVVqamqjn+Xn53u0jRo1Srt27WryfL6+vpo/f77mz59vVokAgDbCo6MAAAAAAACACQjaAAAAAAAAABMQtAEAICk3N1ehoaGy2Wyy2+3atm3bKfvX1tYqMzNTISEhslqtGjRokNasWdNG1QIAAABoj/iONgBAp1dQUKC0tDTl5uZq5MiRWrlypeLi4vThhx+qf//+jR4zefJkffXVV1q9erUuuugiVVRUqL6+vo0rBwAAANCeELQBADq9JUuWKCkpScnJyZKknJwcbdy4UXl5ecrOzvbo/8Ybb2jLli3au3evevXqJUkaMGBAW5YMAAAAoB3i0VEAQKdWV1en0tJSxcbGurXHxsaquLi40WNeeeUVRUZGatGiRbrgggt08cUXa86cOfrvf//b5HVqa2tVXV3ttgEAAAA4t7CiDQDQqVVWVqqhoUEBAQFu7QEBASovL2/0mL1792r79u2y2Wx66aWXVFlZqdTUVH3zzTdNfk9bdna2Fi5caHr9AAAAANoPVrQBACDJYrG47RuG4dF2wrFjx2SxWPT000/ryiuv1Pjx47VkyRLl5+c3uaotIyNDhw4dcm0HDhwwfQwAAAAAvIsVbQCATq1Pnz7y8fHxWL1WUVHhscrthKCgIF1wwQXy9/d3tYWFhckwDP3f//2fBg8e7HGM1WqV1Wo1t3gAAGAK+71rW/0alvo6+Z+0f928dTJ8/Vr9uqWPTGv1awD4ASvaAACdmp+fn+x2u4qKitzai4qKFB0d3egxI0eO1JdffqkjR4642j755BN16dJFF154YavWCwAAAKD9ImgDAHR66enpevLJJ7VmzRrt2bNHs2fPlsPhUEpKiqTjj31Om/bDb4OnTp2q3r176/bbb9eHH36orVu36t5779Udd9yhbt26eWsYAAAAALyMR0cBAJ1eQkKCqqqqlJWVJafTqYiICBUWFiokJESS5HQ65XA4XP179OihoqIizZgxQ5GRkerdu7cmT56sBx54wFtDAAAAANAOELQBACApNTVVqampjX6Wn5/v0TZ06FCPx00BAAAAdG48OgoAAAAAAACYgKANAAAAAAAAMAFBGwAAAAAAAGACgjYAAAAAAADABARtAAAAAAAAgAkI2gAAAAAAAAATELQBAAAAAAAAJiBoAwAAAAAAAExA0AYAAAAAAACYgKANAAAAAAAAMAFBGwAAAAAAAGACgjYAAAAAAADABARtAAAAAAAAgAl8vV0AAAAAAADnOsOnqw4Nm+K2D+DcQ9AGAAAAAEBrs1hk+Pp5uwoArczrj47m5uYqNDRUNptNdrtd27Zta7Kv0+nU1KlTNWTIEHXp0kVpaWkeffLz82WxWDy277//vhVHAQAAAAAAgM7Oq0FbQUGB0tLSlJmZqbKyMsXExCguLk4Oh6PR/rW1terbt68yMzM1fPjwJs/bs2dPOZ1Ot81ms7XWMAAAAAAAAADvBm1LlixRUlKSkpOTFRYWppycHAUHBysvL6/R/gMGDNCyZcs0bdo0+fv7N3lei8WiwMBAtw0AAAAAAABoTV4L2urq6lRaWqrY2Fi39tjYWBUXF7fo3EeOHFFISIguvPBC/fKXv1RZWVmLzgcAAAAAAACcjteCtsrKSjU0NCggIMCtPSAgQOXl5c0+79ChQ5Wfn69XXnlFzz77rGw2m0aOHKlPP/20yWNqa2tVXV3ttgEAAAAAAABnw+svQ7BYLG77hmF4tJ2Nq6++WrfccouGDx+umJgYPffcc7r44ov1l7/8pcljsrOz5e/v79qCg4ObfX0AAAAAAAB0Tl4L2vr06SMfHx+P1WsVFRUeq9xaokuXLrriiitOuaItIyNDhw4dcm0HDhww7foAAAAAAADoHLwWtPn5+clut6uoqMitvaioSNHR0aZdxzAM7d69W0FBQU32sVqt6tmzp9sGAAAAAAAAnA1fb148PT1diYmJioyMVFRUlFatWiWHw6GUlBRJx1eaHTx4UGvXrnUds3v3bknHX3jw9ddfa/fu3fLz81N4eLgkaeHChbr66qs1ePBgVVdX67HHHtPu3bu1fPnyNh8fAAAAAAAAOg+vBm0JCQmqqqpSVlaWnE6nIiIiVFhYqJCQEEmS0+mUw+FwO2bEiBGufy4tLdUzzzyjkJAQ7d+/X5L07bff6q677lJ5ebn8/f01YsQIbd26VVdeeWWbjQsAAAAAAACdj1eDNklKTU1Vampqo5/l5+d7tBmGccrzLV26VEuXLjWjNAAAAAAAAOCMef2towAAAAAAAMC5gKANAAAAAAAAMAFBGwAAAAAAAGACr39HG4DmMQxDNTU1rv3u3bvLYrF4sSIAAAAAADo3gjagg6qpqVF8fLxrf8OGDerRo4cXKwIAAAAAoHPj0VEAAAAAAADABARtAAAAAAAAgAkI2gAAAAAAAAATELQBACApNzdXoaGhstlsstvt2rZtW5N9N2/eLIvF4rF99NFHbVgxAAAAgPaGoA0A0OkVFBQoLS1NmZmZKisrU0xMjOLi4uRwOE553Mcffyyn0+naBg8e3EYVAwAAAGiPCNoAAJ3ekiVLlJSUpOTkZIWFhSknJ0fBwcHKy8s75XHnn3++AgMDXZuPj08bVQwAAACgPSJoAwB0anV1dSotLVVsbKxbe2xsrIqLi0957IgRIxQUFKTRo0frnXfeOWXf2tpaVVdXu20AAAAAzi0EbQCATq2yslINDQ0KCAhwaw8ICFB5eXmjxwQFBWnVqlVav369XnzxRQ0ZMkSjR4/W1q1bm7xOdna2/P39XVtwcLCp4wAAAADgfb7eLgAAgPbAYrG47RuG4dF2wpAhQzRkyBDXflRUlA4cOKDFixfr2muvbfSYjIwMpaenu/arq6sJ2wAAAIBzDEEb0Ars965t9WtY6uvkf9L+dfPWyfD1a/Xrlj4yrdWvAbSlPn36yMfHx2P1WkVFhccqt1O5+uqr9be//a3Jz61Wq6xWa7PrBAAAAND+8egoAKBT8/Pzk91uV1FRkVt7UVGRoqOjz/g8ZWVlCgoKMrs8AAAAAB0IK9oAAJ1eenq6EhMTFRkZqaioKK1atUoOh0MpKSmSjj/2efDgQa1de3y1ak5OjgYMGKBLLrlEdXV1+tvf/qb169dr/fr13hwGAAAAAC8jaAMAdHoJCQmqqqpSVlaWnE6nIiIiVFhYqJCQEEmS0+mUw+Fw9a+rq9OcOXN08OBBdevWTZdccolee+01jR8/3ltDAAAAANAOELQBACApNTVVqampjX6Wn5/vtv/73/9ev//979ugKgAAAAAdCd/RBgAAAAAAAJiAoA0AAAAAAAAwAUEbAAAAALRAbm6uQkNDZbPZZLfbtW3btlP237Jli+x2u2w2mwYOHKgVK1Z49Pn22281ffp0BQUFyWazKSwsTIWFha01BACASQjaAAAAAKCZCgoKlJaWpszMTJWVlSkmJkZxcXFuL9E52b59+zR+/HjFxMSorKxM999/v2bOnOn25uq6ujqNHTtW+/fv1wsvvKCPP/5YTzzxhC644IK2GhYAoJl4GQIAAAAANNOSJUuUlJSk5ORkSVJOTo42btyovLw8ZWdne/RfsWKF+vfvr5ycHElSWFiYdu7cqcWLF2vixImSpDVr1uibb75RcXGxunbtKkmuN2EDANo3VrQBAAAAQDPU1dWptLRUsbGxbu2xsbEqLi5u9JiSkhKP/uPGjdPOnTt19OhRSdIrr7yiqKgoTZ8+XQEBAYqIiNBDDz2khoaGJmupra1VdXW12wYAaHusaAM6KMOnqw4Nm+K2DwAAgLZTWVmphoYGBQQEuLUHBASovLy80WPKy8sb7V9fX6/KykoFBQVp7969evvtt3XzzTersLBQn376qaZPn676+nr98Y9/bPS82dnZWrhwoTkDAwA0GyvagI7KYpHh6+faZLF4uyIAAIBOyfKj/w8zDMOj7XT9T24/duyYzj//fK1atUp2u1033XSTMjMzlZeX1+Q5MzIydOjQIdd24MCB5g4HANACrGhDh2EYhmpqalz73bt3P+X/wAAAAACtqU+fPvLx8fFYvVZRUeGxau2EwMDARvv7+vqqd+/ekqSgoCB17dpVPj4+rj5hYWEqLy9XXV2d/Pz8PM5rtVpltVpbOiQAQAuxog0dRk1NjeLj413byaEbAAAA0Nb8/Pxkt9tVVFTk1l5UVKTo6OhGj4mKivLov2nTJkVGRrpefDBy5Eh99tlnOnbsmKvPJ598oqCgoEZDNgBA+0HQBgAAAADNlJ6erieffFJr1qzRnj17NHv2bDkcDqWkpEg6/kjntGnTXP1TUlL0xRdfKD09XXv27NGaNWu0evVqzZkzx9Xnt7/9raqqqjRr1ix98skneu211/TQQw9p+vTpbT4+oDUZhqEjR464thOPUQMdGY+OAgAAAEAzJSQkqKqqSllZWXI6nYqIiFBhYaFCQkIkSU6nUw6Hw9U/NDRUhYWFmj17tpYvX65+/frpscce08SJE119goODtWnTJs2ePVvDhg3TBRdcoFmzZukPf/hDm48PaE0nnlo6YcOGDerRo4cXKwJajqANAAAAAFogNTVVqampjX6Wn5/v0TZq1Cjt2rXrlOeMiorSu+++a0Z5AIA2xKOjAAAAAAAAgAkI2gAAAAAAAAATELQBAAAAAAAAJiBoAwAAAAAAAExA0AYAAAAAAACYoFlvHX3jjTfUo0cPXXPNNZKk5cuX64knnlB4eLiWL1+un/3sZ6YWCQDAj11//fWyWCxNfv7222+3YTUAgI5mxIgRjc4jFotFNptNF110kW677TZdf/31XqgOANBRNWtF27333qvq6mpJ0gcffKDf/e53Gj9+vPbu3av09HRTCwQAoDGXXXaZhg8f7trCw8NVV1enXbt26dJLL/V2eQCAdu4Xv/iF9u7dq+7du+v666/Xddddpx49eujzzz/XFVdcIafTqTFjxmjDhg3eLhUA0IE0a0Xbvn37FB4eLklav369fvnLX+qhhx7Srl27NH78eFMLRPtnv3dtm1zHUl8n/5P2r5u3ToavX6tes/SRaa16fgDNt3Tp0kbbFyxYoCNHjrRxNQCAjqayslK/+93vNG/ePLf2Bx54QF988YU2bdqk+fPn609/+pPi4+O9VCUAoKNp1oo2Pz8/fffdd5KkN998U7GxsZKkXr16uVa6AQDgDbfccovWrFnj7TIAAO3cc889pylTpni033TTTXruueckSVOmTNHHH3/c1qUBADqwZq1ou+aaa5Senq6RI0fqvffeU0FBgSTpk08+0YUXXmhqgQAAnI2SkhLZbDZvlwEAaOdsNpuKi4t10UUXubUXFxe75pFjx47JarV6ozwAQAfVrKDt8ccfV2pqql544QXl5eXpggsukCS9/vrr+sUvfmFqgQAANOY3v/mN275hGHI6ndq5c6fHY0AAAPzYjBkzlJKSotLSUl1xxRWyWCx677339OSTT+r++++XJG3cuFEjRozwcqUAgI6kWUFb//799eqrr3q0N/V9OQAAmM3f399tv0uXLhoyZIiysrJcX2kAAEBT5s6dq9DQUD3++OP63//9X0nSkCFD9MQTT2jq1KmSpJSUFP32t7/1ZpkAgA6mWUGbJDU0NOjll1/Wnj17ZLFYFBYWpvj4ePn4+JhZHwAAjXrqqae8XQIAoIO7+eabdfPNNzf5ebdu3dqwGqD94IV3QPM1K2j77LPPNH78eB08eFBDhgyRYRj65JNPFBwcrNdee02DBg0yu04AADx8++23euGFF/T555/r3nvvVa9evbRr1y4FBAS4vtYAAICmnJhH9u7dqzlz5jCPAABarFlB28yZMzVo0CC9++676tWrlySpqqpKt9xyi2bOnKnXXnvN1CIBAPix999/X6NHj9Z5552n/fv3684771SvXr300ksv6YsvvtDatW3zm1gAQMf0/vvva8yYMfL399f+/fuVnJzMPAIAaLEuzTloy5YtWrRokStkk6TevXvrz3/+s7Zs2WJacQAANCU9PV233367Pv30U7e3jMbFxWnr1q1erAwA0BGkp6frtttuYx4BAJiqWUGb1WrV4cOHPdqPHDkiP7/WfZ4aAABJ+uc//6m7777bo/2CCy5QeXm5FyoCAHQkzCMAgNbQrKDtl7/8pe666y7t2LFDhmHIMAy9++67SklJ0Y033mh2jQAAeLDZbKqurvZo//jjj9W3b18vVAQA6EiYRwAAraFZQdtjjz2mQYMGKSoqSjabTTabTdHR0brooou0bNkys2sEAMBDfHy8srKydPToUUmSxWKRw+HQfffdp4kTJ3q5OgBAe8c8AgBoDc16GcJ5552nDRs26LPPPtOHH34oSQoPD9dFF11kanHAyQyfrjo0bIrbPoDOa/HixRo/frzOP/98/fe//9WoUaNUXl6uq6++Wg8++KC3ywMAtHPMIwCA1tCsoE2SVq9eraVLl+rTTz+VJA0ePFhpaWlKTk42rTjAjcUiw5fvAARwXM+ePbV9+3a98847Ki0t1bFjx3T55ZdrzJgx3i4NANABMI8AAFpDsx4dnTdvnmbNmqUJEybo+eef1/PPP68JEyZo9uzZmjt3rtk1AgDQqLfeektFRUX66KOP9NFHH+mZZ57RHXfcoTvuuOOsz5Wbm6vQ0FDZbDbZ7XZt27btjI77xz/+IV9fX1122WVnfU0AgHeZOY8AACA1c0VbXl6ennjiCU2Z8sNjfDfeeKOGDRumGTNm6IEHHjCtQAAAGrNw4UJlZWUpMjJSQUFBslgszT5XQUGB0tLSlJubq5EjR2rlypWKi4vThx9+qP79+zd53KFDhzRt2jSNHj1aX331VbOvDwBoe2bOIwAAnNCsoK2hoUGRkZEe7Xa7XfX19S0uCgCA01mxYoXy8/OVmJjY4nMtWbJESUlJrq8/yMnJ0caNG5WXl6fs7Owmj7v77rs1depU+fj46OWXX25xHQCAtmPmPAIAwAnNenT0lltuUV5enkf7qlWrdPPNN7e4KAAATqeurk7R0dGmnKe0tFSxsbFu7bGxsSouLm7yuKeeekqff/655s+ff0bXqa2tVXV1tdsGAPAes+YRAABO1qKXIWzatElXX321JOndd9/VgQMHNG3aNKWnp7v6LVmypOVVAgDwI8nJyXrmmWc0b968Fp2nsrJSDQ0NCggIcGsPCAhQeXl5o8d8+umnuu+++7Rt2zb5+p7ZVJqdna2FCxe2qFYAgHnMmkcANJ/h01WHhk1x2wc6umYFbf/+9791+eWXS5I+//xzSVLfvn3Vt29f/fvf/3b143sOAACt5fvvv9eqVav05ptvatiwYera1f1/zM72Fz0/nrMMw2h0HmtoaNDUqVO1cOFCXXzxxWd8/oyMDLdfRFVXVys4OPisagQAmMfseQRAM1gsMnz9vF0FYKpmBW3vvPOO2XUAAHBW3n//fdebPk/+JY90dr/o6dOnj3x8fDxWr1VUVHiscpOkw4cPa+fOnSorK9M999wjSTp27JgMw5Cvr682bdqkn//85x7HWa1WWa3WM64LANC6zJpHAAA4WbMfHQUAwJvM+qWPn5+f7Ha7ioqK9Otf/9rVXlRUpPj4eI/+PXv21AcffODWlpubq7ffflsvvPCCQkNDTakLANC6WDwAAGgNBG0AgE4vPT1diYmJioyMVFRUlFatWiWHw6GUlBRJxx/7PHjwoNauXasuXbooIiLC7fjzzz9fNpvNox0AAABA50LQBgDo9BISElRVVaWsrCw5nU5FRESosLBQISEhkiSn0ymHw+HlKgEAAAC0dwRtAABISk1NVWpqaqOf5efnn/LYBQsWaMGCBeYXBQAAAKBD6eLtAgAAAAAAAIBzAUEbAAAAAAAAYAKCNgAAAAAAAMAEBG0AAAAAAACACQjaAAAAAAAAABMQtAEAAAAAAAAmIGgDAAAAAAAATOD1oC03N1ehoaGy2Wyy2+3atm1bk32dTqemTp2qIUOGqEuXLkpLS2u03/r16xUeHi6r1arw8HC99NJLrVQ9AAAAAAAAcJxXg7aCggKlpaUpMzNTZWVliomJUVxcnBwOR6P9a2tr1bdvX2VmZmr48OGN9ikpKVFCQoISExP1r3/9S4mJiZo8ebJ27NjRmkMBAAAAAABAJ+fVoG3JkiVKSkpScnKywsLClJOTo+DgYOXl5TXaf8CAAVq2bJmmTZsmf3//Rvvk5ORo7NixysjI0NChQ5WRkaHRo0crJyenFUdiPsMwdOTIEddmGIa3SwIAAAAAAMApeC1oq6urU2lpqWJjY93aY2NjVVxc3OzzlpSUeJxz3LhxpzxnbW2tqqur3TZvq6mpUXx8vGurqanxdkkAAAAAAAA4Ba8FbZWVlWpoaFBAQIBbe0BAgMrLy5t93vLy8rM+Z3Z2tvz9/V1bcHBws68PAAAAAACAzsnrL0OwWCxu+4ZheLS19jkzMjJ06NAh13bgwIEWXR8AAAAAAACdj6+3LtynTx/5+Ph4rDSrqKjwWJF2NgIDA8/6nFarVVartdnXBAAAAAAAALy2os3Pz092u11FRUVu7UVFRYqOjm72eaOiojzOuWnTphadEwAAAAAAADgdr61ok6T09HQlJiYqMjJSUVFRWrVqlRwOh1JSUiQdf6Tz4MGDWrt2reuY3bt3S5KOHDmir7/+Wrt375afn5/Cw8MlSbNmzdK1116rhx9+WPHx8dqwYYPefPNNbd++vc3HBwAAAAAAgM7Dq0FbQkKCqqqqlJWVJafTqYiICBUWFiokJESS5HQ65XA43I4ZMWKE659LS0v1zDPPKCQkRPv375ckRUdHa926dZo7d67mzZunQYMGqaCgQFdddVWbjQsAAAAAAACdj1eDNklKTU1Vampqo5/l5+d7tBmGcdpzTpo0SZMmTWppaQAAAAAAAMAZ83rQ1hHZ7117+k4tZKmvk/9J+9fNWyfD16/Vr1v6yLRWvwYAAAAAAMC5yGsvQwAAAAAAAADOJQRtAAAAAAAAgAkI2gAAAAAAAAATELQBAAAAAAAAJiBoAwAAAAAAAExA0AYAAAAAAACYgKANAAAAAAAAMAFBGwAAAAAAAGACgjYAAAAAAADABARt7ZTh01WHhk1xbYZPV2+XBAAAAKARubm5Cg0Nlc1mk91u17Zt207Zf8uWLbLb7bLZbBo4cKBWrFjRZN9169bJYrHoV7/6lclVAwBaA0Fbe2WxyPD1c22yWLxdEQAAAIAfKSgoUFpamjIzM1VWVqaYmBjFxcXJ4XA02n/fvn0aP368YmJiVFZWpvvvv18zZ87U+vXrPfp+8cUXmjNnjmJiYlp7GAAAkxC0AQAAAEAzLVmyRElJSUpOTlZYWJhycnIUHBysvLy8RvuvWLFC/fv3V05OjsLCwpScnKw77rhDixcvduvX0NCgm2++WQsXLtTAgQPbYigAABMQtAEAAABAM9TV1am0tFSxsbFu7bGxsSouLm70mJKSEo/+48aN086dO3X06FFXW1ZWlvr27aukpKQzqqW2tlbV1dVuG4COxTAMHTlyxLUZhuHtktAMvt4uAAAAAAA6osrKSjU0NCggIMCtPSAgQOXl5Y0eU15e3mj/+vp6VVZWKigoSP/4xz+0evVq7d69+4xryc7O1sKFC896DADaj5qaGsXHx7v2N2zYoB49enixIjQHK9oAAAAAoAUsP/o+ZcMwPNpO1/9E++HDh3XLLbfoiSeeUJ8+fc64hoyMDB06dMi1HThw4CxGAAAwC0EbAAA6uzfGbd++XSNHjlTv3r3VrVs3DR06VEuXLm3DagEA7UGfPn3k4+PjsXqtoqLCY9XaCYGBgY329/X1Ve/evfX5559r//79mjBhgnx9feXr66u1a9fqlVdeka+vrz7//PNGz2u1WtWzZ0+3DQDQ9nh0FADQ6Z14Y1xubq5GjhyplStXKi4uTh9++KH69+/v0b979+665557NGzYMHXv3l3bt2/X3Xffre7du+uuu+7ywggAAN7g5+cnu92uoqIi/frXv3a1FxUVuT3+dbKoqCj9/e9/d2vbtGmTIiMj1bVrVw0dOlQffPCB2+dz587V4cOHtWzZMgUHB5s/EACAaQjaAACd3slvjJOknJwcbdy4UXl5ecrOzvboP2LECI0YMcK1P2DAAL344ovatm0bQRsAdDLp6elKTExUZGSkoqKitGrVKjkcDqWkpEg6/kjnwYMHtXbtWklSSkqKHn/8caWnp+vOO+9USUmJVq9erWeffVaSZLPZFBER4XaN8847T5I82gEA7Q9BGwCgUzvxxrj77rvPrf1Ub4z7sbKyMhUXF+uBBx5osk9tba1qa2td+7wNDgDODQkJCaqqqlJWVpacTqciIiJUWFiokJAQSZLT6ZTD4XD1Dw0NVWFhoWbPnq3ly5erX79+euyxxzRx4kRvDQEAYCKCNgBAp9acN8adcOGFF+rrr79WfX29FixY4FoR1xjeBgcA567U1FSlpqY2+ll+fr5H26hRo7Rr164zPn9j5wAAtE+8DAEAAJ39G+Mkadu2bdq5c6dWrFihnJwc12M/jeFtcAAAAMC5jxVtAIBOrTlvjDshNDRUknTppZfqq6++0oIFCzRlypRG+1qtVlmtVnOKBgAAANAusaINANCpnfzGuJMVFRUpOjr6jM9jGIbbd7ABAAAA6HxY0QYA6PTO9o1xy5cvV//+/TV06FBJ0vbt27V48WLNmDHDa2MAAABA67Hfu7bVr2Gpr5P/SfvXzVsnw9ev1a9b+si0Vr9GZ0LQBgDo9M72jXHHjh1TRkaG9u3bJ19fXw0aNEh//vOfdffdd3trCAAAAADaAYI2AAB0dm+MmzFjBqvXAAAAAHjgO9oAAAAAAAAAExC0AQAAAAAAACYgaAMAAAAAAABMQNAGAAAAAAAAmICgDQAAAAAAADABQRsAAAAAAABgAl9vFwAAAAAAANDZGT5ddWjYFLd9dDwEbQAAAAAAAN5mscjw9fN2FWghHh0FAAAAAAAATEDQBgAAAAAAAJiAoA0AAAAAAAAwAUEbAAAAAAAAYAKCNgAAAAAAAMAEBG0AAAAAAACACQjaAAAAAAAAABMQtAEAAAAAAAAmIGgDAAAAAAAATODr7QIAAAAAAACAHzMMQzU1Na797t27y2KxeLGi0yNoAwAAAAAAQLtTU1Oj+Ph41/6GDRvUo0cPL1Z0ejw6CgAAAAAAAJiAoA0AAAAAAAAwAUEbAAAAAAAAYAK+ow0AAAAAAABnzH7v2ja5jqW+Tv4n7V83b50MX79WvWbpI9NadDwr2gAAAAAAAAATELQBAAAAAAAAJiBoAwAAAAAAAExA0AYAAAAAAACYgKANAAAAAAAAMAFvHQUAAAAAAEC7Y/h01aFhU9z22zuCNgAAAAAAALQ/FosMXz9vV3FWeHQUAAAAAAAAMAFBGwAAAAAAAGACgjYAAAAAAADABARtAAAAAAAAgAkI2gAAkJSbm6vQ0FDZbDbZ7XZt27atyb4vvviixo4dq759+6pnz56KiorSxo0b27BaAAAAAO0RQRsAoNMrKChQWlqaMjMzVVZWppiYGMXFxcnhcDTaf+vWrRo7dqwKCwtVWlqq66+/XhMmTFBZWVkbVw4AAACgPSFoAwB0ekuWLFFSUpKSk5MVFhamnJwcBQcHKy8vr9H+OTk5+v3vf68rrrhCgwcP1kMPPaTBgwfr73//extXDgAAAKA9IWgDAHRqdXV1Ki0tVWxsrFt7bGysiouLz+gcx44d0+HDh9WrV6/WKBEAAABAB+Hr7QIAAPCmyspKNTQ0KCAgwK09ICBA5eXlZ3SORx99VDU1NZo8eXKTfWpra1VbW+var66ubl7BAAAAANotVrQBACDJYrG47RuG4dHWmGeffVYLFixQQUGBzj///Cb7ZWdny9/f37UFBwe3uGYAAAAA7QtBGwCgU+vTp498fHw8Vq9VVFR4rHL7sYKCAiUlJem5557TmDFjTtk3IyNDhw4dcm0HDhxoce0AAAAA2heCNgBAp+bn5ye73a6ioiK39qKiIkVHRzd53LPPPqvbbrtNzzzzjG644YbTXsdqtapnz55uGwAAAIBzC9/RBgDo9NLT05WYmKjIyEhFRUVp1apVcjgcSklJkXR8NdrBgwe1du1aScdDtmnTpmnZsmW6+uqrXavhunXrJn9/f6+NAwAAAIB3eX1FW25urkJDQ2Wz2WS327Vt27ZT9t+yZYvsdrtsNpsGDhyoFStWuH2en58vi8XisX3//fetOQwAQAeWkJCgnJwcZWVl6bLLLtPWrVtVWFiokJAQSZLT6ZTD4XD1X7lyperr6zV9+nQFBQW5tlmzZnlrCAAAAADaAa+uaCsoKFBaWppyc3M1cuRIrVy5UnFxcfrwww/Vv39/j/779u3T+PHjdeedd+pvf/ub/vGPfyg1NVV9+/bVxIkTXf169uypjz/+2O1Ym83W6uMBAHRcqampSk1NbfSz/Px8t/3Nmze3fkEAAAAAOhyvBm1LlixRUlKSkpOTJUk5OTnauHGj8vLylJ2d7dF/xYoV6t+/v3JyciRJYWFh2rlzpxYvXuwWtFksFgUGBrbJGAAAAAAAAADJi4+O1tXVqbS0VLGxsW7tsbGxKi4ubvSYkpISj/7jxo3Tzp07dfToUVfbkSNHFBISogsvvFC//OUvVVZWdspaamtrVV1d7bYBAAAAAAAAZ8NrQVtlZaUaGhoUEBDg1h4QEOD6UukfKy8vb7R/fX29KisrJUlDhw5Vfn6+XnnlFT377LOy2WwaOXKkPv300yZryc7Olr+/v2sLDg5u4egAAAAAAADQ2Xj9ZQgWi8Vt3zAMj7bT9T+5/eqrr9Ytt9yi4cOHKyYmRs8995wuvvhi/eUvf2nynBkZGTp06JBrO3DgQHOHAwAAAAAAgE7Ka9/R1qdPH/n4+HisXquoqPBYtXZCYGBgo/19fX3Vu3fvRo/p0qWLrrjiilOuaLNarbJarWc5AgAAAAAAAOAHXlvR5ufnJ7vdrqKiIrf2oqIiRUdHN3pMVFSUR/9NmzYpMjJSXbt2bfQYwzC0e/duBQUFmVM4AAAAAAAA0AivPjqanp6uJ598UmvWrNGePXs0e/ZsORwOpaSkSDr+SOe0adNc/VNSUvTFF18oPT1de/bs0Zo1a7R69WrNmTPH1WfhwoXauHGj9u7dq927dyspKUm7d+92nRMAAAAAAABoDV57dFSSEhISVFVVpaysLDmdTkVERKiwsFAhISGSJKfTKYfD4eofGhqqwsJCzZ49W8uXL1e/fv302GOPaeLEia4+3377re666y6Vl5fL399fI0aM0NatW3XllVe2+fgAAAAAAADQeXg1aJOk1NRUpaamNvpZfn6+R9uoUaO0a9euJs+3dOlSLV261KzyAAAAAAAAgDPi9beOAgAAAEBHlpubq9DQUNlsNtntdm3btu2U/bds2SK73S6bzaaBAwdqxYoVbp8/8cQTiomJ0c9+9jP97Gc/05gxY/Tee++15hAAACYhaAMAAACAZiooKFBaWpoyMzNVVlammJgYxcXFuX0Fzsn27dun8ePHKyYmRmVlZbr//vs1c+ZMrV+/3tVn8+bNmjJlit555x2VlJSof//+io2N1cGDB9tqWACAZiJoAwAAAIBmWrJkiZKSkpScnKywsDDl5OQoODhYeXl5jfZfsWKF+vfvr5ycHIWFhSk5OVl33HGHFi9e7Orz9NNPKzU1VZdddpmGDh2qJ554QseOHdNbb73VVsMCADQTQRsAAAAANENdXZ1KS0sVGxvr1h4bG6vi4uJGjykpKfHoP27cOO3cuVNHjx5t9JjvvvtOR48eVa9evZqspba2VtXV1W4bAKDtEbQBAAAAQDNUVlaqoaFBAQEBbu0BAQEqLy9v9Jjy8vJG+9fX16uysrLRY+677z5dcMEFGjNmTJO1ZGdny9/f37UFBwef5WgAAGYgaAMAAACAFrBYLG77hmF4tJ2uf2PtkrRo0SI9++yzevHFF2Wz2Zo8Z0ZGhg4dOuTaDhw4cDZDAACYxNfbBQAAAABAR9SnTx/5+Ph4rF6rqKjwWLV2QmBgYKP9fX191bt3b7f2xYsX66GHHtKbb76pYcOGnbIWq9Uqq9XajFEAAMzEijYAAAAAaAY/Pz/Z7XYVFRW5tRcVFSk6OrrRY6Kiojz6b9q0SZGRkerataur7ZFHHtGf/vQnvfHGG4qMjDS/eABAqyBoAwAAAIBmSk9P15NPPqk1a9Zoz549mj17thwOh1JSUiQdf6Rz2rRprv4pKSn64osvlJ6erj179mjNmjVavXq15syZ4+qzaNEizZ07V2vWrNGAAQNUXl6u8vJyHTlypM3HBwA4Ozw6CgAAAADNlJCQoKqqKmVlZcnpdCoiIkKFhYUKCQmRJDmdTjkcDlf/0NBQFRYWavbs2Vq+fLn69eunxx57TBMnTnT1yc3NVV1dnSZNmuR2rfnz52vBggVtMi4AQPMQtAEAAABAC6Smpio1NbXRz/Lz8z3aRo0apV27djV5vv3795tUGQCgrfHoKAAAAAAAAGACgjYAAAAAAADABARtAAAAAAAAgAkI2gAAAAAAAAATELQBAAAAAAAAJiBoAwAAAAAAAExA0AYAAAAAAACYgKANAAAAAAAAMAFBGwAAAAAAAGACgjYAAAAAAADABARtAAAAAAAAgAkI2gAAAAAAAAATELQBAAAAAAAAJiBoAwAAAAAAAExA0AYAgKTc3FyFhobKZrPJbrdr27ZtTfZ1Op2aOnWqhgwZoi5duigtLa3tCgUAAADQbhG0AQA6vYKCAqWlpSkzM1NlZWWKiYlRXFycHA5Ho/1ra2vVt29fZWZmavjw4W1cLQAAAID2iqANANDpLVmyRElJSUpOTlZYWJhycnIUHBysvLy8RvsPGDBAy5Yt07Rp0+Tv79/G1QIAAABorwjaAACdWl1dnUpLSxUbG+vWHhsbq+LiYi9VBQAAAKAj8vV2AQAAeFNlZaUaGhoUEBDg1h4QEKDy8nLTrlNbW6va2lrXfnV1tWnnBgAAANA+sKINAABJFovFbd8wDI+2lsjOzpa/v79rCw4ONu3cAAAAANoHgjYAQKfWp08f+fj4eKxeq6io8Fjl1hIZGRk6dOiQaztw4IBp5wYAAADQPhC0AQA6NT8/P9ntdhUVFbm1FxUVKTo62rTrWK1W9ezZ020DAAAAcG7hO9oAAJ1eenq6EhMTFRkZqaioKK1atUoOh0MpKSmSjq9GO3jwoNauXes6Zvfu3ZKkI0eO6Ouvv9bu3bvl5+en8PBwbwwBAAAAQDtA0AYA6PQSEhJUVVWlrKwsOZ1ORUREqLCwUCEhIZIkp9Mph8PhdsyIESNc/1xaWqpnnnlGISEh2r9/f1uWDgAAAKAdIWgDAEBSamqqUlNTG/0sPz/fo80wjFauCAAAAEBHw3e0AQAAAAAAACYgaAMAAAAAAABMQNAGAAAAAAAAmICgDQAAAAAAADABQRsAAAAAAABgAoI2AAAAAAAAwAQEbQAAAAAAAIAJCNoAAAAAAAAAExC0AQAAAAAAACYgaAMAAAAAAABMQNAGAAAAAAAAmICgDQAAAAAAADABQRsAAAAAAABgAoI2AAAAAAAAwAQEbQAAAAAAAIAJCNoAAAAAAAAAExC0AQAAAAAAACYgaAMAAAAAAABMQNAGAAAAAAAAmICgDQAAAAAAADABQRsAAAAAAABgAoI2AAAAAAAAwAQEbQAAAAAAAIAJCNoAAAAAAAAAExC0AQAAAAAAACYgaAMAAAAAAABMQNAGAAAAAAAAmICgDQAAAAAAADABQRsAAAAAAABgAoI2AAAAAAAAwAQEbQAAAAAAAIAJCNoAAAAAAAAAExC0AQAAAAAAACYgaAMAAAAAAABM4PWgLTc3V6GhobLZbLLb7dq2bdsp+2/ZskV2u102m00DBw7UihUrPPqsX79e4eHhslqtCg8P10svvdRa5QMAzhGtMR8BADoHfqYBAJzg1aCtoKBAaWlpyszMVFlZmWJiYhQXFyeHw9Fo/3379mn8+PGKiYlRWVmZ7r//fs2cOVPr16939SkpKVFCQoISExP1r3/9S4mJiZo8ebJ27NjRVsMCAHQwrTEfAQA6B36mAQCczKtB25IlS5SUlKTk5GSFhYUpJydHwcHBysvLa7T/ihUr1L9/f+Xk5CgsLEzJycm64447tHjxYlefnJwcjR07VhkZGRo6dKgyMjI0evRo5eTktNGoAAAdTWvMRwCAzoGfaQAAJ/Na0FZXV6fS0lLFxsa6tcfGxqq4uLjRY0pKSjz6jxs3Tjt37tTRo0dP2aepcwIAOrfWmo8AAOc+fqYBAPyYr7cuXFlZqYaGBgUEBLi1BwQEqLy8vNFjysvLG+1fX1+vyspKBQUFNdmnqXNKUm1trWpra137hw4dkiRVV1c32r+h9r9ND6yDa2rMp8L98MQ9ccf98HSu3pOm7seJdsMw2rKcM9Ja89GPnc1cc67+/ZD4d+bHuB+euCfuuB+eGrsn3ppn+Jmm/eHfGU/8/7s7/o544u+Iu5b+TOO1oO0Ei8Xitm8Yhkfb6fr/uP1sz5mdna2FCxd6tAcHBzdd+DnK/y8p3i6hXeF+eOKeuON+uDvd/Th8+LD8/f3bqJqz0xrz0cmYa47j3xl33A9P3BN33A9Pp7on3ppn+Jmm/eDfGU/cE3fcD0/cE3ct/ZnGa0Fbnz595OPj4/FbmYqKCo/f3pwQGBjYaH9fX1/17t37lH2aOqckZWRkKD093bV/7NgxffPNN+rdu/cpJ7PWVl1dreDgYB04cEA9e/b0Wh3tCffEHffDE/fEXXu5H4Zh6PDhw+rXr5/XamhKa81HP8Zc0zFwPzxxT9xxP9y1l/vhrXmGn2lOr738HWkvuB+euCfuuB+e2ss9OdO5xmtBm5+fn+x2u4qKivTrX//a1V5UVKT4+PhGj4mKitLf//53t7ZNmzYpMjJSXbt2dfUpKirS7Nmz3fpER0c3WYvVapXVanVrO++88852SK2mZ8+e/Av2I9wTd9wPT9wTd+3hfrTXlWytNR/9GHNNx8L98MQ9ccf9cNce7oc35hl+pjlz7eHvSHvC/fDEPXHH/fDUHu7Jmcw1Xn3raHp6up588kmtWbNGe/bs0ezZs+VwOJSScnyZXkZGhqZNm+bqn5KSoi+++ELp6enas2eP1qxZo9WrV2vOnDmuPrNmzdKmTZv08MMP66OPPtLDDz+sN998U2lpaW09PABAB9Ea8xEAoHPgZxoAwMm8+h1tCQkJqqqqUlZWlpxOpyIiIlRYWKiQkBBJktPplMPhcPUPDQ1VYWGhZs+ereXLl6tfv3567LHHNHHiRFef6OhorVu3TnPnztW8efM0aNAgFRQU6Kqrrmrz8QEAOobWmI8AAJ0DP9MAAE7m9ZchpKamKjU1tdHP8vPzPdpGjRqlXbt2nfKckyZN0qRJk8woz6usVqvmz5/vsQS8M+OeuON+eOKeuON+nLnWmI86Av6OuON+eOKeuON+uON+HMfPNE3j74g77ocn7ok77oenjnZPLEZbvwMbAAAAAAAAOAd59TvaAAAAAAAAgHMFQRsAAAAAAABgAoI2AAAAAAAAwAQEbQAAAAAAAIAJCNraqa1bt2rChAnq16+fLBaLXn75ZW+X5DXZ2dm64oor9NOf/lTnn3++fvWrX+njjz/2dllelZeXp2HDhqlnz57q2bOnoqKi9Prrr3u7rHYjOztbFotFaWlp3i7FaxYsWCCLxeK2BQYGersstCPMM+6Ya9wxz5wa8wzzDE6PecYd84w75plTY57p2PMMQVs7VVNTo+HDh+vxxx/3dilet2XLFk2fPl3vvvuuioqKVF9fr9jYWNXU1Hi7NK+58MIL9ec//1k7d+7Uzp079fOf/1zx8fH6z3/+4+3SvO6f//ynVq1apWHDhnm7FK+75JJL5HQ6XdsHH3zg7ZLQjjDPuGOuccc80zTmmR8wz+BUmGfcMc+4Y55pGvPMDzrqPOPr7QLQuLi4OMXFxXm7jHbhjTfecNt/6qmndP7556u0tFTXXnutl6ryrgkTJrjtP/jgg8rLy9O7776rSy65xEtVed+RI0d0880364knntADDzzg7XK8ztfXt8P81gdtj3nGHXONO+aZxjHPuGOewakwz7hjnnHHPNM45hl3HXWeYUUbOpxDhw5Jknr16uXlStqHhoYGrVu3TjU1NYqKivJ2OV41ffp03XDDDRozZoy3S2kXPv30U/Xr10+hoaG66aabtHfvXm+XBHQYzDU/YJ75AfOMO+YZoPmYZ37APPMD5hl3HXWeYUUbOhTDMJSenq5rrrlGERER3i7Hqz744ANFRUXp+++/V48ePfTSSy8pPDzc22V5zbp167Rr1y7985//9HYp7cJVV12ltWvX6uKLL9ZXX32lBx54QNHR0frPf/6j3r17e7s8oF1jrjmOecYd84w75hmg+ZhnjmOeccc8464jzzMEbehQ7rnnHr3//vvavn27t0vxuiFDhmj37t369ttvtX79et16663asmVLp5ycDhw4oFmzZmnTpk2y2WzeLqddOPlRjUsvvVRRUVEaNGiQ/vrXvyo9Pd2LlQHtH3PNccwzP2Ce8cQ8AzQf88xxzDM/YJ7x1JHnGYI2dBgzZszQK6+8oq1bt+rCCy/0djle5+fnp4suukiSFBkZqX/+859atmyZVq5c6eXK2l5paakqKipkt9tdbQ0NDdq6dasef/xx1dbWysfHx4sVel/37t116aWX6tNPP/V2KUC7xlzzA+aZHzDPnB7zDHBmmGd+wDzzA+aZ0+tI8wxBG9o9wzA0Y8YMvfTSS9q8ebNCQ0O9XVK7ZBiGamtrvV2GV4wePdrjDTS33367hg4dqj/84Q+dflKSpNraWu3Zs0cxMTHeLgVol5hrTo95hnnmVJhngFNjnjk95hnmmVPpSPMMQVs7deTIEX322Weu/X379mn37t3q1auX+vfv78XK2t706dP1zDPPaMOGDfrpT3+q8vJySZK/v7+6devm5eq84/7771dcXJyCg4N1+PBhrVu3Tps3b/Z4m1Fn8dOf/tTj+y26d++u3r17d9rvvZgzZ44mTJig/v37q6KiQg888ICqq6t16623ers0tBPMM+6Ya9wxz7hjnvHEPIPTYZ5xxzzjjnnGHfOMp448zxC0tVM7d+7U9ddf79o/8Qzyrbfeqvz8fC9V5R15eXmSpOuuu86t/amnntJtt93W9gW1A1999ZUSExPldDrl7++vYcOG6Y033tDYsWO9XRraif/7v//TlClTVFlZqb59++rqq6/Wu+++q5CQEG+XhnaCecYdc4075hmcDvMMTod5xh3zjDvmGZxOR55nLIZhGN4uAgAAAAAAAOjouni7AAAAAAAAAOBcQNAGAAAAAAAAmICgDQAAAAAAADABQRsAAAAAAABgAoI2AAAAAAAAwAQEbQAAAAAAAIAJCNoAAAAAAAAAExC0AQAAAAAAACYgaAO8qKKiQnfffbf69+8vq9WqwMBAjRs3TiUlJd4uDQBwDmCeAQC0JuYZwJOvtwsAOrOJEyfq6NGj+utf/6qBAwfqq6++0ltvvaVvvvmmVa5XV1cnPz+/Vjk3AKD9YZ4BALQm5hnAEyvaAC/59ttvtX37dj388MO6/vrrFRISoiuvvFIZGRm64YYbXH3uuusuBQQEyGazKSIiQq+++qrrHOvXr9cll1wiq9WqAQMG6NFHH3W7xoABA/TAAw/otttuk7+/v+68805JUnFxsa699lp169ZNwcHBmjlzpmpqatpu8ACAVsc8AwBoTcwzQOMI2gAv6dGjh3r06KGXX35ZtbW1Hp8fO3ZMcXFxKi4u1t/+9jd9+OGH+vOf/ywfHx9JUmlpqSZPnqybbrpJH3zwgRYsWKB58+YpPz/f7TyPPPKIIiIiVFpaqnnz5umDDz7QuHHj9Jvf/Ebvv/++CgoKtH37dt1zzz1tMWwAQBthngEAtCbmGaBxFsMwDG8XAXRW69ev15133qn//ve/uvzyyzVq1CjddNNNGjZsmDZt2qS4uDjt2bNHF198scexN998s77++mtt2rTJ1fb73/9er732mv7zn/9IOv4boBEjRuill15y9Zk2bZq6deumlStXutq2b9+uUaNGqaamRjabrRVHDABoS8wzAIDWxDwDeGJFG+BFEydO1JdffqlXXnlF48aN0+bNm3X55ZcrPz9fu3fv1oUXXtjopCRJe/bs0ciRI93aRo4cqU8//VQNDQ2utsjISLc+paWlys/Pd/0GqkePHho3bpyOHTumffv2mT9IAIDXMM8AAFoT8wzgiZchAF5ms9k0duxYjR07Vn/84x+VnJys+fPna86cOac8zjAMWSwWj7Yf6969u9v+sWPHdPfdd2vmzJkeffv379+MEQAA2jPmGQBAa2KeAdwRtAHtTHh4uF5++WUNGzZM//d//6dPPvmk0d8ChYeHa/v27W5txcXFuvjii13fe9CYyy+/XP/5z3900UUXmV47AKD9Y54BALQm5hl0djw6CnhJVVWVfv7zn+tvf/ub3n//fe3bt0/PP/+8Fi1apPj4eI0aNUrXXnutJk6cqKKiIu3bt0+vv/663njjDUnS7373O7311lv605/+pE8++UR//etf9fjjj5/2N0d/+MMfVFJSounTp2v37t369NNP9corr2jGjBltMWwAQBthngEAtCbmGaAJBgCv+P7774377rvPuPzyyw1/f3/jJz/5iTFkyBBj7ty5xnfffWcYhmFUVVUZt99+u9G7d2/DZrMZERERxquvvuo6xwsvvGCEh4cbXbt2Nfr372888sgjbtcICQkxli5d6nHt9957zxg7dqzRo0cPo3v37sawYcOMBx98sFXHCwBoW8wzAIDWxDwDNI63jgIAAAAAAAAm4NFRAAAAAAAAwAQEbQAAAAAAAIAJCNoAAAAAAAAAExC0AQAAAAAAACYgaAMAAAAAAABMQNAGAAAAAAAAmICgDQAAAAAAADABQRsAAAAAAABgAoI2AAAAAAAAwAQEbQAAAAAAAIAJCNoAAAAAAAAAExC0AQAAAAAAACb4/wABo6KAAn7Q3QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "# Fix: Use ax parameter instead of positional argument for the axes\n",
    "sns.barplot(data=new_df, x='Score', y='pos', ax=axes[0])\n",
    "sns.barplot(data=new_df, x='Score', y='neu', ax=axes[1])\n",
    "sns.barplot(data=new_df, x='Score', y='neg', ax=axes[2])\n",
    "fig.suptitle('See for yourself')\n",
    "plt.show()  # Added plt.show() instead of plt.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82bf5e10-021d-4997-8271-10d8fd1d1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c95c9f3-1702-494f-997b-5441569080a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ROBERTA PRETRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58225e4a-cc67-4922-8350-a0943cc7e938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\archi\\anaconda3\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\archi\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from transformers) (0.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\archi\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\archi\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df8786f7-5b6a-4129-a108-64c559d14d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\archi\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\archi\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\archi\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\archi\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\archi\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.0-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\archi\\anaconda3\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\archi\\anaconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.71.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Using cached keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\archi\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Using cached optree-0.16.0-cp312-cp312-win_amd64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Using cached tensorflow-2.19.0-cp312-cp312-win_amd64.whl (376.0 MB)\n",
      "Downloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.5/4.3 MB 3.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.0/4.3 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 1.6/4.3 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 2.1/4.3 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.6/4.3 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.4/4.3 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 3.2 MB/s eta 0:00:00\n",
      "Downloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 3.2 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/26.4 MB 6.3 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.8/26.4 MB 4.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.1/26.4 MB 4.5 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.6/26.4 MB 3.3 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 3.1/26.4 MB 3.1 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 3.9/26.4 MB 3.3 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 5.0/26.4 MB 3.6 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.5/26.4 MB 3.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.5/26.4 MB 3.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.5/26.4 MB 3.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.5/26.4 MB 3.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.5/26.4 MB 3.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.8/26.4 MB 2.2 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.8/26.4 MB 2.2 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.6/26.4 MB 2.1 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 7.6/26.4 MB 2.3 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 8.4/26.4 MB 2.4 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 9.2/26.4 MB 2.5 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 10.2/26.4 MB 2.6 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 10.5/26.4 MB 2.6 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 11.3/26.4 MB 2.6 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 12.1/26.4 MB 2.6 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 12.3/26.4 MB 2.7 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 13.1/26.4 MB 2.6 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.9/26.4 MB 2.7 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 14.7/26.4 MB 2.7 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 15.5/26.4 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 16.8/26.4 MB 2.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 17.8/26.4 MB 3.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 18.9/26.4 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 20.2/26.4 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 21.0/26.4 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.0/26.4 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 22.8/26.4 MB 3.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 23.6/26.4 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.6/26.4 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.7/26.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 3.4 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl (210 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.0/5.5 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.8/5.5 MB 4.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.4/5.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.2/5.5 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.7/5.5 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 4.0 MB/s eta 0:00:00\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.16.0-cp312-cp312-win_amd64.whl (315 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "Successfully installed absl-py-2.3.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 keras-3.10.0 libclang-18.1.1 ml-dtypes-0.5.1 namex-0.1.0 opt-einsum-3.4.0 optree-0.16.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a248246-4fdf-47e2-afc2-9d12e957583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a5256e7-e8c9-4757-b59c-dc256712dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38a7fca8-c03f-46fa-b525-cc9f8dfc66e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I am having a cup right now.  I went all over town looking for this coffee after been invited to a non profit organization to a banquet and besides the delicious food they top it off with this amazing delicious coffee.  I paid and extra 20 bucks to get it right way and it is worth the money. Highly recommend.\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9525c054-f641-4a76-8828-97633777aa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roberta_neg': 0.0017571753, 'roberta_neu': 0.015942467, 'rpberta_pos': 0.98230034}\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer(example, return_tensors='pt')\n",
    "output= model(**encoded_text)\n",
    "scores= output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "\n",
    "\n",
    "scores_dict= {\n",
    "    'roberta_neg': scores[0],\n",
    "    'roberta_neu' : scores[1],\n",
    "    'rpberta_pos' : scores[2]\n",
    "}\n",
    "print(scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "044cd321-d2cb-4474-8a54-3960ba30da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(example):\n",
    "    encoded_text = tokenizer(example, return_tensors='pt')\n",
    "    output= model(**encoded_text)\n",
    "    scores= output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    scores_dict= {\n",
    "        'roberta_neg': scores[0],\n",
    "        'roberta_neu' : scores[1],\n",
    "        'rpberta_pos' : scores[2]\n",
    "    }\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d053a258-453c-4923-83ca-7c758cc29003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n",
      "It aint running\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m         \u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m.\u001b[39mappend(func(text))\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIt aint running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m, in \u001b[0;36mfunc\u001b[1;34m(example)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(example):\n\u001b[0;32m      2\u001b[0m     encoded_text \u001b[38;5;241m=\u001b[39m tokenizer(example, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     output\u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoded_text)\n\u001b[0;32m      4\u001b[0m     scores\u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      5\u001b[0m     scores \u001b[38;5;241m=\u001b[39m softmax(scores)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1202\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;124;03mtoken_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;124;03m    Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1200\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1202\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta(\n\u001b[0;32m   1203\u001b[0m     input_ids,\n\u001b[0;32m   1204\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1205\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1206\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1207\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1208\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1209\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1210\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1211\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1212\u001b[0m )\n\u001b[0;32m   1213\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1214\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:869\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    867\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 869\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    870\u001b[0m     embedding_output,\n\u001b[0;32m    871\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m    872\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    873\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    874\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m    875\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    876\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    877\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    878\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    879\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    882\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:618\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    608\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    609\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    615\u001b[0m         output_attentions,\n\u001b[0;32m    616\u001b[0m     )\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 618\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    619\u001b[0m         hidden_states,\n\u001b[0;32m    620\u001b[0m         attention_mask,\n\u001b[0;32m    621\u001b[0m         layer_head_mask,\n\u001b[0;32m    622\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    623\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    624\u001b[0m         past_key_value,\n\u001b[0;32m    625\u001b[0m         output_attentions,\n\u001b[0;32m    626\u001b[0m     )\n\u001b[0;32m    628\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:507\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    497\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    506\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    508\u001b[0m         hidden_states,\n\u001b[0;32m    509\u001b[0m         attention_mask,\n\u001b[0;32m    510\u001b[0m         head_mask,\n\u001b[0;32m    511\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    512\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[0;32m    513\u001b[0m     )\n\u001b[0;32m    514\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:434\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    426\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    432\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    433\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 434\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    435\u001b[0m         hidden_states,\n\u001b[0;32m    436\u001b[0m         attention_mask,\n\u001b[0;32m    437\u001b[0m         head_mask,\n\u001b[0;32m    438\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    439\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    440\u001b[0m         past_key_value,\n\u001b[0;32m    441\u001b[0m         output_attentions,\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    443\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    444\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:325\u001b[0m, in \u001b[0;36mRobertaSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    323\u001b[0m     key_layer, value_layer \u001b[38;5;241m=\u001b[39m past_key_value\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(current_states))\n\u001b[0;32m    326\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(current_states))\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:168\u001b[0m, in \u001b[0;36mRobertaSelfAttention.transpose_for_scores\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranspose_for_scores\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    167\u001b[0m     new_x_shape \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_head_size)\n\u001b[1;32m--> 168\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(new_x_shape)\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list=[]\n",
    "for text in df['Text']:\n",
    "    try:\n",
    "        list.append(func(text))\n",
    "    except RuntimeError:\n",
    "        print('It aint running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c58f71c-27e1-461b-a68a-2656d8ec3b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8ce8f766504f9387b0788cd25064b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broke for id 83\n",
      "Broke for id 187\n",
      "Broke for id 529\n",
      "Broke for id 540\n",
      "Broke for id 746\n",
      "Broke for id 863\n",
      "Broke for id 1053\n",
      "Broke for id 1070\n",
      "Broke for id 1156\n",
      "Broke for id 1321\n",
      "Broke for id 1375\n",
      "Broke for id 1498\n",
      "Broke for id 1575\n",
      "Broke for id 1796\n",
      "Broke for id 1826\n",
      "Broke for id 2169\n",
      "Broke for id 2248\n",
      "Broke for id 2476\n",
      "Broke for id 2492\n",
      "Broke for id 2584\n",
      "Broke for id 2610\n",
      "Broke for id 2897\n",
      "Broke for id 2898\n",
      "Broke for id 2902\n",
      "Broke for id 2928\n",
      "Broke for id 2942\n",
      "Broke for id 2944\n",
      "Broke for id 2947\n",
      "Broke for id 2948\n",
      "Broke for id 3022\n",
      "Broke for id 3023\n",
      "Broke for id 3025\n",
      "Broke for id 3306\n",
      "Broke for id 3788\n",
      "Broke for id 3969\n",
      "Broke for id 4107\n",
      "Broke for id 4110\n",
      "Broke for id 4307\n",
      "Broke for id 4316\n",
      "Broke for id 4408\n",
      "Broke for id 4483\n",
      "Broke for id 4512\n",
      "Broke for id 4553\n",
      "Broke for id 4583\n",
      "Broke for id 5040\n",
      "Broke for id 5182\n",
      "Broke for id 5192\n",
      "Broke for id 5365\n",
      "Broke for id 5442\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m vader_result\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      9\u001b[0m     vader_result_rename[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvader_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m---> 10\u001b[0m roberta_result \u001b[38;5;241m=\u001b[39m func(text)\n\u001b[0;32m     12\u001b[0m both \u001b[38;5;241m=\u001b[39m { \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m vader_result_rename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m roberta_result}\n\u001b[0;32m     13\u001b[0m res[myid] \u001b[38;5;241m=\u001b[39m both\n",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m, in \u001b[0;36mfunc\u001b[1;34m(example)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(example):\n\u001b[0;32m      2\u001b[0m     encoded_text \u001b[38;5;241m=\u001b[39m tokenizer(example, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     output\u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoded_text)\n\u001b[0;32m      4\u001b[0m     scores\u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      5\u001b[0m     scores \u001b[38;5;241m=\u001b[39m softmax(scores)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1202\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;124;03mtoken_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;124;03m    Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1200\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1202\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta(\n\u001b[0;32m   1203\u001b[0m     input_ids,\n\u001b[0;32m   1204\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1205\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1206\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1207\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1208\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1209\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1210\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1211\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1212\u001b[0m )\n\u001b[0;32m   1213\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1214\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:869\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    867\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 869\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    870\u001b[0m     embedding_output,\n\u001b[0;32m    871\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m    872\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    873\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    874\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m    875\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    876\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    877\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    878\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    879\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    882\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:618\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    608\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    609\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    615\u001b[0m         output_attentions,\n\u001b[0;32m    616\u001b[0m     )\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 618\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    619\u001b[0m         hidden_states,\n\u001b[0;32m    620\u001b[0m         attention_mask,\n\u001b[0;32m    621\u001b[0m         layer_head_mask,\n\u001b[0;32m    622\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    623\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    624\u001b[0m         past_key_value,\n\u001b[0;32m    625\u001b[0m         output_attentions,\n\u001b[0;32m    626\u001b[0m     )\n\u001b[0;32m    628\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:549\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    546\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    547\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 549\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[0;32m    551\u001b[0m )\n\u001b[0;32m    552\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    554\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:253\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:562\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    561\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 562\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:473\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 473\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    474\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    475\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = {}\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    try:\n",
    "        text = row['Text' ]\n",
    "        myid = row['Id']\n",
    "        vader_result = sia.polarity_scores(text)\n",
    "        vader_result_rename = {}\n",
    "        for key, value in vader_result.items():\n",
    "            vader_result_rename[f\"vader_{key}\"] = value\n",
    "        roberta_result = func(text)\n",
    "        \n",
    "        both = { ** vader_result_rename, ** roberta_result}\n",
    "        res[myid] = both\n",
    "    except RuntimeError:\n",
    "        print(f'Broke for id {myid}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34e51f-4321-43c3-8d02-7979e6835f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df= new_df.reset_index().rename(columns={'index': 'Id'})\n",
    "result_df.Id= new_df.Id+1 \n",
    "result_df= new_df.merge(df, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90463277-7c99-4a6b-9268-8e95b4d5c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eac264-0141-4135-8b2c-c6b706f72e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.query('Score==1').sort_values('roberta_pos', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968513e-813c-4bfc-b681-1c178451b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.query('Score==5').sort_values('roberta_neg', ascending=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
